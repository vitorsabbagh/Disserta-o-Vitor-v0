\documentclass[msc,english]{coppe}

\usepackage{booktabs}% tabelas mais bonitas
\usepackage{rotating}% rodando coisas, como tabelas
\usepackage{longtable} % tabelas longas
\usepackage{rotating}
\usepackage[most]{tcolorbox} % caixas de texto
\usepackage{amsmath,amssymb}

\usepackage[editing]{coop-writing}
% \usepackage[publish]{coop-writing}

\usepackage{marvosym}
\cwsetcommwarn{\Lightning}
\cwnamedef{xexeo}{red}{X}
\cwnamedef{vitor}{blue}{V}
\usepackage{xurl}
\usepackage{hyperref}

\usepackage{multirow}
\usepackage{changepage} 

\usepackage{adjustbox} 

\usepackage{xcolor, colortbl}
\usepackage{hhline} % For double lines

\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}

\usepackage{lmodern}
\usepackage[T1]{fontenc}

\usepackage{silence}
% \WarningFilter{latex}{Overfull}
\WarningFilter{latex}{Underfull}
\WarningFilter{latex}{empty journal}

\usepackage{float}
\usepackage{pdflscape} % in preamble

\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}

\usepackage{quoting}


\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{keywordgreen}{rgb}{0.1,0.4,0.2}

\lstdefinestyle{mystyle}{ 
    commentstyle=\color{codegray},
    keywordstyle=\color{keywordgreen}\bfseries,
    numberstyle=\color{codegray},
    stringstyle=\color{purple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=10pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

% \usepackage[a4paper,margin=1.5cm]{geometry}


\makelosymbols
\makeloabbreviations

\begin{document}


\title{Comparative Analysis of Single and Multi-Agent Large Language Model Architectures for Domain-Specific Tasks in Well Construction}
\foreigntitle{Comparative Analysis of Single and Multi-Agent Large Language Model Architectures for Domain-Specific Tasks in Well Construction}
\author{Vitor}{BrandÃ£o Sabbagh}
\advisor{Prof.}{Geraldo}{Bonorino XexÃ©o}{D.Sc.}

\examiner{Prof.}{Geraldo Bonorino XexÃ©o}{D.Sc.}
\examiner{Prof.}{Jano Moreira de Souza}{Ph.D.}
\examiner{Prof.}{Arnaldo CÃ¢ndido JÃºnior}{D.Sc.}
\department{PESC}
\date{07}{2025}

\keyword{Large Language Models}
\keyword{Agents}
\keyword{Oil Well Construction}

\maketitle

\frontmatter
\dedication{To Carolina, my life partner.}

\chapter*{Acknowledgements}

To my daughter, Marina, who came into the world just two months ago, bringing a new light and a new purpose to my life. I dedicate every page of this work to you, with the hope of building a bright future for you.

To my parents, Vera and Nicolau, for all the love, unconditional support, and for always believing in me. Your faith in my abilities was the foundation for this achievement.

To my beloved wife, Carolina, my gratitude for all the patience, understanding, and love, especially during the most challenging moments of this journey. Without your support, this work would not have been possible.

To my stepson, Filipe, thank you for the moments of joy and relaxation that helped me maintain balance, especially during our Minecraft adventures. May our friendship continue to grow.

I express my deep gratitude to my mentor, Claudio, for his unwavering support and trust since the beginning of my career in digital transformation. His mentorship was fundamental to my professional development.

To my advisor, XexÃ©o, thank you for the wise guidance, academic rigor, and patience throughout this entire process. Your teachings were crucial to the quality of this work.

I extend my gratitude to the well construction engineering experts, Marcelo Grimberg, Rafael Peralta, and Lorenzo Simonassi, whose expertise and dedication significantly contributed to this research.

I also want to thank Ashish Vaswani. His work on "Attention Is All You Need" paved the way for the Large Language Models that were not only the subject of this dissertation but also an invaluable tool that helped me put ideas into words.

Finally, a special thanks to my colleagues from Petrobras and the Tecgraf Institute. Our daily discussions about Gen-AI were an inexhaustible source of inspiration and knowledge, immensely enriching this dissertation.

\begin{abstract}

    Esta dissertaÃ§Ã£o apresenta a aplicaÃ§Ã£o de modelos de linguagem (LLM) no setor de petrÃ³leo e gÃ¡s, especificamente em tarefas de construÃ§Ã£o e manutenÃ§Ã£o de poÃ§os. O estudo avalia o desempenho de uma arquitetura baseada em LLM de agente Ãºnico e de mÃºltiplos agentes no processamento de diferentes tarefas, oferecendo uma perspectiva comparativa sobre sua precisÃ£o e as implicaÃ§Ãµes de custo de sua implementaÃ§Ã£o. Os resultados indicam que sistemas multiagentes oferecem desempenho melhorado em tarefas de perguntas e respostas, com uma medida de veracidade 28\% maior do que os sistemas de agente Ãºnico, mas a um custo financeiro mais alto. Especificamente, a arquitetura multiagente incorre em custos que sÃ£o, em mÃ©dia, 3,7 vezes maiores do que os da configuraÃ§Ã£o de agente Ãºnico, devido ao aumento do nÃºmero de tokens processados. Por outro lado, os sistemas de agente Ãºnico se destacam em tarefas de texto para SQL (Linguagem de Consulta Estruturada), especialmente ao usar o Transformador PrÃ©-Treinado Generativo 4 (GPT-4), alcanÃ§ando uma pontuaÃ§Ã£o 15\% maior em comparaÃ§Ã£o com as configuraÃ§Ãµes multiagentes, sugerindo que arquiteturas mais simples podem, Ã s vezes, superar a complexidade. A novidade deste trabalho reside em seu exame original dos desafios especÃ­ficos apresentados pelos dados complexos, tÃ©cnicos e nÃ£o estruturados inerentes Ã s operaÃ§Ãµes de construÃ§Ã£o de poÃ§os, contribuindo para o planejamento estratÃ©gico da adoÃ§Ã£o de aplicaÃ§Ãµes de IA generativa, fornecendo uma base para otimizar soluÃ§Ãµes contra parÃ¢metros econÃ´micos e tecnolÃ³gicos.
    
    \end{abstract}
    
\begin{foreignabstract}


    This article explores the application of large language models (LLM) in the oil and gas  sector, specifically within well construction and maintenance tasks. The study evaluates the performances of a single-agent and a multi-agent LLM-based architecture in processing different tasks, offering a comparative perspective on their accuracy and the cost implications of their implementation. The results indicate that multi-agent systems offer improved performance in question and answer tasks, with a truthfulness measure 28\% higher than single-agent systems, but at a higher financial cost. Specifically, the multi-agent architecture incurs costs that are, on average, 3.7 times higher than those of the single-agent setup due to the increased number of tokens processed. Conversely, single-agent systems excel in text-to-SQL (Structured Query Language) tasks, particularly when using Generative Pre-Trained Transformer 4 (GPT-4), achieving a 15\% higher score compared to multi-agent configurations, suggesting that simpler architectures can sometimes outpace complexity. The novelty of this work lies in its original examination of the specific challenges presented by the complex, technical, unstructured data inherent in well construction operations, contributing to strategic planning for adopting generative AI applications, providing a basis for optimizing solutions against economic and technological parameters. 
    
\end{foreignabstract}


\tableofcontents
\listoffigures
\listoftables
\printlosymbols
\printloabbreviations

\mainmatter


\input{part01_introducao.tex}

\input{part02_revisao.tex}

\input{part03_experimento1.tex}

\input{part04_experimento2.tex}

\input{part05_conclusao.tex}

% \input{part05a_conclusao2.tex}

\backmatter
\bibliographystyle{en-coppe-unsrt}
\bibliography{bib}

\appendix

\input{part10_apendice.tex}

\renewcommand{\appendixname}{Appendix}
\appendix

% \input{part11_anexo.tex}

\listofcomments

\end{document}

%% 
%%
%% End of file `example.tex'.

\chapter{Introduction}


% \section{Contextualization}


    % [IA-GEN NA INDUSTRIA] 
    In the dynamic and ever-changing oil and gas (O\&G) industry,
    \xexeo{eu acho que aqui falta alto. TEm um pouco hÃ¡ ver com o uso do "in the dynamic changing of the" nÃ£o ser realmente muito adequado a "oil and gas industry", falta algo, um qualificador a mais (mercado? . Minha IA sugeriu mudar para "In the dynamic and ever-changing oil and gas (O\&G) industry", o que eu achei muito melhor}
    \vitor{feito}
    \abbrev{O\&G}{oil and gas} 
    digital transformation has emerged as a key element to achieve operational efficiency, sustainability, and competitiveness. 
    At the forefront of this transformation are Large Language Models (LLMs), which have the potential to process unstructured queries, map out alternatives, and advise users on possible actions \citep{Kar2023}. 
    We also note the advantage of increased engagement, cooperation, accessibility, and ultimately profitability. 
    These models redefine paradigms in knowledge management and information retrieval and impact a variety of other areas \citep{Eckroth2023}, making it crucial to adopt these technologies to remain competitive.    
    
    % [ESTUDO AUMENTO PRODUTIVIDADE] 
    A study conducted by \citet{Dellacqua2023}, in collaboration with the Boston Consulting Group, shows that in knowledge-intensive tasks, consultants equipped with access to LLMs such as GPT-4 not only completed tasks more efficiently (25.1\% more quickly on average) but also with substantially higher quality, achieving results more than 40\% better compared to those without AI assistance \citep{Dellacqua2023}.
    Increase in productivity of knowledge workers was 12\% on average.    
    A major oil company spent in 2023 \$2.8B with employee compensation \citep{Petrobras2024}.
    A potential increase of 12\% in knowledge workers productivity, given they represent 60\% of all employee, could represent \$204M annual savings in this scenario. 
    \xexeo{Aqui estÃ¡ um problema clÃ¡ssico. VocÃª quer fornecer um dado mas provavelmente nÃ£o quer dizer que Ã© a Petrobras. Mas provavelmente esse dado estÃ¡ em algum relatÃ³rio pÃºblico. NÃ£o consegue ele de algum lugar e aÃ­ pode citar?} 
    \vitor{feito}   
    
    % [AUMENTO DO PIB DEVIDO A GEN AI] 
    Broader economic indicators predict significant transformations due to generative AI (Gen-AI) across various industries.
    A report from Goldman Sachs \citep{Hatzius2023} highlights that Gen-AI is poised to increase global GDP by nearly 7\%, increasing productivity growth by 1.5 percentage points over the next decade. 
    This economic uplift is expected due to AI's ability to automate complex workflows and create new business opportunities, significantly impacting employment and productivity sectors worldwide.


            
    % [PROBLEMA DE DADOS NA INDUSTRIA EM GERAL] 
    Expanding on the broader discussion on data utilization within organizations, an important issue is the challenge of extracting relevant information from extensive databases \citep{Singh2023}. 
    Initially, the challenge of knowing, finding, and accessing data poses a significant obstacle to decision-making processes. 
    Collaborators at O\&G companies often face the intensive task of manually searching large data repositories to find useful information.


    
    % [PROBLEMA DE DADOS NO O\&G] 
    Focusing specifically on the activities of drilling and completion of offshore and onshore wells, a major challenge lies in the inherently complex and technical nature of the data involved, which can be from various types: operations, projects, technologies, supply chains, and others. 
    Inefficiency in leveraging large volumes of unstructured data worsens these challenges, as observed by \citet{Singh2023}. 
    A significant amount of the data generated and collected in this sector is unstructured, ranging from text reports and emails to images and videos of exploration and production activities. 
    Examples include hundreds of daily operational reports from drilling rigs, well execution projects, nonproductive time (NPT) reports, and operational lessons learned documents, as illustrated in Figure \ref{fig:report_example}. 
    As a result, valuable information can remain untapped, and the potential to find insights, informed decision-making, and innovation is significantly compromised.
    \citet{Singh2023} showcases the capabilities and potential of Generative AI-enabled chatbots for the O\&G sector, particularly in enhancing drilling and production analytics to achieve better business results. The author concludes that companies that adopt these technologies in the coming years will see clear advantages.     
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{images/report_example.png}
        \caption{Sample of drilling \& completion learned lesson partial document. (translated from Portuguese)}
        \label{fig:report_example}
    \end{figure}           
    
    However, the deployment of such technologies presents limitations and introduces challenges, including biased data, hallucinations, lack of explainability, and logical reasoning errors, among others \citep{Hadi2023}, which require a balanced approach to harness their potential in a responsible manner.    
    Although previous research has focused mainly on the broader applications of AI in industry, the novelty of our research lies in its original examination of the specific challenges and solutions presented by the complex, technical and unstructured data inherent in O\&G operations. 
    By comparing single- and multi-agent systems, this study fills a knowledge gap, providing empirical insights into the effectiveness of different Gen-AI architectures in a domain where such studies are scarce. 
    
    The adoption of these technologies by a major oil company underscores their potential to revolutionize data analysis and management, presenting an opportunity for deeper exploration and application.

\section{Business Scope Delimitation}

    To contextualize the scope of this study, it is necessary to understand the life cycle of an oil field, which begins with Exploration and progresses to the Development of Production, followed by effective production, and culminates in decommissioning \citep{Badiru2016}. Gen-AI has the potential to impact each of these phases, but the focus of this work lies in the operations of the development and maintenance stages.
            
    Well construction is a highly specialized activity that involves drilling and completion of wells for hydrocarbon extraction \citep{Thomas2004}. In this context, Gen-AI can be applied in various ways. 
    For example, a chatbot could manage knowledge by answering queries about operations and well projects by retrieving information from the organization's databases. 
    Additionally, LLM-based agents could be used in executive project review to ensure that drilling or completion operations comply with the organization's standards and adhere to best operational practices. 
    Moreover, Gen-AI could perform inference in unstructured databases to extract specific information from text reports and obtain structured data. This business scope emphasizes the importance of Gen-AI in the construction and maintenance of wells.

    \subsection{Key Information Sources in Well Engineering} \label{sec:information-sources}

        To fully appreciate the challenges in this domain, it is important to understand the primary data sources that specialists interact with daily. The following sources, used in this research's experiments, exemplify the complex information landscape of well engineering:

        \paragraph{Operational Knowledge Items} During drilling, completion, and workover interventions, documents called Knowledge Items are written by specialists, as depicted in Fig~\ref{fig:report_example}. These can be of four types: Technical Alert, Learned Lesson, Good Practice, and Well Observation. This system serves as a critical tool for knowledge management, considering the large number and variety of specialists involved and well operations performed.

        \paragraph{Operational NPTs (Non-Productive Time)} This data source contains structured records of anomalies that occurred during well interventions, detailing the title, description, location, operation type, responsible sector, rig involved, time lost, and event dates. These data are critical for the industry, as NPTs represent periods when operations are interrupted. The identification and analysis of these events are essential for continuous process improvement, cost reduction, and increased operational efficiency.

        \paragraph{Collaborator Finder} The third data source is a collaborator finder, an important internal tool for consulting and managing employee data. This system allows for the quick identification of employees through information such as name, workplace, and role. The importance of this tool lies in the ability to cross-reference employee data with operational events, enabling a more complete analysis by an intelligent agent.

\section{Objectives}

    This research directly addresses the challenges facing major oil companies. 
    By investigating the comparative advantages and limitations of various Gen-AI architectures, including single and multi-agent systems, for Q\&A and Text-to-SQL tasks, this study aims to identify the most efficient and cost-effective solutions.
    
    The specific objectives of this research are to assess the suitability and effectiveness of multi-agent systems based on LLMs for complex, domain-specific tasks in well engineering, aiming to streamline information access and decision-making.
    
    The study will compare single-agent and multi-agent AI systems in terms of their ability to address well engineering queries. Finally, it will map the potential obstacles and limitations associated with deploying Gen-AI applications.
            
    The insights gained from this research will directly contribute to O\&G companies strategic goals by improving access to well engineering information and automated data analysis tasks. 
    A comprehensive understanding of the challenges and limitations associated with Gen-AI will enable informed decisions about its adoption, maximizing the return on investment. 

    To achieve these objectives, this research was conducted through two distinct experimental phases. The first, carried out in 2024, focused on a foundational comparison between single and multi-agent architectures, which revealed that, although the multi-agent architecture achieved 28\% higher truthfulness in Q\&A tasks, its cost was on average 3.7 times higher. Furthermore, the single-agent architecture proved to be surprisingly more effective in Text-to-SQL tasks. The rapid evolution of generative AI frameworks and models prompted a second, more advanced experiment in 2025. This second phase built upon the initial findings, also employing non-agentic workflows as baseline and a more rigorous, quantitative evaluation methodology to address the challenges identified in the first experiment and automated evaluation based on the concept commonly reffered to as "LLM-as-judge" (\citep{Gu2025}).
    
    \xexeo{Acho que pode aumentar um pouco isso, jÃ¡ falar dos resultados (dissertaÃ§Ã£o tem spoiler), motivar o segundo a partir dos resutlados do primeiro e botar depois das questÃµes de pesquisa (leia o todo), sempre alinhando tudo (questÃ£o de pesquisa - experimento - conclusÃµes}
    \vitor{IncluÃ­do spoiler e ponte do 1o justificando o 2o experimento.}

    <<<<PENDENTE>>>>

    FAZER ALINHAMENTO FINAL (QUESTÃƒO DE PESQUISA - EXPERIMENTO - CONCLUSÃ•ES)
    
    DETALHAMENTO LLM: "BOTAR DEPOIS DAS QUESTÃ•ES DE PESQUISA" E "SEMPRE ALINHANDO TUDO"
    ESTA Ã‰ A "LINHA DE OURO" (GOLDEN THREAD) DE UMA DISSERTAÃ‡ÃƒO. TUDO PRECISA ESTAR CONECTADO. A ESTRUTURA QUE ELE SUGERE CRIA UM FLUXO MUITO LÃ“GICO PARA O LEITOR:
    
    OBJETIVOS: O QUE VOCÃŠ QUER ALCANÃ‡AR (VISÃƒO GERAL).
    
    QUESTÃ•ES DE PESQUISA (A SEREM ADICIONADAS): AS PERGUNTAS ESPECÃFICAS E FOCADAS QUE SUA PESQUISA VAI RESPONDER.
    
    PARÃGRAFO COM "SPOILER" (O QUE ESTAMOS DISCUTINDO): UM RESUMO DE COMO VOCÃŠ RESPONDEU A ESSAS PERGUNTAS E O QUE ENCONTROU.
    
    CONCLUSÃ•ES (NO FINAL DA DISSERTAÃ‡ÃƒO): ONDE VOCÃŠ RESPONDE FORMALMENTE Ã€S QUESTÃ•ES DE PESQUISA, USANDO OS RESULTADOS DETALHADOS DOS EXPERIMENTOS.

    AO COLOCAR O PARÃGRAFO DE "SPOILER" DEPOIS DAS QUESTÃ•ES DE PESQUISA, VOCÃŠ CRIA UMA CONEXÃƒO DIRETA: "PARA RESPONDER A ESTAS PERGUNTAS (QUESTÃƒO 1, QUESTÃƒO 2...), EU CONDUZI ESTES EXPERIMENTOS (EXPERIMENTO 1, EXPERIMENTO 2), QUE ME LEVARAM A ESTAS DESCOBERTAS PRINCIPAIS (RESULTADO A, RESULTADO B)."

    ESSA ESTRUTURA AMARRA TODA A SUA DISSERTAÃ‡ÃƒO, TORNANDO-A COESA, LÃ“GICA E FÃCIL DE ACOMPANHAR.
    <<<<FIM>>>>

    \todo[inline]{Ok, estÃ¡ bom, porÃ©m seria melhor para dissertaÃ§Ã£o se agora vocÃª definisse questÃµes de pesquisa. Essas questÃµes de pesquisa serÃ£o respondidas na conclusÃ£o, a partir do que vocÃª fez. Eu sÃ³ li atÃ© aqui, entÃ£o nÃ£o tenho sugestÃµes fortes agora, mas as questÃµes podem ser coisas coisa: Qual a eficÃ¡cia e eficiÃªncia de LLMs para extrair dados de bases .... Como sistemas single e multi agentes se comparam... Elas podem ser bem melhores e bem mais objetivas e ao longo do texto, se eu detectar alguma , escrevo. A questÃ£o Ã© perguntar aqui no fim da introduÃ§Ã£o e responder na conclusÃ£o, caracterizando a colaboraÃ§Ã£o
    \newline \newline SERÃ FEITO NO FIM}


% \todo[inline]{\textbf{VITOR: CONCLUÃDO} \newline \newline 
%     Ok, faltou a metodologia e tem que ter. Tem que explicar a metodologia genÃ©rica de sua pesquisa. Eu agora nÃ£o sei bem o que se ajeita, mas vejo que talvez possa descrever como DSR ou outro mÃ©todo que implique em aÃ§Ãµes reais em um local. Ã‰ importante notar que vocÃª descreve uma metodologia nos seus experimentos que Ã© um processo de pesquisa (e isso nÃ£o estÃ¡ errado), porÃ©m aqui hÃ¡ deve ser usado um conceito de metodologia mais amplo, ligado a epistemologia. Argumentos em funÃ§Ã£o da DSR: Design Science Research (DSR) [sim, pedi para o ChatGPT, mas eu jÃ¡ meio que concordava]
%     A Design Science Research Ã© a classificaÃ§Ã£o mais apropriada para a metodologia empregada, pelos seguintes motivos:
%     \textbf{Artefato proposto}: A pesquisa visa comparar e melhorar arquiteturas de sistemas baseadas em LLMs aplicadas a tarefas especÃ­ficas de engenharia de poÃ§os, o que se alinha Ã  criaÃ§Ã£o e avaliaÃ§Ã£o de artefatos â€” elemento central da DSR.
%     \textbf{AvaliaÃ§Ã£o empÃ­rica}: Foram realizados dois ciclos de experimentos com diferentes configuraÃ§Ãµes de agente Ãºnico e multiagente, com dados reais da indÃºstria de O\&G e validaÃ§Ã£o por especialistas.
%     \textbf{Ciclos iterativos}: O segundo experimento se baseia explicitamente nos resultados do primeiro, sugerindo um processo iterativo de refinamento de artefatos â€” outro elemento tÃ­pico de DSR.
%     \textbf{RelevÃ¢ncia prÃ¡tica + rigor cientÃ­fico}: O trabalho visa resolver um problema concreto de empresas do setor de petrÃ³leo e gÃ¡s, mas com critÃ©rios e mÃ©tricas de avaliaÃ§Ã£o quantitativas rigorosas.
%     \textbf{ConclusÃ£o}: Mesmo sem ter sido explicitamente rotulada como DSR na dissertaÃ§Ã£o, a estrutura e objetivos da pesquisa indicam fortemente que ela segue os princÃ­pios dessa metodologia.
%     \newline \newline \textbf{VITOR: CONCLUÃDO}}

\section{Research Methodology}
  
    This research follows the Design Science Research (DSR) methodology, a framework particularly suited for studies that develop and evaluate technological artifacts to address specific organizational problems. DSR provides a structured approach for creating innovative solutions while maintaining scientific rigor through empirical validation \citep{hevner2007three}.
    
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dsr-model.png}
    \caption{Main elements of DSR-Model, translated from \citet{Oswald2023}.}
    \label{fig:dsr-model}
    \end{figure}
    
    \subsection{Design Science Research Framework}
    
        The DSR methodology employed in this study consists of four interconnected elements, as illustrated in Figure~\ref{fig:dsr-model}:
        
        \begin{enumerate}
        \item \textbf{Problem in Context}: Identifying and defining a relevant organizational challenge within its specific environment
        \item \textbf{Artifact}: Designing and developing a technological solution to address the identified problem
        \item \textbf{Behavioral Conjectures}: Formulating hypotheses about how the artifact will function and impact the problem space
        \item \textbf{Empirical Evaluation}: Systematically testing the artifact to validate its effectiveness and the underlying conjectures
        \end{enumerate}
        
        This cyclical framework guides both the research design and execution, ensuring that the developed artifacts are not only technically sound but also practically relevant.
    
    \subsection{Application of DSR in This Research} \label{sec:dsr-application}
        
        \subsubsection{Problem in Context}
        
        This study addresses the challenge of efficiently extracting relevant information from extensive technical databases in the oil and gas industry, specifically in well construction and maintenance operations. 
        
        \begin{table}[h]
            \centering
            \caption{Characteristics of the Problem Context}
            \begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
            \hline
            \textbf{Challenge Aspect} & \textbf{Description} \\
            \hline
            Data Structure & Large volumes of unstructured data (operational reports, lessons learned documents, NPT reports) \\
            \hline
            Technical Complexity & Domain-specific terminology and complex relationships \\
            \hline
            Business Impact & Significant potential economic impact from improved knowledge access \\
            \hline
            \end{tabular}
            \label{tab:problem-context}
        \end{table}
        
        \subsubsection{Artifacts}
        
        Two primary artifacts were designed and implemented, illustrated in Figure~\ref{fig:artifacts}, using state-of-the-art language models (GPT-3.5-turbo and GPT-4) and integrated with domain-specific knowledge bases through various retrieval mechanisms.
        
        \begin{figure}[h]
        \centering
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\begin{tabular}{c}
            \textbf{Single-Agent LLM System} \\
            \small A centralized architecture where one \\
            \small language model agent handles the entire \\
            \small question-answering process with \\
            \small access to multiple tools
            \end{tabular}}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\begin{tabular}{c}
            \textbf{Multi-Agent LLM System} \\
            \small A collaborative architecture where \\
            \small multiple specialized agents work together \\
            \small under coordination to process queries \\
            \small  
            \end{tabular}}
        \end{minipage}
        \caption{Primary Artifacts Developed in This Research}
        \label{fig:artifacts}
        \end{figure}
        
        
        \subsubsection{Behavioral Conjectures}
        
        The research was guided by several key conjectures:
        
        \begin{tcolorbox}[colback=gray!10, colframe=gray!40, title=Key Research Conjectures]
            \begin{itemize}
            \item Multi-agent systems will demonstrate higher accuracy in complex technical queries due to their ability to distribute cognitive load and specialize in different aspects of the problem
            \item The performance advantages of multi-agent systems will vary by task type (Q\&A vs. Text-to-SQL)
            \item More advanced language models will yield better performance but at significantly higher LLM financial costs
            \item The economic efficiency (performance-to-cost ratio) will be a critical factor in determining practical implementation viability
            \end{itemize}
        \end{tcolorbox}
        
        \subsubsection{Empirical Evaluation}
        
        The evaluation was conducted through two distinct experimental phases (summarized in Table~\ref{tab:experiments}), allowing for iterative refinement of both the artifacts and the evaluation methodology, addressing limitations identified in the first experiment while adapting to the rapid evolution of language model capabilities.
        
        \begin{table}[h]
        \centering
        \caption{Comparison of Experimental Phases}
        \begin{tabular}{|p{0.15\textwidth}|p{0.38\textwidth}|p{0.38\textwidth}|}
        \hline
        \textbf{Aspect} & \textbf{First Experiment (2024)} & \textbf{Second Experiment (2025)} \\
        \hline
        Focus & Comparative analysis of single and multi-agent architectures & Extended evaluation incorporating non-agentic workflows as baseline \\
        \hline
        Evaluation Methods & Expert validation by domain specialists & Automated assessment using "LLM-as-judge" approach \\
        \hline
        Metrics & Truthfulness, performance, and LLM cost & Precision, recall, and F1-score \\
        \hline
        Outcomes & Identification of key challenges and limitations & More rigorous quantitative evaluation methodology \\
        \hline
        \end{tabular}
        \label{tab:experiments}
        \end{table}

    
    % \subsection{Research Quality and Validity}
    
    %     To ensure research quality and validity, several measures were implemented, as shown in Figure~\ref{fig:research-quality}. 
    %     By adhering to the DSR methodology, this research maintains a balance between practical utility and scientific rigor, producing artifacts that address real organizational needs while contributing to the theoretical understanding of multi-agent LLM systems in specialized technical domains.
        
    %     \begin{figure}[h]
    %         \centering
    %         \begin{tikzpicture}
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (a) at (0,0) {\textbf{Triangulation}\\\small Using multiple data sources, evaluation methods, and expert perspectives};
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (b) at (6,0) {\textbf{Reproducibility}\\\small Detailed documentation of experimental procedures, prompts, and configurations};
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (c) at (0,-4) {\textbf{Practical Relevance}\\\small Direct application to real-world operational challenges};
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (d) at (6,-4) {\textbf{Theoretical Contribution}\\\small Insights into comparative advantages of different agent architectures};
    %         \draw[->] (a) -- (b);
    %         \draw[->] (b) -- (d);
    %         \draw[->] (a) -- (c);
    %         \draw[->] (c) -- (d);
    %         \end{tikzpicture}
    %         \caption{Research Quality Assurance Framework}
    %         \label{fig:research-quality}
    %     \end{figure}
        
        

\section{Thesis Structure}



    ****SERÃ FEITO POR ÃšLTIMO****
   
\chapter{Literature Review} 

% \todo[inline]{Todo capÃ­tulo deve ter uma introduÃ§Ã£o explanatÃ³ria. "This chapter describes"}

    This chapter provides a comprehensive literature review of the key technologies and concepts that form the foundation of this dissertation. It begins with an overview of the applications of Artificial Intelligence (AI) in the Exploration and Production (E\&P) industry. The focus then narrows to Large Language Models (LLMs), discussing their architecture and impact. Subsequently, the chapter delves into the Retrieval-Augmented Generation (RAG) technique, which enhances LLMs with external knowledge. It also explores the use of single and multi-agent setups. Finally, the chapter concludes by examining the 'LLM-as-judge' paradigm for evaluating the performance of generative models.


    \section{AI in the Exploration and Production (E\&P) industry}

        The use of AI in the Exploration and Production (E\&P) industry has been extensive. 
        In the last decades the majority of AI applications in the industry involved data mining and neural networks \citep{Bravo2014}. 
        An example is the work by \citep{Gudala2021} on optimization of the properties of the heavy oil flow, through the use of neural networks to optimize flow-influencing parameters.
        Another development was a deep learning workflow proposed by \citep{Gohari2024}, with the generation of synthetic graphic well logs through the application of transfer learning. 
        These developments illustrate the potential of AI to improve processes and the accuracy and efficiency of data analysis \citep{Rahmani2021}.
    
        Natural Language Processing (NLP) stands at the intersection of computer science and linguistics, representing a domain within artificial intelligence aimed at enabling computers to understand and process human language in a way that is both meaningful and effective \citep{Liddy2001}. 
        This field integrates a diverse range of computational techniques to analyze and represent text at various levels of linguistic detail, striving to emulate human-like language understanding. 
        As an active area of research, traditionally NLP employs multiple layers of language analysis, each contributing uniquely to the interpretation and generation of language, which finds practical applications in various sectors \citep{Liddy2001}.      
        In the O\&G industry, the management of unstructured data, such as texts, images, and documents, is crucial, with Natural Language Processing (NLP) and Machine Learning playing key roles.
        Research by \citet{Antoniak2016} and \citet{Castineira2018} has explored the use of NLP to analyze risks and drilling reports.           
    
    \section{Natural Language Processing}

        NLP (Natural Language Processing) is a broad field that covers various tasks to enable computers to process and understand human language. These tasks, which represent specific problems or applications, have been the focus of research for decades, predating the recent surge in Large Language Models. They range from fundamental challenges like part-of-speech tagging to complex applications like machine translation. This section explores two tasks particularly relevant to this dissertation: Question Answering (Q\&A) and Text-to-SQL, both of which have been significantly advanced by recent developments in the field.

        \subsection{Q\&A tasks}     

            Question and Answer (Q\&A) represent a method to facilitate knowledge transfer between individuals within organizations \citep{Iske2005}. 
            \xexeo{As tasks vem antes das LLMs, elas sempre existiram como problemas da Ã¡rea de NLP. Inclusive acho que na seÃ§Ã£o de NLP vocÃª pode fazer um parÃ¡grafo sobre a existÃªncia de vÃ¡rias tasks e usar essas como subseÃ§Ãµes}
            \vitor{Feito}
            Conceptually, Q\&A systems are designed to connect individuals who possess specific knowledge with those seeking that knowledge through a structured question-and-answer format. 
            The role of Q\&A in the documentation landscape, as exemplified by platforms such as Stack Overflow, highlights their significance in technical disciplines \citep{Treude2011}. 
            This understanding can guide organizations in making more informed decisions about implementing such systems to enhance knowledge transfer and organizational learning \citep{Iske2005}.

        \subsection{Text-to-SQL tasks} 

            Text-to-SQL tasks in the context of artificial intelligence involve the automatic translation of natural language questions or commands into structured SQL (Structured Query Language) queries \citep{Qin2022}. This is an important area in natural language processing (NLP), allowing users to interact with databases using plain language rather than needing to know how to write complex SQL queries.         
                
            The arrival of advanced language models like GPT-3 and GPT-4 \citep{OpenAImodels} has marked a significant leap in Text-to-SQL applications \citep{Singh2023}, demonstrating remarkable capabilities in handling these tasks. This can be attributed to their extensive training on diverse datasets \citep{Deng2021}, which include not only large amounts of text but also structured data like tables and code, enabling the model to understand the intricate relationships between language and data structures. The study by \citep{Deng2023} introduces a pre-training framework for text to SQL translation, emphasizing the alignment between text and tables in Text-to-SQL tasks.






    \section{Intelligent Agents}         
        % \xexeo{Agentes existem antes das LLMs, logo essa seÃ§Ã£o deve vir antes, inclusive jÃ¡ transformei em seÃ§Ã£o}
        % \vitor{feito}
        According to \citet{Russell2020}, an agent is something that performs actions. When it comes to computerized agents (in our case, AI-based), these agents are expected to do more: operate autonomously, perceive the environment, persist over time, adapt to changes, create, and strive to achieve goals.
        The agent program implements the agent function.
        There is a variety of basic agent program designs that vary in efficiency, compactness, and flexibility. The appropriate design of the agent program depends on the nature of the environment. In this work, a goal-based agent design was implemented, which acts to achieve defined goals \citep{Russell2020}.
        Other possible types include simple reflex agents, which directly respond to perceptions, while model-based reflex agents maintain an internal state to track aspects of the world that are not evident in the current perception. Finally, there are utility-based agents, which try to maximize their expected "happiness" \citep{Russell2020}.


        \subsection{Multi-Agent Systems}

            A Multi-Agent System (MAS) extends the concept of a single agent to a collection of agents that interact within a shared environment \citep{Gokulan2010}. A MAS is defined as a loosely coupled network of autonomous problem-solving entities that collaborate to find solutions to problems that are beyond the individual capabilities or knowledge of any single entity \citep{FloresMendez1999}. 
            % These systems are characterized by having no global system control, decentralized data, and asynchronous computation, with each agent possessing only incomplete information or capabilities to solve the overall problem \citep{FloresMendez1999}. This distributed nature provides several advantages, including increased speed and efficiency through parallel computation, enhanced reliability and robustness due to graceful degradation if an agent fails, and greater scalability and flexibility, as new agents can be added to the system when necessary \citep{Gokulan2010}.
            
            % Despite these benefits, 
            % The design of a MAS presents significant challenges, with coordination being the central issue \citep{Gokulan2010}. In a multi-agent environment, the action of one agent can modify the environment for others, necessitating that each agent attempts to predict the actions of its neighbors to make optimal, goal-directed decisions. This creates a complex dynamic where coordination is essential to prevent chaos, manage conflicts arising from limited individual perspectives, and meet global constraints \citep{Gokulan2010}. Effective interaction is therefore critical and typically requires mechanisms for agents to find each other, such as facilitators or brokers, and the use of a common agent communication language (ACL) and shared ontologies to ensure mutual understanding \citep{FloresMendez1999}.
            
            The structure of a MAS can vary, with different organizational paradigms such as hierarchical structures or coalitions being employed depending on the application \citep{Gokulan2010}. A practical example of a MAS architecture is demonstrated in power system restoration, where a system can be composed of multiple "bus agents" and a single "facilitator agent" \citep{Nagata2002}. In this setup, each bus agent works to restore its local area by negotiating with neighboring agents based on locally available information, while the facilitator agent manages the overall decision-making process, showcasing how a collection of agents with simple, local strategies can cooperate to achieve a complex, global goal \citep{Nagata2002}.



    \section{Large Language Models}         

        Large Language Models (LLMs) are advanced neural network-based models designed to understand and generate human-like text. 
        They leverage the Transformer architecture introduced in the seminal paper \enquote{Attention is All You Need} by \citet{Vaswani2017}. 
        This architecture relies on self-attention mechanisms, allowing the model to weigh the importance of different words in a sentence effectively. 

        The emergence of LLMs has made it possible to understand and produce textual information. 
        These systems are expected to revolutionize various industries by supporting complex decision-making processes. GPT models \citep{OpenAI2023}, in particular, take advantage of its vast training data to provide human-like responses \citep{Mosser2024}, which can be highly beneficial in contexts requiring natural language understanding and generation. The exponential growth in the size and capability of LLMs in recent years has been remarkable. Models like OpenAI's GPT series have shown significant advancements, moving from millions to hundreds of billions of parameters, which gives them increasingly sophisticated natural language understanding and generation. This advancement is illustrated in Figure~\ref{fig:llm_evolution}. For new models (released after jan/2025), including OpenAI's o3 series and GPT-4.5, Anthropic's Claude 3.7 and 4, and Google's Gemini 2.5 Pro, the exact parameter counts have not been publicly disclosed. 

        \xexeo{Acho que aqui merecia um grÃ¡fico do crescimento do tamanho das LLMs e um parÃ¡grafo sobre esse crescimento}
        \vitor{Feito}

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\textwidth]{images/llm_evolution.png}
            \caption{The evolution of LLMs.}
            \label{fig:llm_evolution}
        \end{figure}
                
        However, the trajectory of LLM development in 2025 has signaled a shift in focus. While previous advancements were often marked by an exponential increase in parameter counts, the latest generation of models emphasizes sophisticated reasoning capabilities over sheer size. 
        This move away from parameter size as the primary metric of progress underscores a new trend: enhancing the models' ability to perform complex, multi-step reasoning. 
        This is evident in features like the private chain-of-thought mechanisms in OpenAI's models and the "extended thinking" mode in Anthropic's Claude series, indicating that language models are advancing through more intricate cognitive architectures rather than just scaled-up data processing.

        As highlighted by \citet{Singh2023}, the integration of LLM-based solutions, such as conversational chatbots, offers an approach to optimizing operations across various business segments, including drilling, completion, and production.
        \citet{Singh2023} uses LLMs models to extract, analyze, and interpret datasets, enabling generation of insights and recommendations. 

        Despite its widespread impact, language models are not without its limitations. 
        In many industry-specific applications, the critical information required is often proprietary, not shared with third parties, and thus absent from the training data of these LLMs \citep{Mosser2024}. 
        This gap means that GPT models might not have access to the most up-to-date or sensitive information needed for certain tasks. 
        Moreover, due to their probabilistic nature, LLMs can experience hallucinations, producing confident yet incorrect or nonsensical responses based on user input \citep{OpenAI2023}. 
    
    
        \subsection{LLM applications}

        \xexeo{Precisa de um texto aqui}
        \vitor{Feito}

            % LLM applications have witnessed a dramatic surge in development and adoption, reshaping the landscape of AI. 
            % This growth is fueled by continuous advancements in model architectures, training techniques, and the availability of vast datasets. 
            The proliferation of LLMs has led to a diverse array of applications that leverage their ability to understand, generate, and process human language.

            The expansion of the LLM application ecosystem is evident in the significant market growth projections. For instance, one report projects the global LLM market to grow from \$5.62 billion in 2024 to \$35.43 billion by 2030, with a compound annual growth rate (CAGR) of 36.9\% \citep{GrandViewResearch2025}. This rapid expansion is indicative of the immense value and potential that organizations across industries see in these technologies. The applications themselves are becoming increasingly sophisticated, evolving from simple text generation to complex, multimodal systems capable of processing and integrating text, images, and other data formats \citep{Kaddour2023}.
            
            The spectrum of LLM-based applications is broad and continually expanding. Early applications focused on tasks such as text summarization, translation, and sentiment analysis. However, the current generation of LLMs powers a much wider range of tools. These can be broadly categorized into several key areas. Conversational AI, in the form of advanced chatbots and virtual assistants, represents a significant segment of the market, enhancing customer service and user engagement \citep{GrandViewResearch2025}. Content creation is another major application area, where LLMs are employed to generate a variety of materials, from marketing copy and social media posts to technical documentation and even creative writing \citep{V7Labs2025}.            
            
            Furthermore, LLMs are being integrated into more specialized and high-stakes domains. In the legal field, they assist with tasks like contract analysis and legal research. The financial sector utilizes them for fraud detection and market analysis \citep{V7Labs2025}. In software development, LLM-powered tools for code generation and debugging are becoming increasingly prevalent, accelerating development cycles and improving programmer productivity. A key innovation driving the utility of these applications is the advent of techniques like Retrieval-Augmented Generation (RAG), which allows LLMs to retrieve and incorporate information from external knowledge bases, thereby improving the accuracy and relevance of their outputs \citep{KeywordsAI2025}. The ongoing development of multimodal LLMs is further pushing the boundaries of what is possible, enabling applications that can understand and reason about the world in a more holistic manner \citep{Kaddour2023}.
        
        \subsection{Retrieval-Augmented Generation (RAG)} 

            Retrieval-Augmented Generation (RAG) technique combines LLMs with information retrieval to generate accurate and up-to-date responses, as introduced by \citet{Lewis2020}. 
            \xexeo{Aqui merece um desenho ilustrativo, atÃ© para quebrar tanto texto}
            \vitor{Feito}
            It employs a search in a database to find relevant information, overcoming the inherent limitations of LLMs that rely solely on the prior knowledge embedded in the language model during the training phase. 
            With the ongoing evolution of information retrieval, which has moved from term-based methods to more semantic approaches leveraging deep learning and large datasets to tackle more complex challenges.
            
            A RAG consists of two main components: a retriever and a generator, as illustrated in Figure~\ref{fig:rag_diagram}. The retriever is responsible for finding relevant information from a knowledge base, and the generator uses that information to create a human-like response. 
            
            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.4\textwidth]{images/rag_diagram_vertical.png}
                \caption{A diagram illustrating the RAG process.}
                \label{fig:rag_diagram}
            \end{figure}         

            As elucidated by \citet{Lewis2020}, RAG unites the strengths of pre-trained parametric and non-parametric memory, using a dense vector index and a semantic retriever. 
            As demonstrated by \citet{Li2022} in their analysis, RAG is surpassing traditional generative models in terms of performance across a variety of tasks. The study provides a detailed survey on this topic, emphasizing the fundamental concepts and its applicability in specific contexts.

            New tools have been developed to facilitate the implementation of RAG solutions. \citet{Liu2023} present a toolkit that integrates augmented retrieval techniques into LLMs, including modules for query rewriting, document retrieval, passage extraction, response generation, and fact-checking, enabling the creation of more factual and specific responses. The recent study by \citet{Zhao2023} extends this horizon by examining the incorporation of multimodal knowledge into generative models, exploring the integration of diverse external sources such as images, code, tables, graphs, and audio, to enhance the grounding context and improve usability. It also explores potential future trajectories in this emerging field, marking a relevant contribution to the evolving narrative of RAG and its applications.

            
        \subsection{Multi-Agent Setup} 

            \xexeo{Isso aqui seria uma seÃ§Ã£o de multi agentes dentro da LLM, mas vocÃª precisa escrever pelo menos um parÃ¡grafo de multi agentes gerais na seÃ§Ã£o agentes}      
            \vitor{Feito. Inclui uma subseÃ§Ã£o sobre MAS.}

            As demonstrated by \citet{xi2023rise}, the pursuit of Artificial General Intelligence (AGI) has significantly benefited from the development of LLM-based agents, capable of sensing, decision-making, and acting across diverse scenarios.  
            His study outline a foundational framework for such agents, consisting of brain, perception, and action components, which can be customized for various applications including single-agent scenarios, multi-agent systems, and human-agent collaboration . 
            The comprehensive survey underscores the crucial role of LLMs in moving towards AGI, suggesting a promising horizon for operational efficiency and decision-making processes in complex organizational settings \citep{xi2023rise}.

            \citet{Li2024} demonstrated that, through a sampling and voting method, the performance of LLMs scales with the number of instantiated agents.
            Another open-source framework is AutoGen \citep{Wu2023}, that enables the creation of LLM multi-agent applications, allowing for customization across various modes including. It supports diverse applications in fields such as mathematics, coding, and operations research, demonstrating its effectiveness through empirical studies \citep{Wu2023}.

            
        \section{Evaluation} \label{sec:evaluation-review}

            \subsection{Truthfulness}

                In the evaluation of RAG systems, ensuring the truthfulness of the generated output is a primary concern. \citet{Lin2022} introduces a framework for this purpose. The authors define a truthful answer as one that aligns with literal truth about the real world. This is particularly relevant for RAG systems, which can retrieve and incorporate information from vast and varied sources. An answer is considered truthful if it does not assert any false statements, and informative if it provides relevant information that addresses the user's query.
                
                In \citet{Li2023}, the authors conducted an evaluation to determine the effectiveness of their proposed prompts on the performance of various LLMs. The evaluation employed both automated standard experiments and human studies to assess the impact of emotional stimuli on task performance, truthfulness, and responsibility.

                In the first experiment of this study, human experts assessed each Q\&A pair based on the definitions:

                \begin{quoting}[font={small,itshape},indentfirst=false]
                    \begin{itemize}
                    \item \textbf{Truthfulness}: a metric to gauge the extent of divergence from factual accuracy, otherwise referred to as hallucination \citep{Lin2021}.
                        \subitem 1=â€œThe response promulgates incorrect information, detrimentally influencing the ultimate interpretationâ€
                        \subitem 2=â€œA segment of the response deviates from factual accuracy; however,this deviation does not materially affect the ultimate interpretationâ€
                        \subitem 3=â€œThe response predominantly adheres to factual accuracy, with potential for minor discrepancies that do not substantially influence the final interpretationâ€
                        \subitem 4=â€œThe response is largely in consonance with factual evidence, albeit with insignificant deviations that remain inconsequential to the final interpretationâ€
                        \subitem 5=â€œThe response is in meticulous alignment with the facts, exhibiting no deviationsâ€
                                
                    \item \textbf{Performance}: encompasses the overall quality of responses, considering linguistic coherence, logical reasoning, diversity, and the presence of corroborative evidence.
                        \subitem 1 = â€œThe response fails to address the question adequatelyâ€
                        \subitem 2 =â€œThe response addresses the question; however, its linguistic articulation is sub-optimal, and the logical structure is ambiguousâ€
                        \subitem 3 = â€œThe response sufficiently addresses the question, demonstrating clear logical coherenceâ€
                        \subitem 4 = â€œBeyond merely addressing the question, the response exhibits superior linguistic clarity and robust logical reasoningâ€
                        \subitem 5 = â€œThe response adeptly addresses the question, characterized by proficient linguistic expression, lucid logic, and bolstered by illustrative examplesâ€\citep{Lin2021}.         
                    \end{itemize}
                \end{quoting}

            \subsection{Precision, Recall, and F1-Score}
                Precision, recall, and F1-score are fundamental metrics for evaluating classification tasks, particularly in scenarios with imbalanced datasets. These metrics provide a more nuanced understanding of a model's performance than accuracy alone.

                \textbf{Precision} measures the accuracy of positive predictions. It is the ratio of correctly predicted positive observations to the total predicted positive observations. A high precision relates to a low false positive rate.
                \begin{equation}
                    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                    \label{eq:precision}
                \end{equation}

                \textbf{Recall} (or Sensitivity) measures the ability of the model to find all the relevant cases within a dataset. It is the ratio of correctly predicted positive observations to all observations in the actual class.
                \begin{equation}
                    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                    \label{eq:recall}
                \end{equation}

                The \textbf{F1-score} is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is the harmonic mean of the two and is a good way to show that a model has a good performance on both metrics.
                \begin{equation}
                    \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                    \label{eq:f1-score}
                \end{equation}


        \subsection{LLM-as-judge}

            \xexeo{Ã’k, essa seÃ§Ã£o eu nÃ£o gostei muito, apesar de nÃ£o ter nenhum erro. Primeiro tem que fazer uma seÃ§Ã£o de avaliaÃ§Ã£o com as medidas, onde devem estar todas as medidas que vocÃª usar na dissertaÃ§Ã£o (nÃ£o li ainda, estou indo na ordem). AÃ­ entÃ£o vocÃª pode falar disso, mas apenas se usou}
            \vitor{Enxuguei esta seÃ§Ã£o e fiz a inclusÃ£o das demais mÃ©tricas utilizadas antes desta seÃ§Ã£o. Veja se estÃ¡ melhor.}

            The LLM-as-Judge paradigm represents a significant shift in the evaluation of NLP systems in general, using a language model as a scalable proxy for human evaluators (\citep{li2024llmsasjudgescomprehensivesurveyllmbased}). 
            This approach was developed to overcome the semantic shallowness of traditional metrics like BLEU or ROUGE and the logistical challenges of extensive human annotation (\citep{Zheng2023}). 
            By providing a "judge" LLM with a clear rubric and context, it can perform assessments of qualities like coherence, relevance, and factual accuracy (\citep{li2024llmsasjudgescomprehensivesurveyllmbased}).
            This method has proven effective for complex, open-ended tasks where simple string matching is insufficient, with models like GPT-4 demonstrating over 80\% agreement with human preferences in benchmarking studies \citep{Zheng2023}.

            For evaluating Retrieval-Augmented Generation (RAG) systems, the LLM-as-Judge framework can be adapted to produce structured, quantitative assessments. 
            In this application, the judge LLM is tasked with comparing the RAG-generated answer against a ground-truth dataset.
            By using a crafted prompt that defines the classification criteria, the judge can systematically categorize each output into classes such as True Positive (TP) (factually consistent with the ground truth), False Positive (FP) (introduces unsupported information), True Negative (TN) (a correct refusal to answer), or False Negative (FN) (missing relevant information). This approach moves beyond subjective scoring towards a more objective evaluation. The prompt used in this work is presented in the code in Appendix~\ref{code:llm-judge}.

            The advantage of this methodology is its ability to translate qualitative judgments directly into a confusion matrix, allowing the calculation of standard metrics such as precision (Equation~\ref{eq:precision}), recall (Equation~\ref{eq:recall}), and F1-score (Equation~\ref{eq:f1-score}). This process establishes a replicable pipeline for benchmarking the factual accuracy of a RAG system at scale. While it is important to acknowledge the potential for inherent biases in LLM judges (\citep{Gu2025}), studies show high correlation with human-expert evaluations (\citep{li2024llmsasjudgescomprehensivesurveyllmbased}), making it a useful tool for iterative development and system comparison.

\todo[inline]{Como vocÃª criou um dataset de teste para uma tarefa, seria bom falar disso. Mas essa Ã© uma coisa adicional a outras mudanÃ§as que pedi, seria bom, mas se nÃ£o der tempo, nÃ£o deu.
\newline \newline VITOR: O DATASET ENTRARIA NO CAP. 3 E 4 OU AQUI MESMO?}

\xexeo{VocÃª usou no prmeiro experimento outras mÃ©tricas, tem que descreve-las aqui: Truthfulness, Performance, LLMCost}
\vitor{Feito}

\chapter{First Experimental Evaluation Cycle}

    \xexeo{Todo capÃ­tulo deve ter uma introduÃ§Ã£o explanatÃ³ria. "This chapter describes"}
    \vitor{Feito.}

    % This chapter describes the first experimental cycle of this research, structured according to the DSR methodology, as detailed in Section~\ref{sec:dsr-application}. It situates the experiment within the established \textbf{context} of knowledge management in the well construction and maintenance domain of a major oil company. The core \textbf{problem} this experiment addresses is the need for an effective mechanism to query complex technical and operational information within large volumes of unstructured data. As a solution, this experiment proposes and implements two distinct \textbf{artifacts}: a single-agent system and a multi-agent architecture, both designed to leverage LLMs for responding to specialized user queries. Finally, the chapter details the \textbf{evaluation} of these artifacts, outlining the methodology, the dataset creation process, and the metrics (Truthfulness, Performance, and LLM Cost) used to assess their capabilities and limitations.

    \xexeo{Acho que pode atÃ© ser Primeiro Ciclo, ou pode ter um nome como ``Efetividada das LLMs na SoluÃ§Ã£o..''  vai ter que quebrar esse capÃ­tulo, que tem muita informaÃ§Ã£o nos conceitos da DSR: no mÃ­nimo nos quatro principais: Contexto, Problema, Artefato, AvaliaÃ§Ã£o. Grande parte do contexto e problema jÃ¡ devem ser descritos no capitulo que eu pedi para criar e aqui sÃ³ faz referÃªncia.}
    \vitor{O CapÃ­tulo foi refatorado para ficar alinhado com a DSR.}

    \xexeo{Ok, vocÃª foi direto para o experimento mas nÃ£o disse o que ia fazer. Aqui exatamente cabe o quadro da DSR que eu te mandei: qual o contexto, qual o problema, qual a suposiÃ§Ã£o (de utilidade ou de mundanÃ§a de contexto), qual os quadro teÃ³ricos, qual o(s) artefato(s) proposto(s) e como serÃ£o avaliados, vai fechar muito bem.}
    \vitor{O CapÃ­tulo foi refatorado para ficar alinhado com a DSR.}

    
    \xexeo{isso aqui Ã© a validaÃ§Ã£o, mas qual Ã© o problema, qual a proposta, sÃ£o essas informaÃ§Ãµes que faltam para ficar bem organizado} 
    \vitor{O CapÃ­tulo foi refatorado para ficar alinhado com a DSR.}

    This chapter describes the first experimental cycle of this research, as introduced in Section~\ref{sec:dsr-application}, conducted to investigate the effectiveness of different LLM based agent architectures. The primary objective is to address complex, domain-specific queries within the field of well construction and maintenance. This initial cycle serves as a foundational study, comparing single-agent and multi-agent systems to generate empirical insights into their performance, cost, and inherent limitations. The findings from this cycle will inform the more advanced, quantitative evaluation performed in the second experiment.

    Following the principles of DSR, this chapter is structured to clearly present the research components. We will begin by defining the business context and the specific problem this experiment aims to solve. Subsequently, we will describe the design of the proposed technological solutions, referred to as artifacts. Finally, we will detail the evaluation methodology, including the process for data set creation, the metrics used for assessment, and a thorough analysis of the results.
    
    \section{Design Science Research Framework}
    
        To provide a clear and organized structure for this experiment, we adopt the DSR framework. The key components of this research cycle are outlined as follows:

        \begin{description}
            \item[Context] The operational environment of the well construction department within a major oil company, where efficient access to technical knowledge is critical.

            \item[Problem] The challenge faced by engineers and specialists in effectively querying and retrieving accurate information from vast, unstructured, and domain-specific knowledge bases (e.g., operational reports, lessons learned).

            \item[Supposition] Our core supposition is that LLM-based agent systems can improve the efficiency and accuracy of information retrieval for specialized tasks, but that the choice of architecture (single-agent vs. multi-agent) will have a measurable impact on performance and cost.

            \item[Theoretical Frameworks] This work is grounded in the theories of Intelligent Agents, Retrieval-Augmented Generation (RAG), and multi-agent systems, as detailed in the Literature Review.

            \item[Proposed Artifacts] Two distinct LLM-based agent systems are proposed and built:
            \begin{itemize}
                \item A Single-Agent Architecture.
                \item A Multi-Agent Architecture.
            \end{itemize}

            \item[Evaluation] The artifacts are evaluated by a panel of domain experts who assess the quality of their responses to a curated set of real-world queries. The evaluation is based on predefined metrics for truthfulness, performance, and cost.
        \end{description}

    \section{Context and Problem Statement}

        \subsection{Context}

            As established in the Introduction, this research is situated within the oil and gas industry, a sector characterized by complex, expensive operations. This experiment was carried out specifically within the well construction department of a major oil company. In this environment, engineers and technical staff frequently need to access specialized information from a variety of internal data sources, including operational reports, learned lessons, and safety alerts. The efficiency and accuracy of this information retrieval process directly impact operational decision-making, safety, and cost-effectiveness.

            The set of queries used to test the systems
            % , listed in Appendix~\ref{app:dataset}, 
            provides a concrete exemplification of the problem space.

    \section{Proposed Artifacts}

            To address the problem, we designed, built, and tested two distinct artifacts: a single-agent solution and a multi-agent solution. Both are goal-based agents designed to accurately respond to user queries by leveraging a suite of tools.

        \subsection{Single-Agent Architecture.}

            In this work, a goal-based agent \citep{Russell2020} was implemented with the goal of accurately responding to various queries. 
            The agent operates within an environment equipped with multiple tools for task-specific operations, as shown in Figure~\ref{fig:agent_environment}, and interfaces with users to receive queries.
            
            \begin{figure}[h]
                \centering
                \includegraphics[width=0.75\textwidth]{images/agent_environment_4.png}
                \caption{Schematic of the LLM-based agent interacting with an environment containing tools for task-specific operations, and the Human Agent interface for user interaction and feedback.}
                \label{fig:agent_environment}
            \end{figure}           
            
            Initially, a configuration of agents was implemented as described in Figure~\ref{fig:agent_config_1} using AutoGen Framework \citep{Wu2023} with an architecture that allows information retrieval and user interaction. This system consists of two agentic setups:

            \begin{figure}[h]
                \centering
                \includegraphics[width=.5\textwidth]{images/agent_config_1.png}
                \caption{Chat setup with one User Proxy \citep{Wu2023} and one Assistant.}
                \label{fig:agent_config_1}
            \end{figure}

            \begin{itemize}        
                        
                \item \textbf{User Proxy:} represents the interface with the user and with tools to access external databases. The modular nature of the tools allows the User Proxy to be customized and expanded based on the variety of data sources and the specific requirements of the application domain.

                \item \textbf{Agent:} powered by LLMs such as GPT-4 and GPT-3 (the specific model is configurable), is the analytical engine of the system. This agent interprets the queries received from the User Proxy and formulates responses.
                                    
            \end{itemize}

            
            For each question in the data set, the agent's decision-making process is executed as described in Figure~\ref{fig:diagrama_agente_1}, initially selecting the appropriate tool to respond to a query and, finally, compiling the retrieved information to provide a final answer.

            \begin{figure}[h]
                \centering
                \includegraphics[width=0.75\textwidth]{images/agent_diagram_1.png}
                \caption{Decision process of the agent.}
                \label{fig:diagrama_agente_1}
            \end{figure}

        \subsection{Multi-Agent Architecture}

            The second artifact is a multi-agent system where responsibility is distributed among several specialized agents, coordinated by a Chat Manager, as shown in Figure~\ref{fig:agent_config_2}. This architecture is designed to handle queries by routing them to the agent best equipped for the task. As depicted in the decision process in Figure~\ref{fig:diagrama_agente_MultiAgente_2}, a \enquote{speaker selection} step determines the most suitable agent to act at each turn, promoting a more focused and contextualized approach to problem-solving.

            % (Your Figure agent\_config\_2 would be Figure 3.3 here)            
            \begin{figure}[h]
                \centering
                \includegraphics[width=.75\textwidth]{images/agent_config_2.png}
                \caption{Chat setup with one Chat Manager and a group of LLM agents.}
                \label{fig:agent_config_2}
            \end{figure}
            
            % (Your Figure diagrama\_agente\_MultiAgente\_2 would be Figure 3.4 here)
            \begin{figure}[h]
                \centering
                \includegraphics[width=1\textwidth]{images/agent_diagram_2.png}
                \caption{Multi-agent decision process.}
                \label{fig:diagrama_agente_MultiAgente_2}
            \end{figure}

                

        \subsection{Agent's Tools}
            
            In this experiment, three tools were considered in the decision-making process:

            \begin{itemize}            
                
                \item \textbf{Tool 1 - Knowledge Items Search:} a tool to search for learned lessons that may be relevant to the query. 
                \label{Tool1}
        
                \item \label{Tool2} \textbf{Tool 2 - Employee Search:} functionality that allows the search for information related to collaborators of an organization.
        
                \item \label{Tool3} \textbf{Tool 3 - NPT SQL Query:} Interface for executing SQL queries on a database of operational NPTs.    
                
            \end{itemize}

            There is also a pathway that allows the agent to provide a direct response, without the need to resort to other tools, presumably used when the LLM already possesses the necessary information.

    \section{Evaluation}

        The evaluation phase was designed to assess and compare the performance of the two proposed artifacts. This section details the methodology, the data set creation process, the metrics used, and the final results.

        \subsection{Evaluation Methodology}
        
            The evaluation was conducted by presenting a standardized set of questions to both the single-agent and multi-agent systems, using both GPT-3.5-turbo and GPT-4 models. The responses generated by each configuration were then collected and anonymized.

            A panel of three specialist engineers from the well construction department was tasked with analyzing the generated answers. Each specialist independently scored the responses based on the metrics described in Section~\ref{sec:evaluation_metrics}. The final score for each response was calculated by averaging the scores from the three experts, ensuring a robust and comprehensive assessment.
            
            \xexeo{Aqui seria bom fazer um BPMN do passo a passo do seu experimento, veja a figura 4.1 de\url{https://www.cos.ufrj.br/uploadfile/publicacao/3172.pdf}}
            \vitor{Feito}

            To provide a clear visual representation of the experimental workflow, a Business Process Model and Notation (BPMN) diagram is presented in Figure~\ref{fig:experimental_workflow}. This diagram illustrates the step-by-step process, from query submission to expert evaluation.

            \begin{figure}[h]
                \centering
                \includegraphics[width=\textwidth]{images/bpmn_experimento_1.png}
                \caption{Experimental workflow.}
                \label{fig:experimental_workflow}
            \end{figure}

            
        \subsection{Data Set Creation}

            A critical component of this evaluation is the test dataset. The dataset was meticulously created to reflect authentic information needs within the well construction domain. The process was as follows:

            \begin{description}
                \item[Source Selection] We identified three primary internal data sources: a database of Operational Knowledge Items (lessons learned, alerts), a structured database of Non-Productive Time (NPT) incidents, and a Collaborator Finder tool, as described in Section~\ref{sec:information-sources}.
                \item[Document Sampling] A random sample of documents and records was selected from each data source to ensure broad coverage of topics and scenarios.
                \item[Query Formulation] This process was performed by the author, leveraging domain expertise and collaboration with colleagues to ensure the questions were realistic, relevant, and challenging.
                \item[Dataset Composition] In total, a dataset of 33 unique queries was created. 
            \end{description}

            This approach to dataset creation, grounded in author experience and real-world documents, provides a valid basis for evaluating the artifacts. Table~\ref{table:question_examples} presents a sample of the queries formulated for the experiment.

            \xexeo{Coloca todas na tabela! E faz uma seÃ§Ã£o de criaÃ§Ã£o de perguntas, ou subseÃ§Ã£o}
            \vitor{Feito}

            
            \begin{table}[h]
                \centering
                \scriptsize
                \sloppy
                \begin{tabular}{|p{.1\linewidth}|p{.9\linewidth}|}
                \hline
                \textbf{Task category} & \textbf{Question} \\   \hline
                \multirow{17}{*}{Q\&A} & How does the presence of silica in the composition of cement 
                paste affect its thermal stability at high temperatures? \\ \cline{2-2}
                & What are the main challenges and risks associated with through tubing plug and abandonment in highly deviated wells? \\ \cline{2-2}
                % & What can cause hydrate formation in the Tree Running Tool  connector during the HCR (High Collapse Resistance) hose flush  before connecting to the Wet Christmas Tree? \\ \cline{2-2}
                % & What can cause the Down Hole Safety Valve to remain open  due to hydrate formation in the control lines? \\ \cline{2-2}
                % & What can cause damage to thread protectors and sealing  areas of pin ends of pipes stored at the coating yard? \\ \cline{2-2}
                % & What can cause high drag and torque off-bottom during  the drilling of a well with high deviation? \\ \cline{2-2}
                % & What precautions should be taken when performing a top check  of the abandonment plug in wells with higher inclination? \\ \cline{2-2}
                % & What are the critical factors to consider when choosing a base  fluid for manufacturing a viscous support plug? \\ \cline{2-2}
                % & What are the best practices for managing drilling parameters  during cement cutting to avoid premature bit wear? \\ \cline{2-2}
                & Give me all the information about employee BFD1. \\ \cline{2-2}
                & Who are the employees of the POCOS/EP/SASD team? \\ \cline{2-2}
                & How many advisors do we have in the POCOS/SPO department? \\ \cline{2-2}
                & Who are the advisors in the departments belonging to the POCOS/EP department? \\ \cline{2-2}
                & What data sources do you have? \\ \cline{2-2}
                & What functions do you have? \\ \cline{2-2}
                & How does well inclination affect the effectiveness of cementing during through-tubing plugging? \\ \cline{2-2}
                & What can cause difficulty in locking the handling cap of the coiled tubing BOP? \\ \cline{2-2}
                & What can cause anomalous behavior of the AutoTrak with GunDrill during drilling? \\ \cline{2-2}
                & What can be done to optimize the assembly of COP/COI for parallel movement of the JRC/THRT? \\ \cline{2-2}
                & What strategies can be adopted to improve the quality of cementing in highly inclined wells during through-tubing plugging? \\ \cline{2-2}
                & What are the alternatives to accelerate the curing time of cement slurry without compromising its integrity in high-temperature conditions? \\ \cline{2-2}
                & What are the risks associated with the improper substitution of cement with silica for pure cement in surface casing cementations in high-temperature wells? \\ \cline{2-2}
                & What was the strategy adopted to allow the passage of eccentric and/or large-diameter elements through the BOP quickly and without wedging the string with these elements inside the BOP? \\ \cline{2-2}
                \hline                
                \multirow{15}{*}{Text-to-SQL} & What was the longest-lasting NPT on rig number 05? \\ \cline{2-2}
                & How many NPTs occurred on rig number 06 during August 2023? \\ \cline{2-2}
                & What were the 5 most common abnormalities across all rigs? \\ \cline{2-2}
                & What were the abnormalities that occurred on all rigs during the week of September 14th to 20th, 2023? \\ \cline{2-2}
                & Which rigs had the most lost time in 2023? Give me a table with the rigs and the sum of hours. \\ \cline{2-2}
                & Which rigs had the most lost time in the first half of 2023? \\ \cline{2-2}
                & What were the latest abnormalities that occurred on the SS-70 rig? \\ \cline{2-2}
                % & What was the longest-lasting abnormality on the SS-70 rig? \\ \cline{2-2}
                & What was the peak of abnormality occurrences on the NS-52 rig? \\ \cline{2-2}
                & What was the total lost time in hours for abnormalities whose description mentions the term "Coiled Tubing"? \\ \cline{2-2}
                & What was the total lost time in hours on the NS-38 rig in 2023? \\ \cline{2-2}
                & What was the total time lost due to equipment failure on the NS-38 rig in 2023? \\ \cline{2-2}
                % & How many abnormalities occurred on the NS-31 rig during August 2023? \\ \cline{2-2}
                & How many abnormalities occurred on the NS-31 rig during July 2023? \\ \cline{2-2}
                & How many hours of lost time were caused by human error on the NS-47 rig in 2023? \\ \cline{2-2}
                & How many hours of lost time occurred on the MS-20 rig during June 2024? \\ \cline{2-2}
                & How many hours of lost time occurred on the NS-35 rig in 2024? \\
                \hline
                \end{tabular}
                \fussy
                \caption{Queries used in first cycle. }
                \label{table:question_examples}
            \end{table}

        \subsection{Evaluation Metrics} \label{sec:evaluation_metrics}

            To ensure a comprehensive assessment, the expert panel evaluated the artifacts' responses using the following metrics, which are based on the definitions presented in Section~\ref{sec:evaluation-review}:

            \begin{itemize}

                \item \textbf{Truthfulness}: A 1-5 Likert scale score measuring the factual accuracy of the response and the extent of any divergence from the ground truth. A higher score indicates a more factually correct answer with no hallucinations.

                \item \textbf{Performance}: A 1-5 Likert scale score assessing the overall quality of the response, including its linguistic coherence, logical structure, relevance, and conciseness.

                \item \textbf{LLM Cost}: A quantitative metric representing the financial cost in US dollars (USD) to generate a response for a given query using the OpenAI API. This reflects the computational expense and efficiency of each configuration. While other costs exist (development, infrastructure, maintenance), the API cost is a primary operational expenditure that scales directly with usage and is therefore a key metric for evaluating the economic viability of the artifacts, as established in our DSR framework.
            
            \end{itemize}

            To illustrate the application of the first two metrics, an example of an expert evaluation is provided in Table~\ref{tab:tabela_inputs_example}. The table shows the responses of different models to the query: "How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures?". It details the scores for Truthfulness and Performance, along with the specialists' comments that justify the ratings.

            \xexeo{Por que essa pergunta? Isso Ã© um exemplo? Como o multi agente entrou aqui se ainda nÃ£o falou de multi agente? Melhor deixar para mais tarde ou mostrar sÃ³ para single-agent?} 
            \vitor{Refatorado pra deixar claro q esse eh um exemplo e inserido apÃ³s a explicaÃ§Ã£o das arquiteturas.}
            % This 
            \xexeorem[comprehensive]{OLHA UMA PALAVRA INÃšTIL TÃPICA DE LLM} 
            % comparison allows for an 
            \xexeorem[in-depth]{Outra} 
            % assessment of the models' capabilities in generating accurate and relevant responses to technical questions.
            \vitor{Essa parte com palavras tÃ­picas de llm foi escrita no artigo, inÃ­cio do ano passado, quando essas expressÃµes ainda nÃ£o eram indesejÃ¡veis kkkk estou retirando tudo. Se ainda tiver passado algo, me avise pfv.}


        \begin{landscape}
            \begin{table}[h]
            \small
            \centering
            \caption{Query example with inputs, outputs, and human expert evaluations.}
            \label{tab:tabela_inputs_example}
            \begin{tabular}{| m{3cm} | m{1.5cm} | m{1.5cm} |>{\scriptsize}m{9cm} | m{1.3cm} | m{1.2cm} |>{\scriptsize}m{4cm} |}                    
            \toprule
            % \textbf{Input query} & \textbf{Model} & \textbf{Setup} & \textbf{Final output} & \textbf{Truthfulness} & \textbf{Performance} & \textbf{Specialist \ Comments} \\
            Input query & Model & Setup & Final output & Truth. \newline fulness & Perfor- \newline mance & Specialist \ Comments \\
            \midrule
            How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures? & GPT-3.5-turbo & Single-Agent & The presence of silica in the composition of cement paste can affect its thermal stability at high temperatures due to some characteristics of silica. Silica is an inorganic material that has a stable crystalline structure at high temperatures. When exposed to high temperatures, the silica present in the cement paste can undergo structural transformations, resulting in changes in the properties of the paste.

            One of the main changes that can occur is the thermal expansion of the cement paste due to the formation of unstable silica compounds. This thermal expansion can lead to cracks and fissures in the structure, compromising the material's integrity and stability.

            Moreover, the presence of silica in the composition of cement paste can affect the material's thermal resistance. Silica has low thermal conductivity, which can lead to a reduced ability of the cement to withstand high temperatures. This can result in a decrease in the material's strength and, consequently, a reduction in its thermal stability.

            In summary, the presence of silica (...) & 4 & 5 & Created a section that is irrelevant to the question: "Moreover, the presence of silica in the composition of cement paste can affect the material's thermal resistance. (...)" \\
            \cline{3-7}
            &  & Multi-Agent & The presence of silica in the composition of cement paste has a positive effect on thermal stability at high temperatures. Silica is added to cement to mitigate the retrogression of cement strength when exposed to temperatures above 230Â°F (110Â°C). Deep wells or those with a high geothermal gradient may require the use of silica cement to ensure the thermal stability of the paste. The use of silica cement is particularly important in surface casing cementing, where the temperature can exceed the critical limit. \ (...) & 2 & 2 & Retrieved an unrelated Lesson Learned on the subject (about plugs). \\ 
            \cline{2-7}
            & GPT-4 & Single-Agent & The presence of silica in the composition of cement paste increases thermal stability, especially at high temperatures. Silica is used to transform the cement into a refractory material, capable of withstanding extreme conditions. 
            
            The cement (...).

            However, (...). & 3 & 4 & Although it is true, the item on the side is not relevant to the question: "However, it is important to note that (...)." \\
            \cline{3-7}
            &  & Multi-Agent & Silica is added to cement at a ratio of 35\% by weight of cement to mitigate the retrogression (loss of compressive strength) of the cement in the well when subjected to temperatures above 230ÂºF (110ÂºC). \ (...) & 5 & 4 & Cited an unrelated (but true) Lesson Learned about plugs, Lesson Learned LA7005. \\
            \hline
            \end{tabular}
            \end{table}
        \end{landscape}
            

        % \subsection{Results and Discussion}

            % (This section would contain the complete text from your original "Results" and "Discussion" sections. The content is good, but it logically belongs here as the final part of the DSR evaluation phase. You can simply move that text here, ensuring it flows from the description of the metrics.)


        \subsection{Results} % NEW SUBSECTION for your results
            % The results table and the sections analyzing each metric go here.
            % The original \subsection{Truthfulness}, \subsection{Performance},
            % and \subsection{LLM Cost} become \subsubsections.

            % The third metric, LLM Cost
            \xexeo{Isso aqui Ã© uma pergunta de pesquisa tem que entrar de alguma maneira na definiÃ§Ã£o do DSR, lembrando que as avaliaÃ§Ãµes do DSR podem ser mais de uma}
            \vitor{Feito. Movido p/ definiÃ§Ã£o do DSR}
            % , is
            \xexeo{NÃ£o Ã© represents, jÃ¡ que Ã© o custo mesmo, acho que  corresponds to, ou mesmo sÃ³ is }
            \vitor{Feito} 
            % the financial cost associated with using OpenAI's API for the language models in each configuration. This metric is measured in US dollars and reflects the computational resources required for each task.
            \xexeo{Tem que falar alguma coisa que nÃ£o Ã© o Ãºnico custo, e quais sÃ£o os outros e porque esse Ã© importante, isso pode estar descrito no modelo DSR, antes}
            \vitor{Feito}
            \xexeo{Esse parÃ¡grafo tipicamente aparece na revisÃ£o}


            This section provides an analysis of the data collected during the first experimental cycle. The aggregated results are presented in Table~\ref{tab:tabela_resultados}, followed by a discussion of each evaluation metric established in our DSR framework: Truthfulness, Performance, and LLM Cost.

            \begin{table}[h]
                \small % Reduce the font size
                \centering % Center the table on the page
                \caption{Results on Q\&A and Text-to-SQL tasks, including standard deviation (Std). The best metrics are highlighted with \textbf{\underline{bold and underline}}. The second best are highlighted with \textbf{bold}.}
                \label{tab:tabela_resultados}
                \begin{tabular}{|>{\raggedright\arraybackslash}p{2.0cm}|>{\centering\arraybackslash}p{0.85cm}|>{\centering\arraybackslash}p{0.95cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.85cm}|>{\centering\arraybackslash}p{0.95cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|}
                \hline
                \rowcolor{gray!20}
                \textbf{Task}           & \multicolumn{5}{c|}{\textbf{Single-Agent}}           & \multicolumn{5}{c|}{\textbf{Multi-Agent}} \\ % Merging cells and adding heading
                \textbf{Model}          & \textbf{LLM Cost} & \textbf{Truth.} & \textbf{Std} & \textbf{Perf.} & \textbf{Std} & \textbf{LLM Cost} & \textbf{Truth.} & \textbf{Std} & \textbf{Perf.} & \textbf{Std} \\ \hline
                \cellcolor{gray!20} Q\&A & & & & & & & & & &\\
                GPT-3.5-turbo            & 0.005             & 2.94              & 1.48 & 3.94          & 1.09 & 0.02              & 4.09              & 1.22 & 3.82 & 0.98 \\
                GPT-4                   & 0.12              & \textbf{3.88}     & 1.41 & \textbf{4.06} & 1.30 & 0.45              & \underline{\textbf{4.57}} & 0.79 & \underline{\textbf{4.43}} & 0.79 \\
                \cellcolor{gray!20} Text-to-SQL & & & & & & & & & &\\
                GPT-3.5-turbo            & 0.009             & 4.13              & 1.41 & 4.44          & 1.03 & 0.02              & \textbf{4.29}     & 1.20 & \textbf{4.29} & 1.33 \\
                GPT-4                   & 0.10 & \underline{\textbf{4.56}} & 0.96 & \underline{\textbf{4.63}} & 0.81 & 0.51      & 3.20              & 1.99 & 3.70 & 1.89 \\ \hline
                \end{tabular}
            \end{table}

            The comparative analysis between single and multi-agent setups for RAG, using GPT-3.5-turbo and GPT-4 models, revealed insights regarding the metrics of truthfulness, performance, and costs of the language model.


            \subsubsection{Truthfulness} 

                In assessing the truthfulness metric, significant differences are noted between the single and multi-agent settings in both Q\&A and Text-to-SQL tasks. The results are illustrated in Figures \ref{fig:truthfulness_QA} and \ref{fig:truthfulness_text2sql}.
                For Q\&A tasks, GPT-4 in a multi-agent configuration significantly exceeded the performance of the single-agent with a truthfulness score of 4.57 compared to 3.88. The GPT-3.5-turbo model showed distinct results between the two configurations, with the multi-agent surpassing the single-agent with scores of 4.09 and 2.94, respectively.
                In terms of Text-to-SQL queries, a different outcome was observed. GPT-4 single-agent achieved a score of 4.56, while the same model in the multi-agent configuration obtained 3.20, highlighting a limitation for the multi-agent in this task. Conversely, the GPT-3.5-turbo maintained a more balanced performance between configurations, scoring 4.29 for multi-agent and 4.13 for single-agent.
                
                \begin{figure}[h]
                    \centering
                    \begin{minipage}{.48\textwidth}
                        \centering                
                        \includegraphics[width=1\linewidth]{images/truthfulness_QA.png}
                        \caption{Truthfulness and standard deviation in Q\&A tasks by LLM model and agent configuration.}
                        \label{fig:truthfulness_QA}
                    \end{minipage}%
                    \hspace{0.2cm}
                    \begin{minipage}{.48\textwidth}
                        \centering
                        \includegraphics[width=1\linewidth]{images/truthfulness_text2sql.png}
                        \caption{Truthfulness and standard deviation in Text-to-SQL tasks by LLM model and agent configuration.}
                        \label{fig:truthfulness_text2sql}
                    \end{minipage}
                \end{figure}

                
            \subsubsection{Performance}        

                The evaluation of LLM performance \citep{Li2023} in the tasks of Q\&A and Text-to-SQL reveals trends which are similar to the truthfulness results. 
                % As shown in Figures \ref{fig:performance_QA} and \ref{fig:performance_text2sql} and summarized in \ref{tab:tabela_resultados}, the text performance in single and multi-agent setups was compared using the GPT-3.5-turbo and GPT-4 models.        
                For Q\&A tasks, the multi-agent setup shows a performance boost compared to the single-agent setup. In particular, the multi-agent GPT-4 achieves a performance score of 4.43, which is higher than the single-agent GPT-4 score of 4.06. This pattern is consistent with the GPT-3.5-turbo, where the multi-agent system also surpasses the single-agent system, scoring 3.82 and 3.94, respectively. These findings emphasize the effectiveness of the multi-agent approach in handling technical user queries.
                        
                \begin{figure}[h]
                    \centering
                    \begin{minipage}{.48\textwidth}
                        \centering                
                        % \framebox{
                            \includegraphics[width=1\linewidth]{images/performance_QA.png}
                        % }
                        \caption{Performance and standard deviation in Q\&A tasks by LLM model and agent configuration.}
                        \label{fig:performance_QA}
                    \end{minipage}
                    \hspace{0.2cm}
                    \begin{minipage}{.48\textwidth}
                        \centering
                        % \framebox{
                        % \includegraphics[width=1\linewidth]{images/performance_text2sql.png}
                        % }
                        \caption{Performance and standard deviation in Text-to-SQL tasks by LLM model and agent configuration.}
                        \label{fig:performance_text2sql}
                    \end{minipage}%
                \end{figure}


            \subsubsection{LLM Cost} 
                Language model services are typically composed by a values per token. For instance, GPT-4 model costs US\$30.00 (input) and US\$60.00 (output) per 1 million tokens received and sent, respectively.        
                The single-agent architecture demonstrated substantially lower costs for both Q\&A and Text-to-SQL tasks compared to the multi-agent setup as shown in Figure~\ref{fig:truthfulness_vs_cost_vs_config_model}. For instance, the average cost of the GPT-4 model \citep{OpenAI2023} for a Q\&A task was \$0.12 per processed question for the single-agent, while the multi-agent recorded an average cost of \$0.45. This trend of higher costs for the multi-agent architecture was also maintained for Text-to-SQL tasks, with an average cost of \$0.51 for the multi-agent architecture in contrast to \$0.10 for the single agent.
                The higher token count and cost for multi-agent setting is due to the inclusion of intermediate calls, for example, when the "Agent Selector" needs to decide which agent to pass the turn to. All the message history is passed to the LLM at this stage, substantially increasing the number of tokens submitted and response time.


                \begin{figure}[h]
                    \centering              
                    % \framebox{
                        \includegraphics[width=0.75\textwidth]{images/truthfulness_vs_cost_vs_config_model.png}
                    % }
                    \caption{Average LLM costs and Truthfulness per completed task according to setup and model.}
                    \label{fig:truthfulness_vs_cost_vs_config_model}
                \end{figure}
                
                

        \subsection{Discussion} % NEW SUBSECTION for your discussion
            % The original \section{Discussion} and all its content go here.
            % The original \subsections become \subsubsections.

            
            The comparison between single and multi-agent systems revealed significant differences in terms of performance and cost:
            
            \subsubsection{General Performance.}     
                The results indicate that for Q\&A tasks in the context of O\&G, truthfulness measure was 28\% higher with the multi-agent architecture compared to single. 
                However, for Text-to-SQL tasks, this trend was inverted, where the single-agent scored 15\% higher.

                These findings suggest that for Q\&A tasks, the multi-agent setup may be more advantageous in terms of providing truthful information, particularly when utilizing the more advanced GPT-4 model. 
                Conversely, in Text-to-SQL tasks, the GPT-4 model in a single-agent configuration proved more effective. 
                This might imply that the added complexity of managing multiple agents in some tasks does not necessarily lead to improved performance in responses, underscoring the importance of carefully selecting the agent configuration based on the task type and specific features of the language model used.
                    
            \subsubsection{Cost-Performance Analysis.}
                While the multi-agent system shows higher truthfulness in Q\&A tasks, it is crucial to consider the associated costs. 
                To provide a clearer comparison, let us consider the score/cost ratios. For Q\&A tasks using GPT-4, the single-agent configuration yields a ratio of 32.33 truthfulness points per dollar, compared to 10.16 for the multi-agent setup. This indicates that while the multi-agent system shows a 17.8\% improvement in truthfulness, it comes at a 275\% increase in cost.
                
                % Based on our analysis, we recommend using a multi-agent system for Q\&A tasks when the budget allows for it and accuracy is a critical factor. 
                % However, decision-makers should consider setting a cost-performance threshold to guide the choice of system configuration, ensuring that the benefits justify the expenses involved.
                \xexeo{TEm que deduzir a necessidade de fazer um experimento antes levando essas coisas em consideraÃ§Ã£o}
                \vitor{Feito abaixo.}

                This trade-off highlights an important implication for any organization considering the adoption of these technologies. The optimal architecture is not universal; it is highly dependent on specific task requirements and budget constraints. 
                This reality underscores the necessity of conducting a preliminary, cost-performance evaluation. Rather than simply selecting a model, decision-makers must first perform a targeted analysis to establish a cost-benefit threshold. 
                Our work not only provides initial data for the O\&G domain but also demonstrates a foundational methodology for this evaluation process, which ultimately motivated the more rigorous and quantitative approach of our second experimental cycle.


            \subsubsection{Model Performance Variations.}
                Interestingly, our results show that GPT-3.5-turbo outperforms GPT-4 in certain tasks, particularly in the Text-to-SQL multi-agent configuration, despite GPT-4's larger size and more extensive training. 
                This unexpected performance could be attributed to several factors. 
                First, GPT-3.5-turbo may have undergone more specific fine-tuning for structured query tasks, allowing it to excel in Text-to-SQL scenarios. 
                Additionally, GPT-3.5-turbo's training data might be more recent or more relevant to the specific domain of our study. 
                Another possibility is that the smaller model size of GPT-3.5-turbo allows for faster processing and more efficient handling of the multi-agent setup, resulting in better performance in some contexts.

                However, it is important to note that GPT-4, when used in a multi-agent setup, demonstrated more consistent truthfulness and performance, as evidenced by its reduced standard deviation in results. 
                This consistency can be particularly advantageous in applications where reliability and accuracy are critical. 
                Multi-agent systems have the advantage of maintaining separate contexts for different aspects of a task \citep{Langchain2025}. 
                \xexeo{VocÃª pode suportar essa afirmaÃ§Ã£o com uma citaÃ§Ã£o?}
                \vitor{Feito.}
                This compartmentalization can lead to better handling of complex, multi-faceted queries, as each agent can focus on its specific context without being overwhelmed by irrelevant information. However, this advantage may be offset in tasks like Text-to-SQL, where maintaining a unified context of the database schema and query structure is crucial, possibly explaining the better performance of single-agent setups in this task.
                Furthermore, the multi-agent architecture inherently involves multiple stages of information processing, which can serve as natural filtering mechanisms.
                As information passes from one agent to another, irrelevant or low-quality data may be naturally filtered out, leading to more refined and accurate final outputs. 
                This could explain the superior performance in filtering irrelevant information observed in multi-agent setups.
            
            
            \subsubsection{Economic Efficiency.} 
            
                The multi-agent architecture incurs significantly higher costs compared to the single-agent system, primarily due to additional intermediate calls to the language model and multiple iterations between agents for action planning. 
                Also, the cost differences between using GPT-4 and GPT-3.5-turbo are substantial, with GPT-4 being 20 times more expensive (in early 2024).
                \xexeo{Dizer x vezes mais caro em julho de 2025}.
                \vitor{Feito}

                The average cost per query for each configuration is presented in Table \ref{tab:cost_per_query}. These figures highlight the direct cost implications of the chosen architecture and model.
                
                \begin{table}[h!]
                \centering
                \caption{Average LLM Cost Per Query (USD). Values from early 2024.}
                \label{tab:cost_per_query}
                \begin{tabular}{l r}
                \toprule
                \textbf{Configuration} & \textbf{Cost per Query} \\
                \midrule
                Single-Agent (GPT-3.5-Turbo) & \$0.0068 \\
                Single-Agent (GPT-4) & \$0.1095 \\
                Multi-Agent (GPT-3.5-Turbo) & \$0.0197 \\
                Multi-Agent (GPT-4) & \$0.4896 \\
                \bottomrule
                \end{tabular}
                \end{table}

                To illustrate the financial implications of adopting different models and architectures, we estimate the annual costs for a large company with 40,000 knowledge workers. Our calculations are based on an average of 5 queries per worker per day, over 250 working days per year.
                
                Under these assumptions, the total annual query volume is 50 million (40,000 workers $\times$ 5 queries/day $\times$ 250 days). For a single-agent configuration, this results in an annual cost of approximately \$337,843 for GPT-3.5 and \$5.47 million for GPT-4.
                
                In a multi-agent architecture, the costs increase substantially, escalating to approximately \$986,631 for GPT-3.5 and \$24.48 million for GPT-4. These estimates underscore the significant financial trade-offs when adopting a multi-agent system, which, while potentially offering performance benefits, comes with a considerable increase in LLM operational costs.

                While multi-agent systems and more advanced models like GPT-4 offer improvements in performance, the economic efficiency, as measured by truthfulness per dollar, may favor single-agent systems and less costly models like GPT-3.5-turbo, depending on the specific application and budget constraints.

                It is important to note that, as of July 2025, the landscape of LLMs has evolved substantially. The emergence of more efficient models, has led to a significant decrease in API's costs. This suggests that the financial trade-offs discussed previously may no longer be as pronounced, and that high-performance multi-agent systems could become economically viable much sooner than anticipated.

                \xexeo{In summary Ã© o parÃ¡grafo tÃ­pico das LLMs... Mas Ã© isso mesmo. PorÃ©m tem que colocar um ponto: o custo dos modelos estÃ¡ caindo barbaramente com o aparecimento de novos modelos no topo de desempenho e novas tecnologias tem permitido alcanÃ§ar resultados de Ã³tima qualidade com mÃ¡quinas muito menores, o que tambÃ©m derruba o custo. Pode atÃ© citar o exemplo do DeepSeek (buscando na literatura o desempenho x custo dele)}
                \vitor{Feito}
                
            
            \subsubsection{Challenges and Limitations}     
                During the evaluation of the agents, several challenges and limitations were identified.

                \textbf{\textit{Contextualization and Interpretation.}} 
                    In many cases, the single-agent solution had difficulty understanding the context of the question. For example, a question about cementing was interpreted in the context of the construction industry, a theme to which the language models were more exposed during the training phase. 
                    However, the multi-agent structure, with its well-defined roles, better understood the questions and showed superior performance in Q\&A tasks, corroborating the findings of \citep{Li2024}.
                
                \textbf{\textit{Filtering Irrelevant Information.}} 
                    The agent often receives irrelevant documents along with important ones in the prompt context, and it is up to the LLM to ignore these. 
                    For example, when asked about alternatives to accelerate the curing time of cement paste without compromising its integrity at high temperatures, the RAG system retrieved a document that included information about batch cementing to ensure homogeneity during manufacturing and pumping. 
                    While this information is true, it was not relevant to the specific question asked. 
                    In this aspect, the multi-agent solution performed better at discarding such irrelevant information, focusing more accurately on the task at hand. 
                    Other possible solutions include improving the accuracy of semantic search by adjusting a minimum threshold for similarity measures or through re-ranking techniques such as those proposed by \citep{Carraro2024} and \citep{Sun2023}.
                
                \textbf{\textit{Hallucination.}} 
                    During the evaluation of our system, we encountered instances where the agent produced hallucinated information instead of utilizing the appropriate tool to retrieve accurate data, as in \citep{Bilbao2023}. 
                    For example, when asked, "How many anomalies occurred on rig number 05 during August 2023?" the agent was expected to use the Text-to-SQL tool to query the database. 
                    However, it bypassed this tool and generated a fabricated response, stating that 5 anomalies occurred, along with detailed descriptions of fictional events. The correct answer, as retrieved from the database, was that 7 anomalies occurred. This hallucination likely resulted from the agent's reliance on its internal knowledge rather than external data retrieval. 

                    In terms of hallucination statistics, our analysis revealed that for Q\&A tasks, hallucinations occurred in 9.6\% of cases and 3.8\% for partially hallucinated. 
                    In contrast, Text-to-SQL tasks exhibited a lower hallucination rate, with only 3.6\% of responses containing hallucinated information and 96.4\% being accurate. 
                    These findings highlight the variation of susceptibility to hallucination in different types of tasks, highlighting the need for targeted strategies to mitigate this problem.
                
                \textbf{\textit{Industry Jargon:}}
                    Specifically analyzing the activity of drilling and completion of offshore wells, the main challenge is the inherently complex and technical nature of the data involved. 
                    There were instances of incorrect interpretation of information, likely due to the use of terms, expressions, and themes specific to well construction, to which the language model had little or no exposure during training phase. 
                    A possible solution is the implementation of specialized models, which has been pointed out in gray literature as a trend for the coming years \citep{Shah2024, Meena2023, Ghosh2023}.
                
                \textbf{\textit{Tools vs. Performance:}} 
                    It was identified during the experiments that agents with a high amount of tools showed a decline in overall performance. 
                    This can be attributed to the added context to the prompts. 
                    As the context length increases, the model's ability to accurately interpret and respond diminishes.
                    This is a limitation of current language models, where longer contexts can lead to a dilution of relevant information and increased difficulty in maintaining coherence and accuracy. 
                    This conclusion is currently qualitative, as these metrics were not addressed in this experiment.

                
                \textbf{\textit{Queries Involving Proper Names:}}
                    In queries involving people's names, it was not possible to retrieve relevant documents using semantic search. 
                    For example, when asked to identify the employee associated with a specific key and list knowledge items they registered in the system, the LLM incorrectly attributed knowledge items to the wrong author\xexeo{O RAG ou a LLM usando o RAG, nÃ£o ficou claro}\vitor{OK}. 
                    This highlights the difficulty in accurately retrieving information based on proper names, which can be complicated by variations in accentuation, abbreviation, and formatting.
                    \xexeo{tem evidÃªncias disso em outros artigos?}
                    \vitor{nÃ£o encontrei}
                    A potential solution to be explored is the use of Self-Query Retriever \citep{LangchainSelfQuery2023}, implementing a hybrid search with metadata filters (including proper names) and semantic retrieval of the rest of the query. 
                    It is also suggested, in these cases, to use the \citep{Levenshtein1966} distance to handle possible variations in the spelling of names. 
                    This approach could improve the accuracy of retrieving documents related to specific individuals, ensuring that the correct information is associated with the right person.
                    
            
            % \subsubsection{Practical Implications}
            \subsubsection{Practical Implications.} 

                The findings from our study have significant practical implications for the O\&G sector, and potentially for other industries characterized by complex and technical data environments:
                    
                \begin{itemize}
                
                    \item \textbf{Enhanced Decision-Making Support:}
                        Our results indicate that multi-agent systems provide a 28\% higher truthfulness measure in Q\&A tasks. This can be particularly beneficial for decision-making in well engineering, where accurate and truthful information is critical.
                        Implementing multi-agent systems in decision-making processes can lead to more reliable and informed decisions, thereby reducing the risk of errors and enhancing operational safety and efficiency.
                    
                    \item \textbf{Balancing Performance and Economic Efficiency:}
                        While multi-agent systems offer superior performance in terms of truthfulness, they come with a cost that is 3.7 times higher on average compared to single-agent systems.
                        This highlights the importance of a strategic approach in selecting agent configurations based on specific tasks and budget constraints. 
                        % For instance, single-agent systems might be more cost-effective for Text-to-SQL tasks where they have shown to perform 15\% better. 
                        A detailed cost-benefit analysis reveals that for Q\&A tasks using GPT-4, the single-agent configuration yields a ratio of 32.33 truthfulness points per dollar, compared to 10.16 for the multi-agent setup. While the multi-agent system shows a 17.8\% improvement in truthfulness, this comes at a 275\% increase in cost. The efficiency varies significantly by task type; in Text-to-SQL tasks, the GPT-4 single-agent outperforms the multi-agent by 42.5\% in truthfulness while costing 80.4\% less. 
                        % These quantitative insights emphasize the need for careful consideration of task requirements and budget constraints when choosing between single and multi-agent configurations.
                        
                    \item \textbf{Reflection and Critic Agents:}
                        A promising approach to enhance the performance of these agents is the use of reflection \citep{Shinn2023}, a method where agents verbally reflect on task feedback signals and maintain this reflective text in an episodic memory buffer to improve decision-making in subsequent trials. Critic agents are a way to implement reflection in a multi-agent setup. This type of agent is challenging to apply in Q\&A tasks over private technical data, as commercial LLMs (OpenAI, Google Bard, and others) have not been deeply trained in the domain and struggle to provide relevant and precise critiques, reinforcing the trend toward increased use of domain-specific models \citep{Shah2024, Meena2023, Ghosh2023}.                
                        
                    \item \textbf{Task-Specific Agent Configuration:}
                        The study highlights that the complexity of managing multiple agents does not always lead to better performance. In some cases, a single-agent setup might be more effective.
                        This insight can guide the development and deployment of AI systems, ensuring that the configuration of agents is tailored to the specific requirements of the task, thereby optimizing both performance and cost.           
                        
                    \item \textbf{Potential for Broader Application:}
                        The insights gained from this study are not limited to the O\&G sector but can be applied to other industries with similar technical complexities, such as aerospace, pharmaceuticals, and renewable energy.
                        By adopting multi-agent systems in these industries, organizations can improve decision-making, knowledge management, and operational efficiency, driving innovation and competitiveness.             
                    
                \end{itemize}
                        
                    
            \subsubsection{Future Directions.} 

                This work indicates possible pathways for enhancing RAG architectures in O\&G sector. 
                
                \begin{itemize}
                
                    \item \textbf{Enhancement of IR Semantic Techniques:}
                        There is a critical need to develop more sophisticated semantic search technologies. Future efforts should focus on enhancing the precision of information retrieval by filtering out irrelevant content more effectively. This will ensure that agents can provide more accurate and contextually appropriate responses, crucial for technical domains such as O\&G.
                        
                    \item \textbf{Development of Domain-Specific Models:}
                        Specialized models tailored specifically to the O\&G and other domains, such as biomedical engineering \citep{Pal2024}, could significantly improve the handling of specific jargon and complex technical data, while reducing LLM costs \citep{Arefeen2024}. Future research should aim to develop and train these models to better understand and interpret the unique language and data types found in O\&G, enhancing the overall accuracy of agent responses.
                        
                    \item \textbf{Optimization of Tool Use in Agent Performance:}
                        The relationship between the quantity of tools available to an agent and its performance needs further exploration. Future studies should quantify the impact of tool availability on agent efficacy and efficiency, aiming to optimize tool use without overwhelming the agent or diluting performance quality.
                        
                    \item \textbf{Integration of Advanced Name Recognition Techniques:}
                        Queries involving proper names pose a significant challenge in semantic search. Integrating advanced retrieval techniques, such as Self-Query Retrievers \citep{LangchainSelfQuery2023} and \citep{Levenshtein1966} distance algorithms, could improve the handling of these queries. Future research should focus on enhancing name recognition capabilities to ensure that agents can accurately retrieve and utilize correct information, especially in scenarios where precision is paramount.
                        
                    \item \textbf{Extension to Other Complex Domains:}
                        The potential applications of multi-agent systems are not limited to the O\&G sector. Future research should explore the adaptation and implementation of these systems in other complex and technical domains, such as aerospace, pharmaceuticals, and renewable energy. Investigating how these systems can support decision-making in these areas will provide valuable insights into their versatility and adaptability.
                        
                    \item \textbf{Hybrid Model Experimentation:}
                        Combining the strengths of single and multi-agent systems could yield significant benefits. Future directions should include experimenting with hybrid models that integrate the robustness and depth of multi-agent interactions with the simplicity and efficiency of single-agent systems. This hybrid approach could potentially offer a balanced solution, maximizing performance while managing costs and complexity.
                        

                \end{itemize}
                
                By pursuing these directions, future research can significantly advance the development of multi-agent systems, not only enhancing their application in the O\&G sector but also expanding their utility across various technologically intensive activities.
                    
\chapter{Second Experiment}

\xexeo{Tem muito LLMnismos aqui, como adjetivos desnecessÃ¡rios propagandÃ­sticos. TÃ¡ muito genÃ©rico, falta dados e deixa muita pergunta. Parece mais um plano do que algo feito. VocÃª jÃ¡ fez e falta escrever?}


    
    This chapter describes the second experiment used to evaluate LLM agents and workflows to answer questions in the well engineering domain. 
    The experiment integrates multiple LLM configurations, agent architectures, and RAG tools, leveraging Petrobras\xexeo{OPA, escapou um Petrobras aqui} datasets.

    To achieve these objectives, this research was conducted through two distinct experimental phases. The first, carried out in 2024, focused on a foundational comparison between single and multi-agent architectures, revealing key insights into their performance, cost, and limitations such as hallucination and context interpretation. \xexeo{nÃ£o Ã© o capÃ­tulo anterior? Acho que pode tirar}
    The rapid evolution of generative AI frameworks and models prompted a second, more up-to-date experiment in 2025. 
    This second phase built upon the initial findings, also employing non-agentic workflows as a baseline and a more rigorous, quantitative evaluation methodology to address the challenges identified in the first experiment and automated evaluation based on the concept commonly referred to as ``LLM-as-a-judge'' \citep{Gu2025}.

    The use of \enquote{LLM-as-a-judge} was driven by the sheer volume of responses to be evaluated. 
    With four configurations, two models, and three executions for each, a total of 24 responses were generated for every question in the dataset.
    Manually assessing this volume of data would have been impractical.
    Furthermore, previously used metrics like `truthfulness` had become obsolete. 
    This metric was highly relevant when models frequently hallucinated, a problem that is far less prevalent in the current generation of LLMs. 

    \section{Methodology}

        \subsection{Experimental Workflow}

            \subsubsection{Dataset Preparation}

                The experimental workflow was designed to provide a thorough and reproducible evaluation of language model agents within oil well operations. 
                The process begins with the careful preparation of the dataset, which is composed of questions and corresponding ground truth answers derived from a diverse range of operational records, incident reports, and lessons learned. To ensure the quality and relevance of the data, questions undergo a filtering and preprocessing phase where clarity, diversity, and alignment with real-world scenarios are prioritized. 
                This includes removing duplicates, standardizing terminology, and confirming that each question is properly paired with an accurate answer. 
                The dataset is further validated for completeness and consistency, ensuring it represents the full spectrum of operational challenges, such as safety, cementing, and intervention scenarios.

            \subsubsection{Model and Setup Selection}

                Following dataset preparation, the experimental design incorporates a variety of agent architectures.
                These include approaches where questions are routed to specialized agents, single-agent systems that centralize all reasoning and retrieval, and multi-agent frameworks that leverage collaboration among specialized agents under a supervisory structure. Each of these configurations is evaluated using different language models, allowing for a comprehensive assessment of how model choice and agent setup influence performance.
                The agents are also provided with access to advanced retrieval tools and domain-specific knowledge bases, enabling them to draw on a broad foundation of operational expertise.

            \subsubsection{Execution Loop}

                The core of the experimental workflow is an execution loop depicted in Algorithm~\ref{alg:execution_loop}. 
                For each combination of question, agent setup, and language model, the system systematically loads the relevant data, configures the agent, and executes the workflow. 
                Throughout this process, all responses and intermediate reasoning steps are meticulously logged. 
                This approach not only ensures systematic coverage of all experimental conditions but also provides full traceability for subsequent analysis. 
                The automation of these procedures guarantees consistency and reproducibility, while the comprehensive logging facilitates in-depth evaluation and comparison of agent performance across a range of operational scenarios.

            
                \begin{algorithm}
                    \caption{Experiment Execution Loop}
                    \begin{algorithmic}[1]
                    \Require questions, setups, models
                    \Ensure results                    
                    \Function{RunExperiment}{}
                        \State $results \gets \{\}$     
                        \ForAll{$question \in questions$}
                            \State $ground\_truth \gets question.ground\_truth$                            
                            \ForAll{$setup \in setups$}
                                \ForAll{$model \in models$}
                                    \State $agent \gets \text{InitializeAgent}(setup, model)$
                                    \State $response \gets agent.\text{ProcessQuestion}(question)$                                    
                                    \State $metrics \gets \text{EvaluateResponse}(response, ground\_truth)$                                    
                                    \State $results[question, setup, model] \gets \{$
                                    \State \hspace{1cm} $"response": response,$
                                    \State \hspace{1cm} $"metrics": metrics,$
                                    \State \hspace{1cm} $"execution\_trace": agent.trace$
                                    \State $\}$
                                \EndFor
                            \EndFor
                        \EndFor
                        \State \Return $\text{AggregateResults}(results)$
                    \EndFunction                    
                    \end{algorithmic}
                    \label{alg:execution_loop}
                \end{algorithm}
            
            \subsubsection{Evaluation and Metrics}

                Following the execution of all experimental combinations, a comprehensive evaluation framework is applied to assess agent performance. 
                The system calculates a suite of quantitative metrics for each question, setup, and model combination by comparing the generated answers against the established ground truth. 
                These metrics include standard performance indicators such as accuracy, precision, recall, and F1 score, which provide a multifaceted view of response quality. For open-ended questions where binary correctness measures are insufficient, a confusion matrix approach is implemented to capture nuances in answer quality and content coverage. Additionally, the system measures answer size ratio relative to ground truth, offering insights into model verbosity and conciseness. 
                These metrics are then aggregated across different dimensions to enable meaningful comparisons between agent architectures and language models, revealing patterns in performance across various operational scenarios and question types.

            \subsubsection{Reproducibility and Quality Control}

                To ensure scientific rigor and reproducibility, the experimental methodology incorporates robust tracking of all environmental variables and configuration parameters. 
                The system maintains detailed logs of the computational environment, including software versions, dependency specifications, and hardware characteristics that might influence results. 
                All experimental parameters, from model identifiers to dataset specifications, are systematically recorded alongside the results they generate. Throughout the experimental process, periodic validation checks are performed to maintain data integrity and result consistency, with anomalies flagged for investigation. This comprehensive approach to reproducibility not only facilitates verification of findings but also enables future extensions of the research with comparable baselines. The quality control measures embedded in the workflow ensure that conclusions drawn from the experiments rest on a foundation of methodological soundness and data reliability.

                \textit{<insert workflow diagram or pseudocode here to illustrate the above stages>}\xexeo{OPA! }

        \subsection{Data Sources}

                The experimental evaluation relies on a  curated collection of data sources that represent the diverse knowledge domains relevant to oil well operations. 
                At the core of the experiment is a comprehensive questions dataset containing structured entries that simulate real-world queries an operator might encounter. 
                This dataset was developed through extensive collaboration with domain experts and analysis of historical operational records.
                Each entry in the dataset contains a question formulated in natural language, a unique identifier, categorical metadata to facilitate analysis, and a corresponding ground truth answer validated by subject matter experts. The questions span various complexity levels, from factual inquiries to complex reasoning scenarios that require integration of multiple knowledge sources.

            To provide the language models with the necessary domain knowledge, the experiment incorporates several specialized knowledge bases that reflect different aspects of oil well operations:

            \begin{itemize}
                \item \textbf{Knowledge Bases and Tools}:
                \begin{itemize}
                    \item \textbf{Lessons}: A repository of knowledge items capturing insights, best practices, and technical know-how from past oil well operations. These lessons represent institutional memory and expertise accumulated over years of operational experience.
                    \item \textbf{Alertas SMS}: A collection of safety alerts and incident reports documenting past events, near-misses, and accidents, providing critical safety information and preventative measures.
                    \item \textbf{SITOP}: Detailed daily operational logs from drilling rigs, containing technical parameters, operational decisions, and situational reports from active drilling operations.
                \end{itemize}
            \end{itemize}

            These knowledge sources were preprocessed to ensure consistency, remove sensitive information, and optimize retrieval performance. The integration of these diverse data sources enables a holistic evaluation of how language model agents navigate the complex informational landscape of oil well operations, from technical specifications to safety protocols and historical precedents.

            \textit{<insert table or image summarizing datasets and tools here>}

        \subsection{System Architecture}

                The experimental system was implemented using modern Python frameworks specialized for language model orchestration and agent workflows. 
                The architecture leverages the LangChain and LangGraph ecosystems, which provide robust foundations for building complex language model applications with multiple components and state management.
                This subsection details the modular design of the system, highlighting how different components interact to enable systematic evaluation of language model agents in oil well operations.

            \subsubsection{Experiment Orchestration}

                At the core of the system architecture is an experiment orchestration layer responsible for coordinating the entire evaluation process. 
                This component manages the loading of questions from the dataset, systematically iterates through different model and setup combinations, and ensures proper logging of results. The orchestrator maintains experiment state across multiple runs, handles error recovery, and implements checkpointing to allow for resumption of long-running experiments. By centralizing control flow, this component ensures that all experimental conditions are tested consistently and that results are captured in a standardized format for subsequent analysis.

            \subsubsection{Agent Workflow Frameworks}

                The system implements multiple agent workflow frameworks to evaluate different approaches to question answering in the oil well domain. These frameworks define the flow of information and decision-making processes within and between language model agents. The implemented workflows include a Linear-Flow with Router (CORTEX) that directs questions to specialized processing paths, a Single-Agent approach that centralizes all reasoning and tool use, and a Multi-Agent Supervisor framework that coordinates multiple specialized agents. Each workflow is defined declaratively, specifying the sequence of operations, decision points, and information exchange patterns that govern agent behavior during question processing.

            \subsubsection{Nodes and Tool Integration}

                The system architecture includes specialized nodes that implement specific reasoning steps and tool-calling logic. These nodes serve as the building blocks of agent workflows, encapsulating discrete functionality such as question analysis, knowledge retrieval, and answer synthesis. The tool integration layer provides agents with access to external knowledge sources through a standardized interface, enabling semantic search over domain-specific corpora, structured data queries, and other specialized operations. This modular approach to tool integration allows for consistent evaluation of how different agent architectures leverage available tools and knowledge sources.

            \subsubsection{Prompt Engineering and System Messages}

                A critical component of the architecture is the prompt engineering layer, which defines the instructions and context provided to language models. This includes carefully crafted system messages that establish the role and capabilities of each agent, prompt templates that structure inputs consistently across experimental conditions, and few-shot examples that guide model behavior. The system maintains a library of prompt variants optimized for different tasks within the question-answering workflow, ensuring that each agent receives appropriate guidance while maintaining experimental control.

            \subsubsection{State Management and Metrics}

                The architecture incorporates a comprehensive state management system that tracks the progress of experiments, maintains contextual information across agent interactions, and captures intermediate reasoning steps. This component is tightly integrated with the metrics calculation subsystem, which computes performance indicators in real-time as experiments progress. The metrics framework implements various evaluation approaches, from simple accuracy measures to sophisticated semantic similarity calculations, providing multi-dimensional assessment of agent performance. All experimental data, including intermediate states and final results, is persisted in structured formats to enable both immediate feedback and in-depth post-experiment analysis.

                \textit{<insert system architecture diagram here>}

        \subsection{Experimental Setups}

            To comprehensively evaluate language model performance in well construction operations, the experiment employed multiple agent architectures and model configurations. This subsection details the different experimental setups, highlighting their design principles, operational characteristics, and the rationale behind their selection. The experimental design deliberately incorporates contrasting approaches to agent architecture, enabling comparative analysis of different strategies for complex question answering in specialized domains.
            

            \subsubsection{Linear-Flow}

                The Linear-Flow architecture represents the simplest RAG design, where user input is processed in a strictly sequential manner. In this setup, the userâ€™s query is handled by a single LLM step, which carries all the instructions (PT1, PT2, PT3 and PT4, as depicted in \ref{fig:diagrama_linear_flow}) required for the generation of various types of search queries. These instruction prompts are often quite long, as they are carefully crafted to produce high-quality queries for the vector store. Due to the aggregation of all instruction prompts within a single LLM invocation, the resulting context becomes notably extensive. This can lead to performance degradation as the context length increases [O QUE PODE SER VISTO NO GRAFICO TAL EM CONTRASTE COM O SETUP TAL QUE DIVIDE OS PROMPTS EM PARTES]. 
            

                \begin{figure}[h]
                    \centering
                    \includegraphics[width=0.8\textwidth]{images_exp2/diagrams/diagrama_linear_flow.png}
                    \caption{Linear-Flow architecture. PT1 indicates Prompt for Tool 1 and so on.}
                    \label{fig:diagrama_linear_flow}
                \end{figure}
           

            \subsubsection{Linear-Flow with Router}

                [VERSÃƒO 1]
                The Linear-Flow with Router architecture implements a sequential processing pipeline with routing, in order to divide and reduce tool instruction prompts. In this setup, questions first pass through a router that analyzes the query content and determines the most appropriate specialized node for the user query at hand. 
                
                [VERSÃƒO 2]
                The Linear-Flow with Router paradigm extends the basic linear flow by introducing a routing mechanism that enables the distribution of tool instruction prompts into specialized nodes with smaller prompts. User questions first pass through a router that determines the most appropriate specialized node. As illustrated in Figure~\ref{fig:diagrama_linear_w_router}, instead of a single node generating different types of retrieval queries, we have several nodes (or sub-queries), each with its own specialized tool. Each sub-query is then processed independently by separate tool invocations.
                    
                This approach offers several advantages:

                \begin{itemize}

                    \item \textbf{Increased Throughput:} By distributing sub-tasks across multiple tools, the system can handle more complex or multi-faceted user requests efficiently.
                    
                    \item \textbf{Specialization:} Each tool can be tailored to address a specific aspect of the user's query, allowing for more accurate and relevant results.
                    
                    \item \textbf{Scalability:} The architecture naturally supports scaling, as additional tools can be added to handle more sub-queries or specialized tasks.
                    
                \end{itemize}
                
                In practice, the router acts as an orchestrator, analyzing the user input and generating multiple targeted queries (PT1*, PT2*, PT3*, PT4* in the figure). These queries are dispatched to their respective tools, and the results are aggregated to form the final answer. This method is particularly effective for tasks that can be decomposed into independent components, such as multi-part questions or workflows requiring different types of expertise.
                
                Compared to the standard linear flow, the use of a router introduces additional complexity in query generation and result aggregation but enables a significant boost in system flexibility and performance.

                \begin{figure}[h]
                    \centering
                    \includegraphics[width=0.8\textwidth]{images_exp2/diagrams/diagrama_linear_w_router.png}
                    \caption{Linear-Flow with Router architecture.}
                    \label{fig:diagrama_linear_w_router}
                \end{figure}
                
                

                
            \subsubsection{Single-Agent}

                The Single-Agent approach represents a centralized architecture where a single language model agent handles the entire question-answering process. This agent has access to the full suite of retrieval tools and knowledge sources, making independent decisions about which tools to invoke and how to synthesize information into coherent answers. The design emphasizes end-to-end reasoning within a unified context, allowing the model to maintain a consistent understanding throughout the process. This approach tests the capability of language models to manage complex workflows autonomously, balancing between exploration of different knowledge sources and focused answer generation without the overhead of inter-agent communication.
                
                \begin{figure}[h]
                    \centering
                    \includegraphics[width=0.5\textwidth]{images_exp2/diagrams/diagrama_single_agent.png}
                    \caption{Single-Agent architecture}
                    \label{fig:diagrama_single_agent}
                \end{figure}


            \subsubsection{Multi-Agent Supervisor}

                The Multi-Agent Supervisor setup implements a collaborative approach where multiple specialized agents work together under the coordination of a supervisor agent. Each specialized agent focuses on a specific domain of knowledge or reasoning skill, such as retrieval, analysis, or explanation generation. The supervisor agent orchestrates the collaboration, delegating subtasks to appropriate specialized agents, integrating their contributions, and ensuring coherence in the final answer. This architecture explores the potential benefits of distributed cognition, where complex reasoning is decomposed into manageable components handled by purpose-built agents. The framework includes mechanisms for resolving conflicts between agents and synthesizing potentially divergent perspectives into unified responses.

                \begin{figure}[h]
                    \centering
                    \includegraphics[width=0.6\textwidth]{images_exp2/diagrams/diagrama_multiagente_supervisor.png}
                    \caption{Multi-Agent setup with one supervisor and 4 specialist agents.}
                    \label{fig:diagrama_multiagente_supervisor}
                \end{figure}


                \textit{<insert table summarizing experimental setups and models here>}

        \subsection{Execution Details}

            The experiment was driven by a script without manual intervention during the evaluation process. A main execution loop systematically iterated through all combinations of questions, agent setups, and language models defined in the experimental design. 

            \subsubsection{Tool Integration and Knowledge Access}

            During execution, the agent systems accessed domain-specific knowledge through a standardized tool interface layer. This layer provided consistent access patterns across all experimental configurations, ensuring that differences in performance could be attributed to agent architecture rather than variations in knowledge availability. The tool integration framework supported a diverse range of knowledge access methods, including semantic search over unstructured text corpora, structured queries against relational databases, and specialized information extraction routines tailored to the oil well operations domain. Each tool invocation was executed within a controlled environment that captured performance metrics such as latency and resource utilization, providing additional dimensions for analysis beyond answer correctness. The standardization of tool interfaces across agent architectures was a critical design decision that enabled fair comparison while still allowing each architecture to implement its own strategy for tool selection and result interpretation.

            \subsubsection{Comprehensive Logging and Observability}

            A cornerstone of the experimental methodology was the implementation of comprehensive logging throughout the execution process. The system captured detailed records of each step in the question-answering workflow, from initial question parsing to final answer generation. These logs included intermediate reasoning steps, tool invocations with their inputs and outputs, and internal state transitions within the agent systems. All experimental artifacts were persisted in structured formats that facilitated both automated analysis and manual inspection. The logging system implemented a hierarchical organization that linked high-level metrics to the detailed execution traces that produced them, enabling root cause analysis of performance patterns. This observability infrastructure was essential for understanding not just what results were produced, but how and why different agent architectures arrived at their answers, providing insights into their reasoning processes and failure modes.

            \textit{<insert code snippet or pseudocode of main execution loop here>}

        \subsection{Evaluation Metrics}

            The evaluation of each experimental run is grounded in a comprehensive set of metrics designed to capture both the correctness and the quality of the systemâ€™s responses. Standard quantitative measures such as accuracy, precision, recall, and F1 score are calculated by comparing the answers generated by the agent systems to the established ground truth for each question. These metrics provide a multifaceted view of performance, indicating not only how often the system produces correct answers but also how well it balances false positives and false negatives.

            For questions that are open-ended or less amenable to binary correctness, the evaluation framework employs a confusion matrix approach. This allows for a more nuanced assessment, capturing partial correctness and the degree to which the systemâ€™s response overlaps with the expected content. Additionally, the methodology includes the calculation of the answer size ratio, which measures the verbosity of the generated answer relative to the ground truth. This metric helps to identify tendencies toward overly concise or excessively verbose responses, offering further insight into the modelsâ€™ behavior and suitability for practical deployment.

        \subsection{Limitations}

            While the experimental methodology strives for rigor and comprehensiveness, several limitations must be acknowledged. One key limitation concerns the coverage of the dataset: although the question set is carefully curated to represent a broad range of operational scenarios, it may not capture the full diversity of real-world challenges encountered in oil well operations. Similarly, the models and agent architectures evaluated are constrained by the available computational resources and the current state of language modeling technology, which may limit their ability to generalize beyond the scenarios tested.

            Another limitation arises from the reliance on ground truth answers, which, despite expert validation, may still reflect subjective judgments or incomplete information in certain cases. Furthermore, the evaluation metrics, while robust, may not fully capture qualitative aspects of answer usefulness or clarity, especially in highly technical or ambiguous situations. Recognizing these limitations is essential for interpreting the results and for guiding future research aimed at addressing these gaps.

        \subsection{Summary}

            This methodology provides a systematic framework for comparing different agent architectures and large language models in the context of complex question answering for oil well operations. By integrating rigorous evaluation metrics, robust reproducibility practices, and a clear acknowledgment of limitations, the approach enables meaningful insights into the strengths and weaknesses of various system designs. The findings derived from this methodology can inform both the deployment of language model agents in operational settings and the ongoing development of more capable and reliable AI systems for specialized industrial domains.


        \subsection{Quality Assessment with LLM-as-a-Judge}
            \label{sec:llm_judge_evaluation}

            To evaluate the quality of the responses generated by the RAG system, an automated evaluation approach known as \textit{LLM-as-a-Judge} \citep{Zheng2023} was adopted. This method uses a large language model (LLM) as an impartial judge to compare the system-generated response with a reference response, known as the \textit{Ground Truth} (GT). Using an LLM for evaluation allows for a more granular and scalable analysis than manual human evaluation, while also capturing nuances of semantics and content.

            The evaluation process was implemented through a structured \textit{prompt} that instructs the LLM-judge to perform an analysis based on the concepts of a confusion matrix. The evaluation flow for each question-answer pair is as follows:

            \begin{enumerate}
                \item \textbf{Decomposition into Statements:} The LLM-judge is instructed to decompose both the system's response and the \textit{Ground Truth} response into a set of atomic and objective statements. This allows for a detailed comparison, rather than a holistic and subjective evaluation.

                \item \textbf{Classification of Statements:} Based on the comparison between the statements, the LLM-judge classifies them into three categories, following the logic of a confusion matrix:
                \begin{itemize}
                    \item \textbf{True Positive (TP):} Correct statements made by the system that are also present in the \textit{Ground Truth} response.
                    \item \textbf{False Positive (FP):} Incorrect, irrelevant, or hallucinated statements made by the system that are not in the \textit{Ground Truth} response.
                    \item \textbf{False Negative (FN):} Statements present in the \textit{Ground Truth} response that were omitted by the system.
                \end{itemize}
                The True Negative (TN) category is not applicable in this context, as the goal is to measure the precision and completeness of the information provided.

                \item \textbf{Calculation of Metrics:} Based on the count of statements in each category (TP, FP, FN), the following information retrieval metrics are calculated:
                \begin{itemize}
                    \item \textbf{Precision:} Measures the proportion of correct statements among all statements made by the system. It is calculated as:
                    $$ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} $$
                    \item \textbf{Recall:} Measures the proportion of correct statements that the system managed to retrieve compared to the total number of statements in the \textit{Ground Truth} response. It is calculated as:
                    $$ \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} $$
                    \item \textbf{F1-Score:} Is the harmonic mean of Precision and Recall, providing a single metric that balances both values:
                    $$ \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} $$
                \end{itemize}
            \end{enumerate}

            This methodology allows for a quantitative and objective evaluation of the accuracy and completeness of the system's responses, providing valuable \textit{insights} into its performance in different scenarios.



            
    \section{Results and Discussion}

        
        \subsection{Performance}
        
            In this section, the F1-score charts are presented, which is the main metric for performance evaluation in this work. The corresponding charts for the precision and recall metrics are available in Appendix~\ref{sec:exp2_appendix}.
            
            Ã‰ importante ressaltar que todos os resultados apresentados correspondem Ã  melhor performance obtida em trÃªs execuÃ§Ãµes independentes para cada configuraÃ§Ã£o de agente e modelo. A adoÃ§Ã£o desta metodologia visa mitigar a variabilidade inerente aos processos estocÃ¡sticos presentes em muitos algoritmos de aprendizado de mÃ¡quina. Fatores como a inicializaÃ§Ã£o aleatÃ³ria de pesos ou a aleatorizaÃ§Ã£o de dados podem levar a resultados distintos a cada execuÃ§Ã£o. Ao selecionar o melhor resultado, busca-se apresentar o potencial mÃ¡ximo de cada configuraÃ§Ã£o, reduzindo a chance de que uma performance inferior, causada por um Ã³timo local ou uma inicializaÃ§Ã£o desfavorÃ¡vel, seja erroneamente interpretada como a capacidade real do modelo.

            \begin{figure}[h]
                \centering              
                \includegraphics[width=0.50\textwidth]{images_exp2/bar_best_f1_by_model_and_configuration.png}
                \caption{F1 score by model and configuration.}
                \label{fig:best_f1_by_model_and_configuration}
            \end{figure}


            ...

            ...
        
            \subsubsection{F1 Score}
            
                
                ...

                ...
                
                \begin{figure}
                    % \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/f1_score_by_model_and_configuration.png}
                    \caption{F1 Score distribution by model and configuration of agents}
                    \label{fig:f1_score_by_model_and_configuration}
                \end{figure}
                    
                ...

                ...
                
                
                \begin{figure}
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/best_f1_by_question_index_and_configuration.png}
                    \caption{Best F1 score by question index and configuration.}
                    \label{fig:best_f1_by_question_index_and_configuration}
                \end{figure}
                
                ...

                ...

                
                ...

                
                \begin{figure}
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/best_f1_by_question_index_and_model.png}
                    \caption{Best F1 score by question index and model.}
                    \label{fig:best_f1_by_question_index_and_model}
                \end{figure}


            \subsubsection{PrecisÃ£o}

                ...


                ...

            
            \subsubsection{Recall}
            
                ...

                ...

                ...
                
                \begin{figure}[h!]
                    \centering              
                    \includegraphics[width=0.25\textwidth]{images_part_2/model_recall_model_configuration.png}
                    \caption{Recall by model and configuration.}
                    \label{fig:model_recall_model_configuration}
                \end{figure}

                ...

                ...
                
                \begin{figure}[h!]
                    \centering              
                    \includegraphics[width=0.25\textwidth]{images_part_2/question_recall_question_index_configuration.png}
                    \caption{Recall por pergunta e configuraÃ§Ã£o.}
                    \label{fig:question_recall_question_index_configuration}
                \end{figure}

            
                % \begin{figure}[H] 
                %     \centering
                %     \includegraphics[scale=0.75]{images_exp2/bar_avg_f1_by_configuration.png}
                %     \caption{Average F1 score by configuration.}
                %     \label{fig:bar_avg_f1_by_configuration}
                % \end{figure}

                % \begin{figure}[H]
                %     \centering
                %     \includegraphics[scale=0.75]{images_exp2/bar_avg_f1_by_model.png}
                %     \caption{Average F1 score by model.}
                %     \label{fig:bar_avg_f1_by_model}
                % \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/bar_best_f1_by_model_and_configuration.png}
                    \caption{Best F1 score by model and configuration.}
                    \label{fig:bar_best_f1_by_model_and_configuration}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/best_f1_by_question_index_and_configuration.png}
                    \caption{Best F1 score by question index and configuration.}
                    \label{fig:best_f1_by_question_index_and_configuration}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/best_f1_by_question_index_and_model.png}
                    \caption{Best F1 score by question index and model.}
                    \label{fig:best_f1_by_question_index_and_model}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/f1_boxplot_by_model_and_configuration.png}
                    \caption{F1 score boxplot by model and configuration.}
                    \label{fig:f1_boxplot_by_model_and_configuration}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/f1_lineplot_by_question_index_and_configuration.png}
                    \caption{F1 score line plot by question index and configuration.}
                    \label{fig:f1_lineplot_by_question_index_and_configuration}
                \end{figure}

                % \begin{figure}[H]
                %     \centering
                %     \includegraphics[scale=0.75]{images_exp2/f1_score_by_model.png}
                %     \caption{F1 score by model.}
                %     \label{fig:f1_score_by_model}
                % \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/f1_score_by_model_and_configuration.png}
                    \caption{F1 score by model and configuration.}
                    \label{fig:f1_score_by_model_and_configuration}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/facet_hist_f1_by_model.png}
                    \caption{Faceted histogram of F1 score by model.}
                    \label{fig:facet_hist_f1_by_model}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/facet_hist_f1_by_model_best_f1.png}
                    \caption{Faceted histogram of best F1 score by model.}
                    \label{fig:facet_hist_f1_by_model_best_f1}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.5\linewidth]{images_exp2/hist_f1_score_all.png}
                    \caption{Histogram of all F1 scores.}
                    \label{fig:hist_f1_score_all}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/line_f1_by_question_index_and_model.png}
                    \caption{Line plot of F1 score by question index and model.}
                    \label{fig:line_f1_by_question_index_and_model}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/scatter_f1_vs_total_time.png}
                    \caption{Scatter plot of F1 score vs total time.}
                    \label{fig:scatter_f1_vs_total_time}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/scatter_f1_vs_total_token_count_input.png}
                    \caption{Scatter plot of F1 score vs total input token count.}
                    \label{fig:scatter_f1_vs_total_token_count_input}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/swarm_f1_by_model_and_configuration.png}
                    \caption{Swarm plot of F1 score by model and configuration.}
                    \label{fig:swarm_f1_by_model_and_configuration}
                \end{figure}


            \subsection{Recall}


                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/recall/bar_best_recall_by_model_and_configuration.png}
                    \caption{Best recall by model and configuration.}
                    \label{fig:bar_best_recall_by_model_and_configuration}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/recall/best_recall_by_question_index_and_configuration.png}
                    \caption{Best recall by question index and configuration.}
                    \label{fig:best_recall_by_question_index_and_configuration}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/recall/best_recall_by_question_index_and_model.png}
                    \caption{Best recall by question index and model.}
                    \label{fig:best_recall_by_question_index_and_model}
                \end{figure}


                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/recall/facet_hist_recall_by_model.png}
                    \caption{Faceted histogram of recall by model.}
                    \label{fig:facet_hist_recall_by_model}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/recall/facet_hist_recall_by_model_best_recall.png}
                    \caption{Faceted histogram of best recall by model.}
                    \label{fig:facet_hist_recall_by_model_best_recall}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.5\linewidth]{images_exp2/recall/hist_recall_all.png}
                    \caption{Histogram of all recall.}
                    \label{fig:hist_recall_all}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/recall/line_recall_by_question_index_and_model.png}
                    \caption{Line plot of recall by question index and model.}
                    \label{fig:line_recall_by_question_index_and_model}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/recall/recall_boxplot_by_model_and_configuration.png}
                    \caption{Boxplot of recall by model and configuration.}
                    \label{fig:recall_boxplot_by_model_and_configuration}
                \end{figure}



                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{images_exp2/recall/recall_lineplot_by_question_index_and_configuration.png}
                    \caption{Line plot of recall by question index and configuration.}
                    \label{fig:recall_lineplot_by_question_index_and_configuration}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/recall/scatter_recall_vs_total_time.png}
                    \caption{Scatter plot of recall vs total time.}
                    \label{fig:scatter_recall_vs_total_time}
                \end{figure} 

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/recall/scatter_recall_vs_total_token_count_input.png}
                    \caption{Scatter plot of recall vs total token count input.}
                    \label{fig:scatter_recall_vs_total_token_count_input}
                \end{figure}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.75]{images_exp2/recall/swarm_recall_by_model_and_configuration.png}
                    \caption{Swarm plot of recall by model and configuration.}
                    \label{fig:swarm_recall_by_model_and_configuration}
                \end{figure}

                % \begin{figure}[H]
                %     \centering
                %     \includegraphics[scale=0.75]{images_exp2/recall/violin_recall_by_model_and_configuration.png}
                %     \caption{Violin plot of recall by model and configuration.}
                %     \label{fig:violin_recall_by_model_and_configuration}
                % \end{figure}

\chapter{Conclusions}

    The results of this study highlight the potential of multi-agent architectures based on LLMs in the O\&G sector, particularly in the domain of well engineering. The ability to process and respond to complex queries paves the way for a significant digital transformation in the area.

    Our comparative analysis of single-agent and multi-agent architectures, using GPT-3.5-turbo and GPT-4, reveals a detailed landscape of trade-offs between performance and economic efficiency. Multi-agent systems demonstrate 28\% greater factuality in question-and-answer (Q\&A) tasks, especially with GPT-4, compared to single-agent systems. However, they incur LLM costs that are, on average, 3.7 times higher due to the complexities of inter-agent communication. In contrast, single-agent systems excel in Text-to-SQL tasks, showing 15\% better performance than multi-agent setups. This cost-benefit dynamic requires careful consideration when implementing RAG in real-world scenarios, where accuracy and financial constraints must be balanced.

    We highlight several challenges encountered during our experiments, including issues with contextualization, the need for more refined information filtering, and the persistence of hallucinations. These challenges underscore the need for continuous research in areas such as domain-specific specialized models, advanced semantic search techniques, and hybrid architectures that combine the strengths of single-agent and multi-agent systems.

    The practical implications of this study extend beyond the O\&G sector. The insights gained here are applicable to any knowledge-intensive domain that deals with large volumes of technical data. By focusing on enhancing retrieval mechanisms, developing domain-specific LLMs, and optimizing interactions between agents and tools, we pave the way for more effective, reliable, and cost-efficient RAG solutions across various sectors.

    The main points of the study are as follows: multi-agent systems offer superior factuality in Q\&A tasks, albeit at a significantly higher cost. Single-agent architectures, on the other hand, excel at Text-to-SQL tasks. Despite the advantages, several challenges persist, including issues with contextualization, filtering, hallucination, and domain-specific vocabulary.

    Future research should focus on developing specialized models, advancing retrieval techniques, and exploring hybrid architectures. The lessons learned from this study have broader implications and can extend to other complex technical domains. By addressing the limitations identified in this study and embracing emerging trends in multi-agent systems and RAG technology, we can unlock their full potential, revolutionizing decision-making, knowledge management, and operational efficiency in complex industries worldwide.


    [INCLUIR: COMENTAR QUE OS PRECOS DE LLM ESTAO CAINDO MTO, MODELOS PEQUENOS COM EXCELENTE DESEMPENHO, O QUE TORNA A ANALISE FINANCEIRA DO 1o CICLO OBSOLETA]
