\documentclass[msc,english]{coppe}

\usepackage{booktabs}% tabelas mais bonitas
\usepackage{rotating}% rodando coisas, como tabelas
\usepackage{longtable} % tabelas longas
\usepackage{rotating}
\usepackage[most]{tcolorbox} % caixas de texto
\usepackage{amsmath,amssymb}

% \usepackage[editing]{coop-writing}
\usepackage[publish]{coop-writing}

\usepackage{marvosym}
\cwsetcommwarn{\Lightning}
\cwnamedef{xexeo}{red}{X}
\cwnamedef{vitor}{blue}{V}
\usepackage{xurl}
\usepackage{hyperref}

\usepackage{multirow}
\usepackage{changepage} 

\usepackage{adjustbox} 

\usepackage{xcolor, colortbl}
\usepackage{hhline} % For double lines

\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}

\usepackage{enumitem} % for custom enumerate labels and spacing

\usepackage{lmodern}
\usepackage[T1]{fontenc}

\usepackage{silence}
% \WarningFilter{latex}{Overfull}
\WarningFilter{latex}{Underfull}
\WarningFilter{latex}{empty journal}

\usepackage{float}
\usepackage{placeins} % for \FloatBarrier
\usepackage{pdflscape} % in preamble

\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}

\usepackage{quoting}


\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{keywordgreen}{rgb}{0.1,0.4,0.2}

\lstdefinestyle{mystyle}{ 
    commentstyle=\color{codegray},
    keywordstyle=\color{keywordgreen}\bfseries,
    numberstyle=\color{codegray},
    stringstyle=\color{purple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=10pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

% \usepackage[a4paper,margin=1.5cm]{geometry}


\makelosymbols
\makeloabbreviations

\begin{document}


\title{Comparative Analysis of Single and Multi-Agent Large Language Model Architectures for Domain-Specific Tasks in Well Construction}
\foreigntitle{Comparative Analysis of Single and Multi-Agent Large Language Model Architectures for Domain-Specific Tasks in Well Construction}
\author{Vitor}{BrandÃ£o Sabbagh}
\advisor{Prof.}{Geraldo}{Bonorino XexÃ©o}{D.Sc.}

\examiner{Prof.}{Geraldo Bonorino XexÃ©o}{D.Sc.}
\examiner{Prof.}{Jano Moreira de Souza}{Ph.D.}
\examiner{Prof.}{Arnaldo CÃ¢ndido JÃºnior}{D.Sc.}
\department{PESC}
\date{07}{2025}

\keyword{Large Language Models}
\keyword{Agents}
\keyword{Oil Well Construction}

\maketitle

\frontmatter
\dedication{To Carolina, my life partner.}

\chapter*{Acknowledgements}

To my daughter, Marina, who came into the world just two months ago, bringing a new light and a new purpose to my life. I dedicate every page of this work to you, with the hope of building a bright future for you.

To my parents, Vera and Nicolau, for all the love, unconditional support, and for always believing in me. Your faith in my abilities was the foundation for this achievement.

To my beloved wife, Carolina, my gratitude for all the patience, understanding, and love, especially during the most challenging moments of this journey. Without your support, this work would not have been possible.

To my stepson, Filipe, thank you for the moments of joy and relaxation that helped me maintain balance, especially during our Minecraft adventures. May our friendship continue to grow.

I express my deep gratitude to my mentor, Claudio, for his unwavering support and trust since the beginning of my career in digital transformation. His mentorship was fundamental to my professional development.

To my advisor, XexÃ©o, thank you for the wise guidance, academic rigor, and patience throughout this entire process. Your teachings were crucial to the quality of this work.

I extend my gratitude to the well construction engineering experts, Marcelo Grimberg, Rafael Peralta, and Lorenzo Simonassi, whose expertise and dedication significantly contributed to this research.

I also want to thank Ashish Vaswani. His work on ``Attention Is All You Need'' paved the way for the Large Language Models that were not only the subject of this dissertation but also an invaluable tool that helped me put ideas into words.

Finally, a special thanks to my colleagues from Petrobras, Tecgraf Institute, and the UNESP team. Our daily discussions about Gen-AI were an inexhaustible source of inspiration and knowledge, immensely enriching this dissertation.

\begin{abstract}
    
    A indÃºstria de Ã³leo e gÃ¡s, particularmente no domÃ­nio da construÃ§Ã£o de poÃ§os, enfrenta desafios significativos para extrair eficientemente conhecimento de vastos repositÃ³rios de dados tÃ©cnicos nÃ£o estruturados e semiestruturados. Esta dissertaÃ§Ã£o avalia e compara a eficÃ¡cia, a eficiÃªncia e a viabilidade prÃ¡tica de diferentes arquiteturas de GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o (RAG), abrangendo desde \textit{pipelines} nÃ£o-agÃªnticos atÃ© configuraÃ§Ãµes de agente Ãºnico e multiagente, para a resoluÃ§Ã£o de desafios de recuperaÃ§Ã£o de informaÃ§Ã£o em domÃ­nios especÃ­ficos.
    Adotando a metodologia de \textit{Design Science Research} (DSR), este estudo foi conduzido atravÃ©s de dois ciclos experimentais distintos. O primeiro ciclo (2024) forneceu uma comparaÃ§Ã£o fundamental, revelando que, embora uma arquitetura multiagente tenha alcanÃ§ado 28\% mais veracidade (\textit{truthfulness}) em tarefas de Pergunta e Resposta (Q\&A), seu custo operacional foi, em mÃ©dia, 3,7 vezes maior, e um sistema de agente Ãºnico foi surpreendentemente mais eficaz para tarefas de Texto-para-SQL (\textit{Text-to-SQL}). O segundo experimento, mais rigoroso (2025), introduziu \textit{workflows} nÃ£o-agÃªnticos como linha de base (\textit{baseline}) e empregou uma metodologia de avaliaÃ§Ã£o quantitativa e automatizada, utilizando um ``LLM-como-juiz'' (\textit{LLM-as-a-judge}), para avaliar o desempenho com base em precisÃ£o, revocaÃ§Ã£o (\textit{recall}) e F1-score.
    Este segundo ciclo produziu uma descoberta crucial e contraintuitiva: uma arquitetura RAG nÃ£o-agÃªntica, utilizando um roteador inteligente para selecionar a fonte de conhecimento apropriada, superou tanto os sistemas de agente Ãºnico quanto os multiagente. Este resultado levou Ã  hipÃ³tese central do ``dÃ©ficit de conhecimento de domÃ­nio'' (\textit{domain knowledge deficit}), que postula que, em domÃ­nios tÃ©cnicos altamente especializados onde o LLM carece de conhecimento profundo prÃ©-treinado, as capacidades de raciocÃ­nio cÃ­clico e reflexivo dos sistemas agÃªnticos sÃ£o menos eficazes do que um processo de recuperaÃ§Ã£o otimizado e bem direcionado. A pesquisa conclui que, para aplicaÃ§Ãµes industriais prÃ¡ticas, o foco arquitetÃ´nico deve estar na otimizaÃ§Ã£o dos mecanismos de recuperaÃ§Ã£o e roteamento, em vez de recorrer a \textit{frameworks} agÃªnticos mais complexos e dispendiosos, fornecendo uma orientaÃ§Ã£o clara e baseada em evidÃªncias para a adoÃ§Ã£o estratÃ©gica dessas tecnologias.

\end{abstract}
    
\begin{foreignabstract}
    
    The oil and gas industry, particularly in the domain of well construction, faces significant challenges in efficiently extracting knowledge from vast repositories of unstructured and semi-structured technical data. This dissertation evaluates and compares the effectiveness, efficiency, and practical viability of different Retrieval-Augmented Generation (RAG) architectures, ranging from non-agentic pipelines to single and multi-agent setups, for resolving domain-specific information retrieval challenges.
    Adopting a Design Science Research (DSR) methodology, this study was conducted through two distinct experimental cycles. The first cycle (2024) provided a foundational comparison, revealing that while a multi-agent architecture achieved 28\% higher truthfulness in Question-Answering (Q\&A) tasks, its operational cost was, on average, 3.7 times higher, and a single-agent system was surprisingly more effective for Text-to-SQL tasks. The second, more rigorous experiment (2025) introduced non-agentic workflows as a baseline and employed an automated, quantitative evaluation methodology using an LLM-as-a-Judge to assess performance based on precision, recall, and F1-score.
    This second cycle produced a crucial and counter-intuitive discovery: a non-agentic RAG architecture using an intelligent router to select the appropriate knowledge source outperformed both single and multi-agent systems. This finding led to the central hypothesis of the ``domain knowledge deficit'', which postulates that in highly specialized technical domains where the LLM lacks deep pre-trained knowledge, the cyclical reasoning and reflective capabilities of agentic systems are less effective than a streamlined, well-directed retrieval process. The research concludes that for practical industrial applications, architectural focus should be on optimizing the retrieval and routing mechanisms rather than defaulting to more complex and costly agentic frameworks, providing clear, evidence-based guidance for the strategic adoption of these technologies.
    
\end{foreignabstract}


\tableofcontents
\listoffigures
\listoftables
\printlosymbols
\printloabbreviations

\mainmatter


\input{part01_introducao.tex}

\input{part02_revisao.tex}

\input{part03_experimento1.tex}

\input{part04_experimento2.tex}

\input{part05_conclusao.tex}

% \input{part05a_conclusao2.tex}

\backmatter
\bibliographystyle{en-coppe-unsrt}
\bibliography{bib}

\appendix

\input{part10_apendice.tex}

\renewcommand{\appendixname}{Appendix}
\appendix

% \input{part11_anexo.tex}

\listofcomments

\end{document}

%% 
%%
%% End of file `example.tex'.

\chapter{Introduction}


% \section{Contextualization}


    % [IA-GEN NA INDUSTRIA] 
    In the dynamic and ever-changing oil and gas (O\&G) industry,
    \abbrev{O\&G}{oil and gas} 
    digital transformation has emerged as a key element to achieve operational efficiency, sustainability, and competitiveness. 
    At the forefront of this transformation are Large Language Models \abbrev{LLM}{Large Language Model}(LLM), which have the potential to process unstructured queries, map out courses of action, and advise users on possible solutions to industrial problems \citep{Kar2023}. 
    We also note the advantage of increased engagement, cooperation, accessibility, and ultimately profitability. 
    These models redefine paradigms in knowledge management and information retrieval and impact a variety of other areas \citep{Eckroth2023}, making it crucial to adopt these technologies to remain competitive.    
    
    % [ESTUDO AUMENTO PRODUTIVIDADE] 
    A study conducted by \citet{Dellacqua2023}, in collaboration with the Boston Consulting Group, shows that in knowledge-intensive tasks, consultants equipped with access to LLMs such as GPT-4 not only completed tasks more efficiently (25.1\% more quickly on average) but also with substantially higher quality, achieving results more than 40\% better compared to those without AI assistance \citep{Dellacqua2023}.
    Increase in productivity of knowledge workers was 12\% on average.    
    A major oil company spent in 2023 \$2.8B with employee compensation \citep{Petrobras2024}.
    A potential increase of 12\% in knowledge workers productivity, given they represent 60\% of all employee, could represent \$204M annual savings in this scenario. 
    \xexeo{Aqui estÃ¡ um problema clÃ¡ssico. VocÃª quer fornecer um dado mas provavelmente nÃ£o quer dizer que Ã© a Petrobras. Mas provavelmente esse dado estÃ¡ em algum relatÃ³rio pÃºblico. NÃ£o consegue ele de algum lugar e aÃ­ pode citar?} 
    \vitor{feito}   
    
    % [AUMENTO DO PIB DEVIDO A GEN AI] 
    Broader economic indicators predict significant transformations due to generative AI (Gen-AI) across various industries.
    A report from Goldman Sachs \citep{Hatzius2023} highlights that Gen-AI is poised to increase global GDP by nearly 7\%, increasing productivity growth by 1.5 percentage points over the next decade. 
    This economic uplift is expected due to AI's ability to automate complex workflows and create new business opportunities, significantly impacting employment and productivity sectors worldwide.


            
    % [PROBLEMA DE DADOS NA INDUSTRIA EM GERAL] 
    Expanding on the broader discussion on data utilization within organizations, an important issue is the challenge of extracting relevant information from extensive databases \citep{Singh2023}. 
    Initially, the challenge of knowing, finding, and accessing data poses a significant obstacle to decision-making processes. 
    Collaborators at O\&G companies often face the intensive task of manually searching large data repositories to find useful information.


    
    % [PROBLEMA DE DADOS NO O\&G] 
    Focusing specifically on the activities of drilling and completion of offshore and onshore wells, a major challenge lies in the inherently complex and technical nature of the data involved, which can be from various types: operations, projects, technologies, supply chains, and others. 
    Inefficiency in leveraging large volumes of unstructured data increases these challenges, as observed by \citet{Singh2023}. 
    A significant amount of the data generated and collected in this sector is unstructured, ranging from text reports and emails to images and videos of exploration and production activities. 
    Examples include hundreds of daily operational reports from drilling rigs, well execution projects, nonproductive time (NPT) reports, and documents of operational lessons learned, as illustrated in Figure \ref{fig:report_example}. 
    As a result, valuable information can remain untapped, and the potential to find insights, informed decision-making, and innovation is significantly compromised.
    \citet{Singh2023} showcases the capabilities and potential of Generative AI-enabled chatbots for the O\&G sector, particularly in enhancing drilling and production analytics to achieve better business results. The author concludes that companies that adopt these technologies in the coming years will see clear advantages.     
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{images/report_example.png}
        \caption{Sample of drilling \& completion learned lesson partial document. (translated from Portuguese)}
        \label{fig:report_example}
    \end{figure}           
    
    However, the deployment of such technologies presents limitations and introduces challenges, including biased data, hallucinations\footnote{Information that is incorrect or simply fictional.}, lack of explainability, and logical reasoning errors, among others \citep{Hadi2023}, which require a balanced approach to harness their potential in a responsible manner.    
    Although previous research has focused mainly on the broader applications of AI in industry, the novelty of our research lies in its original examination of the specific challenges and solutions presented by the complex, technical and unstructured data inherent in O\&G operations. 
    By comparing single- and multi-agent systems, this study fills a knowledge gap, providing empirical insights into the effectiveness of different Gen-AI architectures in a domain where such studies are scarce. 
    
    The adoption of these technologies by a major oil company underscores their potential to revolutionize data analysis and management, presenting an opportunity for deeper exploration and application.

\section{Business Scope Delimitation}

    To contextualize the scope of this study, it is necessary to understand the life cycle of an oil field, which begins with Exploration and progresses to the Development of Production, followed by effective Production, and culminates in Decommissioning \citep{Badiru2016}. Gen-AI has the potential to impact each of these phases, but the focus of this work lies in the operations of the development and maintenance stages.
            
    Well construction is a highly specialized activity that involves drilling and completion of wells for hydrocarbon extraction \citep{Thomas2004}. In this context, Gen-AI can be applied in various ways. 
    For example, a chatbot could manage knowledge by answering queries about operations and well projects by retrieving information from the organization's databases. 
    Additionally, LLM-based agents could be used in executive project review to ensure that drilling or completion operations comply with the organization's standards and adhere to best operational practices. 
    Moreover, Gen-AI could perform inference in unstructured databases to extract specific information from text reports and obtain structured data. This business scope emphasizes the importance of Gen-AI in the construction and maintenance of wells.

    \subsection{Key Information Sources in Well Engineering} \label{sec:information-sources}

        To fully appreciate the challenges in this domain, it is important to understand the primary data sources that specialists interact with daily. The following sources, used in this research's experiments, exemplify the complex information landscape of well engineering:

        \paragraph{Operational Learned Lessons.} During drilling, completion, and workover interventions, documents called Knowledge Items are written by specialists, as depicted in Fig~\ref{fig:report_example}. These can be of four types: Technical Alert, Learned Lesson, Good Practice, and Well Observation. This system serves as a critical tool for knowledge management, considering the large number and variety of specialists involved and well operations performed.

        \paragraph{Operational NPTs (Non-Productive Time).} This data source contains structured records of anomalies that occurred during well interventions, detailing the title, description, location, operation type, responsible sector, rig involved, time lost, and event dates. These data are critical for the industry, as NPTs represent periods when operations are interrupted. The identification and analysis of these events are essential for continuous process improvement, cost reduction, and increased operational efficiency.

        \paragraph{Collaborator Finder.} The third data source is a collaborator finder, an important internal tool for consulting and managing employee data. This system allows for the quick identification of employees through information such as name, workplace, and role. The importance of this tool lies in the ability to cross-reference employee data with operational events, enabling a more complete analysis by an intelligent agent.

\section{Objectives}

    The primary goal of this dissertation is to systematically evaluate and compare the effectiveness, efficiency, and practical viability of different LLM-based architectures for resolving domain-specific information retrieval challenges in well construction engineering. 
    This research aims to move beyond generalized performance metrics to provide specific, empirical insights into how architectural choices impact outcomes in a industrial setting.

    To achieve this overarching goal, the following specific objectives have been defined:

    \begin{enumerate}
        \item \textbf{Design and Implement LLM Artifacts:} designing and implementing a set of distinct retrieval-augmented generation (RAG)
        \abbrev{RAG}{Retrieval-Augmented Generation} 
        architectures, including non-agentic (baseline and router-based) and agentic (single-agent and multi-agent) systems, tailored to the operational context of well construction.

        \item \textbf{Evaluate Performance Quantitatively:} evaluating the performance of these artifacts on domain-specific tasks using both expert-led qualitative assessments and automated quantitative metrics, including truthfulness, precision, recall, and F1-score.

        \item \textbf{Analyze Cost-Effectiveness:} conducting a comparative analysis of the economic efficiency of each architecture, focusing on the trade-offs between performance gains and the computational costs associated with LLM API usage.

        \item \textbf{Derive Actionable Guidance:} identifying the key challenges, limitations, and failure modes of each architecture within a specialized technical domain, and to derive practical, evidence-based guidelines for the strategic adoption of these technologies in the oil and gas industry.
    \end{enumerate}




\section{Research Questions} \label{sec:research_questions}

    To guide this investigation and structure the research, the study addresses a central research question, which is broken down into three specific sub-questions. These questions will be formally answered in the conclusion, based on the evidence gathered from the two experimental cycles.

    \vspace{\baselineskip}
    \begin{tcolorbox}[colback=gray!10, colframe=gray!40, title=\textbf{Main Research Question}]
    How do different LLM based architectures, ranging from non-agentic RAG pipelines to multi-agent systems, compare in terms of performance, efficiency, and practical viability when applied to domain-specific information retrieval tasks in well construction engineering?
    \end{tcolorbox}
    \vspace{\baselineskip}

    \paragraph{RQ1: Performance and Task-Dependency} Which architecture (non-agentic, single-agent, or multi-agent) provides the highest factual accuracy and overall performance for different types of domain-specific tasks, specifically complex Question-Answering (Q\&A) and structured Text-to-SQL generation?

    \paragraph{RQ2: Cost-Effectiveness} What is the relationship between architectural complexity and economic cost? How do the performance benefits of more complex systems (e.g., multi-agent) weigh against their significantly increased computational (API) costs, and what are the implications for practical deployment?

    \paragraph{RQ3: Agentic Systems and Domain Specificity} Under what conditions do agentic architectures, with their capacity for cyclical reasoning and reflection, offer a tangible performance advantage over simpler, non-agentic RAG workflows in a highly specialized technical domain where the LLM has a significant ``knowledge deficit''?

    \vspace{\baselineskip}

    To answer these questions, this research was conducted through two distinct experimental cycles. The first, carried out in 2024, established a foundational comparison, revealing that while a multi-agent architecture achieved 28\% higher truthfulness in Q\&A tasks, its cost was on average 3.7 times higher. Furthermore, a single-agent architecture proved to be surprisingly more effective in Text-to-SQL tasks.

    The rapid evolution of generative AI frameworks and models prompted a second, more advanced experiment in 2025. This second phase built upon the initial findings, employing non-agentic workflows as a baseline and a more rigorous, automated evaluation methodology based on the ``LLM-as-a-Judge'' concept \citep{Gu2025}. This led to a crucial and counter-intuitive discovery: a non-agentic architecture using an intelligent router to select the correct knowledge source decisively outperformed both single and multi-agent systems. This finding suggests that in specialized domains where the LLM lacks deep pre-trained knowledge, the reflective capabilities of agentic systems are less effective than a streamlined, well-directed retrieval process, fundamentally shaping the answers to our research questions.



\section{Research Methodology}
  
    This research follows the Design Science Research (DSR)\abbrev{DSR}{Design Science Research} methodology, a framework particularly suited for studies that develop and evaluate technological artifacts to address specific organizational problems. DSR provides a structured approach for creating innovative solutions while maintaining scientific rigor through empirical validation \citep{hevner2007three}.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{images/dsr-model.png}
        \caption{Main elements of DSR-Model, translated from \citet{Oswald2023}.}
        \label{fig:dsr-model}
    \end{figure}
    
    \subsection{Design Science Research Framework}
    
        The DSR methodology employed in this study consists of four interconnected elements, as illustrated in Figure~\ref{fig:dsr-model}:
        
        \begin{enumerate}
        \item \textbf{Problem in Context}: Identifying and defining a relevant organizational challenge within its specific environment
        \item \textbf{Artifact}: Designing and developing a technological solution to address the identified problem
        \item \textbf{Behavioral Conjectures}: Formulating hypotheses about how the artifact will function and impact the problem space
        \item \textbf{Empirical Evaluation}: Systematically testing the artifact to validate its effectiveness and the underlying conjectures
        \end{enumerate}
        
        This cyclical framework guides both the research design and execution, ensuring that the developed artifacts are not only technically sound but also practically relevant.
    
    \subsection{Application of DSR in This Research} \label{sec:dsr-application}
        
        \subsubsection{Problem in Context}
        
        This study addresses the challenge of efficiently extracting relevant information from extensive technical databases in the oil and gas industry, specifically in well construction and maintenance operations, as listed in Table~\ref{tab:problem-context}. 
        
        \begin{table}[h]
            \centering
            \caption{Characteristics of the Problem Context}
            \begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
            \hline
            \textbf{Challenge Aspect} & \textbf{Description} \\
            \hline
            Data Structure & Large volumes of unstructured data (operational reports, lessons learned documents, NPT reports) \\
            \hline
            Technical Complexity & Domain-specific terminology, complex relationships and tacit knowledge \\
            \hline
            Business Impact & Significant potential economic impact from improved knowledge access \\
            \hline
            \end{tabular}
            \label{tab:problem-context}
        \end{table}
        
        \subsubsection{Artifacts}
        
        Four primary artifacts were designed and implemented across the two experimental cycles, illustrated in Figure~\ref{fig:artifacts}, using state-of-the-art language models and integrated with domain-specific knowledge bases through various retrieval mechanisms.
        
        \begin{figure}[h]
        \centering
        
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\parbox[c][3.8cm][c]{\linewidth}{\centering
                \textbf{Linear-Flow (Baseline)} \\
                \small Non-agentic RAG pipeline with a \\
                \small single sequential LLM step that \\
                \small queries all tools and synthesizes\\
                \small the answer}}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\parbox[c][3.8cm][c]{\linewidth}{\centering
                \textbf{Linear-Flow with Router} \\
                \small Non-agentic pipeline with an initial \\
                \small routing LLM call that selects the most \\
                \small appropriate tool(s) before retrieval}}
        \end{minipage}
        \vspace{0.6em}
        
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\parbox[c][3.8cm][c]{\linewidth}{\centering
            \textbf{Single-Agent LLM System} \\
            \small A centralized architecture where one \\
            \small language model agent handles the entire \\
            \small question-answering process with \\
            \small access to multiple tools}}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\parbox[c][3.8cm][c]{\linewidth}{\centering
            \textbf{Multi-Agent LLM System} \\
            \small A collaborative architecture where \\
            \small multiple specialized agents work together \\
            \small under coordination to process queries}}
        \end{minipage}
        \caption{Primary artifacts developed and evaluated across the two experimental cycles.}
        \label{fig:artifacts}
        \end{figure}
        
        
        \subsubsection{Behavioral Conjectures}
        
        The research was guided by several key conjectures:
        
        \begin{tcolorbox}[colback=gray!10, colframe=gray!40, title=Key Research Conjectures]
            \begin{itemize}
            \item Multi-agent systems will demonstrate higher accuracy in complex technical queries due to their ability to distribute cognitive load and specialize in different aspects of the problem
            \item The performance advantages of multi-agent systems will vary by task type (Q\&A vs. Text-to-SQL)
            \item More advanced language models will yield better performance but at significantly higher LLM financial costs
            \item The economic efficiency (performance-to-cost ratio) will be a critical factor in determining practical implementation viability
            \end{itemize}
        \end{tcolorbox}
        
        \subsubsection{Empirical Evaluation}
        
        The evaluation was conducted through two distinct experimental phases (summarized in Table~\ref{tab:experiments}), allowing for iterative refinement of both the artifacts and the evaluation methodology, addressing limitations identified in the first experiment while adapting to the rapid evolution of language model capabilities.
        
        \begin{table}[h]
        \centering
        \caption{Comparison of Experimental Phases}
        \begin{tabular}{|p{0.15\textwidth}|p{0.38\textwidth}|p{0.38\textwidth}|}
        \hline
        \textbf{Aspect} & \textbf{First Experiment (2024)} & \textbf{Second Experiment (2025)} \\
        \hline
        Focus & Comparative analysis of single and multi-agent architectures & Extended evaluation incorporating non-agentic workflows as baseline \\
        \hline
        Evaluation Methods & Expert validation by domain specialists & Automated assessment using LLM-as-a-Judge approach \\
        \hline
        Metrics & Truthfulness, performance, and LLM cost & Precision, recall, and F1-score \\
        \hline
        Outcomes & Identification of key challenges and limitations & More rigorous quantitative evaluation methodology \\
        \hline
        \end{tabular}
        \label{tab:experiments}
        \end{table}

    

\section{Thesis Structure}


    This dissertation is organized into five main chapters, followed by appendices, to present the research in a logical and structured manner.

    \begin{itemize}
        \item \textbf{Chapter 1 - Introduction:} This chapter introduces the research context within the oil and gas industry, highlighting the challenges of knowledge management in well construction. It defines the business scope, establishes the research objectives and guiding research questions, and outlines the DSR methodology that structures the study.

        \item \textbf{Chapter 2 - Literature Review:} This chapter provides the theoretical foundation for the research. It reviews the key concepts of LLMs, RAG, and the architecture of both single and multi-agent systems. It also covers the evaluation methodologies pertinent to this work, including traditional metrics and the LLM-as-a-Judge paradigm.

        \item \textbf{Chapter 3 - First Experimental Evaluation Cycle:} This chapter details the initial experiment comparing single-agent and multi-agent architectures. It follows the DSR framework to describe the artifact design, the expert-led evaluation process, and the results based on metrics of truthfulness, performance, and cost. The findings from this cycle reveal the initial trade-offs and limitations that motivate the second experiment.

        \item \textbf{Chapter 4 - Second Experimental Evaluation Cycle:} This chapter presents a more rigorous and extensive evaluation. It introduces non-agentic workflows as baselines and employs an automated, quantitative evaluation methodology using an LLM-as-a-Judge approach. The results from this cycle provide crucial insights into the performance of different architectures, leading to the central hypothesis of the ``domain knowledge deficit''.

        \item \textbf{Chapter 5 - Conclusion:} This final chapter synthesizes the findings of the entire study. It provides direct answers to the research questions, summarizes the main contributions to theory and practice, acknowledges the limitations of the work, and proposes promising directions for future research.
    \end{itemize}

    Finally, the \textbf{Appendices} provide supplementary material, including the detailed dataset used in the experiments and the source code for the evaluation prompts, to ensure transparency and facilitate reproducibility.
    
\chapter{Literature Review} 

    This chapter provides a comprehensive literature review of the key technologies and concepts that form the foundation of this dissertation. It begins with an overview of the applications of Artificial Intelligence (AI) in the Exploration and Production (E\&P) industry. The focus then narrows to LLMs, discussing their architecture and impact. Subsequently, the chapter delves into the RAG technique, which enhances LLMs with external knowledge. It also explores the use of single and multi-agent setups. Finally, the chapter concludes by examining the LLM-as-a-Judge paradigm for evaluating the performance of generative models.

    \section{AI in the Exploration and Production (E\&P) industry}

        The use of AI in the Exploration and Production (E\&P) industry has been extensive. 
        In the last decades the majority of AI applications in the industry involved data mining and neural networks \citep{Bravo2014}. 
        An example is the work by \citep{Gudala2021} on optimization of the properties of the heavy oil flow, through the use of neural networks to optimize flow-influencing parameters.
        Another development was a deep learning workflow proposed by \citep{Gohari2024}, with the generation of synthetic graphic well logs through the application of transfer learning. 
        These developments illustrate the potential of AI to improve processes and the accuracy and efficiency of data analysis \citep{Rahmani2021}.

        Recent studies highlight domain-specific advances in textual AI for geosciences, particularly in Named Entity Recognition (NER) under low-resource conditions. \citet{maze2024textual} proposed a two-phase pipeline that (i) builds a high-quality, semi-automatically labeled dataset via ontology-driven rules, taxonomies, and expert validation, and (ii) augments it using LLM-based rephrasing constrained to preserve entities, cosine-similarity filtering to ensure semantic fidelity and diversity, and entity substitution from curated whitelists. The augmented corpus substantially improved downstream BERT-based NER performance on petroleum technical documents, evidencing the practicality of LLM-driven augmentation for metadata extraction at scale.
    
        Natural Language Processing (NLP) stands at the intersection of computer science and linguistics, representing a domain within artificial intelligence aimed at enabling computers to understand and process human language in a way that is both meaningful and effective \citep{Liddy2001}. 
        This field integrates a diverse range of computational techniques to analyze and represent text at various levels of linguistic detail, striving to emulate human-like language understanding. 
        As an active area of research, traditionally NLP employs multiple layers of language analysis, each contributing uniquely to the interpretation and generation of language, which finds practical applications in various sectors \citep{Liddy2001}.      
        In the O\&G industry, the management of unstructured data, such as texts, images, and documents, is crucial, with NLP and Machine Learning playing key roles.
        Research by \citet{Antoniak2016} and \citet{Castineira2018} has explored the use of NLP to analyze risks and drilling reports.           
        
        Complementing these efforts, \citet{gharieb_role_2024} outline a roadmap for personalized, on-premises LLMs tailored to petroleum engineering and education. Their pipeline benchmarks embeddings and chunking strategies for retrieval. Results indicate that smaller, locally hosted LLMs can deliver competitive summarization and knowledge-integration performance with reduced latency and lower operating costs.
        Extending to drilling operations, \citet{yi2024applications} demonstrate a GPT-based system with retrieval over a curated corpus spanning sensor logs, reports, after-action reviews, and external well construction planning and real-time Q\&A. Reported outcomes include significant time savings in retrieving past incident context (e.g., stuck pipe) and benchmarking parameters (e.g., lateral-section ROP).

    \section{Natural Language Processing} \label{sec:nlp-review}

        NLP is a broad field that covers various tasks to enable computers to process and understand human language \citep{jurafsky2025}. These tasks, which represent specific problems or applications, have been the focus of research for decades, predating the recent surge in LLMs. They range from fundamental challenges like part-of-speech tagging to complex applications like machine translation. This section explores two tasks particularly relevant to this dissertation: Q\&A and Text-to-SQL, both of which have been significantly advanced by recent developments in the field.

        \subsection{Q\&A tasks}    

            Q\&A can be viewed from two complementary perspectives. From the organizational view, Q\&A serves as a mechanism to facilitate knowledge transfer between individuals \citep{Iske2005}. Platforms such as Stack Overflow illustrate how structured question-and-answer workflows support technical communities \citep{Treude2011}. This understanding helps organizations design processes that enhance knowledge transfer and learning.

            From the artificial intelligence perspective, \textit{automated question answering} is a long-standing research area in NLP that aims to answer user queries automatically from available evidence (documents, databases, or parametric model knowledge). 
            % Typical task taxonomies include: (i) \textbf{closed-domain} vs. \textbf{open-domain} QA, depending on whether the knowledge source is restricted; (ii) \textbf{extractive} QA, which selects spans from a context, versus \textbf{abstractive/generative} QA, which produces novel text; and (iii) \textbf{closed-book} QA, where the model relies solely on parametric memory, versus \textbf{open-book} QA that consults external sources.
            % Modern open-domain QA systems commonly follow a \textit{retrieverâ€“reader} (or retrieverâ€“generator) architecture: a retriever selects candidate passages, and a reader/generator produces the answer. Dense retrieval methods such as DPR \citep{Karpukhin2020} outperform traditional sparse retrieval for passage selection, while Retrieval-Augmented Generation (RAG) \citep{Lewis2020} integrates the retrieved evidence directly into the prompt for an LLM to generate grounded answers. These approaches reduce hallucination and improve factuality compared to purely parametric generation.
            In specialized settings, domain-specific Q\&A adds constraints such as terminology, safety, and privacy. 
            Recent work explores cost-efficient, domain-specific Q\&A with LLMs by optimizing retrieval and context selection \citep{Arefeen2024}. 
            In the petroleum context specifically, applications have leveraged GPT-style models to answer natural-language questions over proprietary corpora and operational documents \citep{Eckroth2023}, aligning with the retrieval-and-generation paradigm adopted in this dissertation. 
            Together, these advancements motivate the use of RAG pipelines for auditable Q\&A in E\&P environments.

        \subsection{Text-to-SQL tasks} 

            Text-to-SQL tasks in the context of artificial intelligence involve the automatic translation of natural language questions or commands into structured SQL (Structured Query Language) queries \citep{Qin2022}. This is an important area in NLP, allowing users to interact with databases using plain language rather than needing to know how to write complex SQL queries.         
                
            The arrival of advanced language models like GPT-3 and GPT-4 \citep{OpenAImodels} has marked a significant leap in Text-to-SQL applications \citep{Singh2023}, demonstrating remarkable capabilities in handling these tasks. This can be attributed to their extensive training on diverse datasets \citep{Deng2021}, which include not only large amounts of text but also structured data like tables and code, enabling the model to understand the intricate relationships between language and data structures. The study by \citep{Deng2023} introduces a pre-training framework for text to SQL translation, emphasizing the alignment between text and tables in Text-to-SQL tasks.



    \section{Intelligent Agents}         

        According to \citet{Russell2020}, an agent is something that performs actions. When it comes to computerized agents (in our case, AI-based), these agents are expected to do more: operate autonomously, perceive the environment, persist over time, adapt to changes, create, and strive to achieve goals. The agent program implements the agent function.

        \citet{Russell2020} present a taxonomy of agent programs that we adopt here in \textit{increasing order of complexity}:
        \begin{enumerate}[label=(\alph*)]
            \item \textbf{Simple reflex}: act based solely on the current percept using conditionâ€“action rules.
            \item \textbf{Model-based reflex}: maintain an internal state (a world model) to handle partial observability.
            \item \textbf{Goal-based}: choose actions that achieve explicitly represented goals, enabling lookahead and planning.
            \item \textbf{Utility-based}: select actions to maximize an expected utility over outcomes when trade-offs exist.
            \item \textbf{Learning/adaptive}: improve performance over time by learning components such as perception, model, or utility.
        \end{enumerate}
        The appropriate design depends on the environment and task constraints. In this work, a \textbf{goal-based} agent design was implemented to act toward achieving defined objectives.


        \subsection{Multi-Agent Systems}

            A Multi-Agent System (MAS) extends the concept of a single agent to a collection of agents that interact within a shared environment \citep{Gokulan2010}. A MAS is defined as a loosely coupled network of autonomous problem-solving entities that collaborate to find solutions to problems that are beyond the individual capabilities or knowledge of any single entity \citep{FloresMendez1999}. 

            The structure of a MAS can vary, with different organizational paradigms such as hierarchical structures or coalitions being employed depending on the application \citep{Gokulan2010}. A practical example of a MAS architecture is demonstrated in power system restoration, where a system can be composed of multiple "bus agents" and a single "facilitator agent" \citep{Nagata2002}. In this setup, each bus agent works to restore its local area by negotiating with neighboring agents based on locally available information, while the facilitator agent manages the overall decision-making process, showcasing how a collection of agents with simple, local strategies can cooperate to achieve a complex, global goal \citep{Nagata2002}.



    \section{Large Language Models}         

        LLMs are advanced neural network-based models designed to understand and generate human-like text. 
        They leverage the Transformer architecture introduced in the seminal paper \enquote{Attention is All You Need} by \citet{Vaswani2017}. 
        This architecture relies on self-attention mechanisms, allowing the model to weigh the importance of different words in a sentence effectively. 

        In practice, contemporary generative LLMs are typically \textit{decoder-only} Transformer models, stacking decoder blocks with causal self-attention to autoregressively produce tokens. By contrast, widely used classifiers such as BERT adopt an \textit{encoder-only} configuration that produces contextualized representations for discrimination tasks rather than generation \citep{Devlin2018}. 

        The emergence of LLMs has made it possible to understand and produce textual information. 
        These systems are expected to revolutionize various industries by supporting complex decision-making processes. GPT models \citep{OpenAI2023}, in particular, take advantage of its vast training data to provide human-like responses \citep{Mosser2024}, which can be highly beneficial in contexts requiring natural language understanding and generation. The exponential growth in the size and capability of LLMs in recent years has been remarkable. Models like OpenAI's GPT series have shown significant advancements, moving from millions to hundreds of billions of parameters, which gives them increasingly sophisticated natural language understanding and generation. This advancement is illustrated in Figure~\ref{fig:llm_evolution}. For new models (released after jan/2025), including OpenAI's o3 series and GPT-4.5, Anthropic's Claude 3.7 and 4, and Google's Gemini 2.5 Pro, the exact parameter counts have not been publicly disclosed. 

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\textwidth]{images/llm_evolution.png}
            \caption{The evolution of LLMs.}
            \label{fig:llm_evolution}
        \end{figure}
                
        However, the trajectory of LLM development in 2025 has signaled a shift in focus. While previous advancements were often marked by an exponential increase in parameter counts, the latest generation of models emphasizes sophisticated reasoning capabilities over sheer size. 
        This move away from parameter size as the primary metric of progress underscores a new trend: enhancing the models' ability to perform complex, multi-step reasoning. 
        This is evident in features like the private chain-of-thought mechanisms in OpenAI's models and the "extended thinking" mode in Anthropic's Claude series, indicating that language models are advancing through more intricate cognitive architectures rather than just scaled-up data processing.

        As highlighted by \citet{Singh2023}, the integration of LLM-based solutions, such as conversational chatbots, offers an approach to optimizing operations across various business segments, including drilling, completion, and production.
        \citet{Singh2023} uses LLMs models to extract, analyze, and interpret datasets, enabling generation of insights and recommendations. 

        Despite its widespread impact, language models are not without its limitations. 
        In many industry-specific applications, the critical information required is often proprietary, not shared with third parties, and thus absent from the training data of these LLMs \citep{Mosser2024}. 
        This gap means that GPT models might not have access to the most up-to-date or sensitive information needed for certain tasks. 
        Moreover, due to their probabilistic nature, LLMs can experience hallucinations, producing confident yet incorrect or nonsensical responses based on user input \citep{OpenAI2023}. 
    
    
        \subsection{LLM applications}

            The proliferation of LLMs has led to a diverse array of applications that leverage their ability to understand, generate, and process human language.

            The expansion of the LLM application ecosystem is evident in the significant market growth projections. For instance, one report projects the global LLM market to grow from \$5.62 billion in 2024 to \$35.43 billion by 2030, with a compound annual growth rate (CAGR) of 36.9\% \citep{GrandViewResearch2025}. This rapid expansion is indicative of the immense value and potential that organizations across industries see in these technologies. The applications themselves are becoming increasingly sophisticated, evolving from simple text generation to complex, multimodal systems capable of processing and integrating text, images, and other data formats \citep{Kaddour2023}.
            
            The spectrum of LLM-based applications is broad and continually expanding. Early applications focused on tasks such as text summarization, translation, and sentiment analysis. However, the current generation of LLMs powers a much wider range of tools. These can be broadly categorized into several key areas. Conversational AI, in the form of advanced chatbots and virtual assistants, represents a significant segment of the market, enhancing customer service and user engagement \citep{GrandViewResearch2025}. Content creation is another major application area, where LLMs are employed to generate a variety of materials, from marketing copy and social media posts to technical documentation and even creative writing \citep{V7Labs2025}.            
            
            Furthermore, LLMs are being integrated into more specialized and high-stakes domains. In the legal field, they assist with tasks like contract analysis and legal research. The financial sector utilizes them for fraud detection and market analysis \citep{V7Labs2025}. In software development, LLM-powered tools for code generation and debugging are becoming increasingly prevalent, accelerating development cycles and improving programmer productivity. A key innovation driving the utility of these applications is the advent of techniques like RAG, which allows LLMs to retrieve and incorporate information from external knowledge bases, thereby improving the accuracy and relevance of their outputs \citep{KeywordsAI2025}. The ongoing development of multimodal LLMs is further pushing the boundaries of what is possible, enabling applications that can understand and reason about the world in a more holistic manner \citep{Kaddour2023}.
        
        \subsection{RAG} 

            RAG technique combines LLMs with information retrieval to generate accurate and up-to-date responses, as introduced by \citet{Lewis2020}. 
            It employs a search in a database to find relevant information, overcoming the inherent limitations of LLMs that rely solely on the prior knowledge embedded in the language model during the training phase. 
            With the ongoing evolution of information retrieval, which has evolved from term-based methods to more semantic approaches leveraging deep learning and large datasets to tackle more complex challenges.
            
            A RAG consists of two main components: a retriever and a generator, as illustrated in Figure~\ref{fig:rag_diagram}. The retriever is responsible for finding relevant information from a knowledge base, and the generator uses that information to create a human-like response. 
            
            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.4\textwidth]{images/rag_diagram_vertical.png}
                \caption{A diagram illustrating the RAG process.}
                \label{fig:rag_diagram}
            \end{figure}         

            As elucidated by \citet{Lewis2020}, RAG unites the strengths of pre-trained parametric and non-parametric memory, using a dense vector index and a semantic retriever. 
            As demonstrated by \citet{Li2022} in their analysis, RAG is surpassing traditional generative models in terms of performance across a variety of tasks. The study provides a detailed survey on this topic, emphasizing the fundamental concepts and its applicability in specific contexts.

            New tools have been developed to facilitate the implementation of RAG solutions. \citet{Liu2023} present a toolkit that integrates augmented retrieval techniques into LLMs, including modules for query rewriting, document retrieval, passage extraction, response generation, and fact-checking, enabling the creation of more factual and specific responses. The recent study by \citet{Zhao2023} extends this horizon by examining the incorporation of multimodal knowledge into generative models, exploring the integration of diverse external sources such as images, code, tables, graphs, and audio, to enhance the grounding context and improve usability. It also explores potential future trajectories in this emerging field, marking a relevant contribution to the evolving narrative of RAG and its applications.

            
        \subsection{Multi-Agent Setup} 

            As demonstrated by \citet{xi2023rise}, the pursuit of Artificial General Intelligence\footnote{AGI is the ability of a machine to perform any intellectual task that a human can perform.} (AGI) has significantly benefited from the development of LLM-based agents, capable of sensing, decision-making, and acting across diverse scenarios.  
            His study outline a foundational framework for such agents, consisting of brain, perception, and action components, which can be customized for various applications including single-agent scenarios, multi-agent systems, and human-agent collaboration. 
            The comprehensive survey underscores the crucial role of LLMs in moving towards AGI, suggesting a promising horizon for operational efficiency and decision-making processes in complex organizational settings \citep{xi2023rise}.

            \citet{Li2024} demonstrated that, through a sampling and voting method, the performance of LLMs scales with the number of instantiated agents.
            Another open-source framework is AutoGen \citep{Wu2023}, that enables the creation of LLM multi-agent applications, allowing for customization across various modes. It supports diverse applications in fields such as mathematics, coding, and operations research, demonstrating its effectiveness through empirical studies \citep{Wu2023}.

            
        \section{Evaluation} \label{sec:evaluation-review}

            \subsection{Truthfulness}

                In the evaluation of RAG systems, ensuring the truthfulness of the generated output is a primary concern. \citet{Lin2022} introduces a framework for this purpose. The authors define a truthful answer as one that aligns with literal truth about the real world. This is particularly relevant for RAG systems, which can retrieve and incorporate information from vast and varied sources. An answer is considered truthful if it does not assert any false statements, and informative if it provides relevant information that addresses the user's query.
                
                In \citet{Li2023}, the authors conducted an evaluation to determine the effectiveness of their proposed prompts on the performance of various LLMs. The evaluation employed both automated standard experiments and human studies to assess the impact of emotional stimuli on task performance, truthfulness, and responsibility.

                In the first experiment of this study, human experts assessed each Q\&A pair based on the definitions:

                \begin{quoting}[font={small,itshape},indentfirst=false]
                    \begin{itemize}
                    \item \textbf{Truthfulness}: a metric to gauge the extent of divergence from factual accuracy, otherwise referred to as hallucination \citep{Lin2021}.
                        \subitem 1=â€œThe response promulgates incorrect information, detrimentally influencing the ultimate interpretationâ€
                        \subitem 2=â€œA segment of the response deviates from factual accuracy; however,this deviation does not materially affect the ultimate interpretationâ€
                        \subitem 3=â€œThe response predominantly adheres to factual accuracy, with potential for minor discrepancies that do not substantially influence the final interpretationâ€
                        \subitem 4=â€œThe response is largely in consonance with factual evidence, albeit with insignificant deviations that remain inconsequential to the final interpretationâ€
                        \subitem 5=â€œThe response is in meticulous alignment with the facts, exhibiting no deviationsâ€
                                
                    \item \textbf{Performance}: encompasses the overall quality of responses, considering linguistic coherence, logical reasoning, diversity, and the presence of corroborative evidence.
                        \subitem 1 = â€œThe response fails to address the question adequatelyâ€
                        \subitem 2 =â€œThe response addresses the question; however, its linguistic articulation is sub-optimal, and the logical structure is ambiguousâ€
                        \subitem 3 = â€œThe response sufficiently addresses the question, demonstrating clear logical coherenceâ€
                        \subitem 4 = â€œBeyond merely addressing the question, the response exhibits superior linguistic clarity and robust logical reasoningâ€
                        \subitem 5 = â€œThe response adeptly addresses the question, characterized by proficient linguistic expression, lucid logic, and bolstered by illustrative examplesâ€\citep{Lin2021}.         
                    \end{itemize}
                \end{quoting}

            \subsection{Precision, Recall, and F1-Score} \label{sec:precision_recall_f1_review}
            Precision, recall, and F1-score are fundamental metrics for evaluating classification tasks, particularly in scenarios with imbalanced datasets. These metrics provide a more nuanced understanding of a model's performance than accuracy alone.

            In a binary confusion matrix, we denote: \textbf{TP} (True Positives), \textbf{FP} (False Positives), \textbf{TN} (True Negatives), and \textbf{FN} (False Negatives). The formulas below use these standard abbreviations.

                \textbf{Precision} measures the accuracy of positive predictions. It is the ratio of correctly predicted positive observations to the total predicted positive observations. A high precision relates to a low false positive rate.
                \begin{equation}
                    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                    \label{eq:precision}
                \end{equation}

                \textbf{Recall} (or Sensitivity) measures the ability of the model to find all the relevant cases within a dataset. It is the ratio of correctly predicted positive observations to all observations in the actual class.
                \begin{equation}
                    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                    \label{eq:recall}
                \end{equation}

                The \textbf{F1-score} is the harmonic mean of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is a good way to show that a model has a good performance on both metrics.
                \begin{equation}
                    \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                    \label{eq:f1-score}
                \end{equation}


        \subsection{LLM-as-a-Judge}

            The LLM-as-a-Judge paradigm represents a significant shift in the evaluation of NLP systems in general, using a language model as a scalable proxy for human evaluators \citep{li2024llmsasjudgescomprehensivesurveyllmbased}. 
            This approach was developed to overcome the semantic shallowness of traditional metrics like BLEU or ROUGE and the logistical challenges of extensive human annotation \citep{Zheng2023}. 
            By providing a "judge" LLM with a clear rubric and context, it can perform assessments of qualities like coherence, relevance, and factual accuracy \citep{li2024llmsasjudgescomprehensivesurveyllmbased}. 
            This method has proven effective for complex, open-ended tasks where simple string matching is insufficient, with models like GPT-4 demonstrating over 80\% agreement with human preferences in benchmarking studies \citep{Zheng2023}.

            For evaluating RAG systems, the LLM-as-a-Judge framework can be adapted to produce structured, quantitative assessments. 
            In this application, the judge LLM is tasked with comparing the RAG-generated answer against a ground-truth dataset.
            By using a crafted prompt that defines the classification criteria, the judge can systematically categorize each output with domain-tailored definitions: \textbf{TP} = a claim is made and supported by the evidence; \textbf{FP} = a claim is made but not supported by the evidence; \textbf{FN} = a claim is not made but is supported by the evidence (it should have been made); \textbf{TN} = a claim is not made and not supported by the evidence. In this domain, TN is less informative and not emphasized in the analysis. This approach moves beyond subjective scoring towards a more objective evaluation. The prompt used in this work is presented in the code in Appendix~\ref{code:llm-judge}.

            The advantage of this methodology is its ability to translate qualitative judgments directly into a confusion matrix, allowing the calculation of standard metrics such as precision (Equation~\ref{eq:precision}), recall (Equation~\ref{eq:recall}), and F1-score (Equation~\ref{eq:f1-score}). This process establishes a replicable pipeline for benchmarking the factual accuracy of a RAG system at scale. While it is important to acknowledge the potential for inherent biases in LLM judges \citep{Gu2025}, studies show high correlation with human-expert evaluations \citep{li2024llmsasjudgescomprehensivesurveyllmbased}, making it a useful tool for iterative development and system comparison.
\chapter{First Experimental Evaluation Cycle} 
\label{chap:first_experiment}

    This chapter describes the first experimental cycle of this research, as introduced in Section~\ref{sec:dsr-application}, conducted to investigate the effectiveness of different LLM based agent architectures. The primary objective is to address complex, domain-specific queries within the field of well construction and maintenance. This initial cycle serves as a foundational study, comparing single-agent and multi-agent systems to generate empirical insights into their performance, cost, and inherent limitations. The findings from this cycle will inform the more advanced, quantitative evaluation performed in the second experiment.

    Following the principles of DSR, this chapter is structured to clearly present the research components. We will begin by defining the business context and the specific problem this experiment aims to solve. Subsequently, we will describe the design of the proposed technological solutions, referred to as artifacts. Finally, we will detail the evaluation methodology, including the process for data set creation, the metrics used for assessment, and a thorough analysis of the results.
    
    \section{Design Science Research Framework}
    
        To provide a clear and organized structure for this experiment, we adopt the DSR framework. The key components of this research cycle are outlined as follows:

        \begin{description}
            \item[Context:] The operational environment of the well construction department within a major oil company, where efficient access to technical knowledge is critical.

            \item[Problem:] The challenge faced by engineers and specialists in effectively querying and retrieving accurate information from vast, unstructured, and domain-specific knowledge bases (e.g., operational reports, lessons learned).

            \item[Behavioral Conjectures:] Our core supposition is that LLM-based agent systems can improve the efficiency and accuracy of information retrieval for specialized tasks, but that the choice of architecture (single-agent vs. multi-agent) will have a measurable impact on performance and cost.

            \item[Theoretical Frameworks:] This work is grounded in the theories of Intelligent Agents, RAG, and multi-agent systems, as detailed in the Literature Review.

            \item[Proposed Artifacts:] Two distinct LLM-based agent systems are proposed and built:
            \begin{itemize}
                \item A Single-Agent Architecture.
                \item A Multi-Agent Architecture.
            \end{itemize}

            \item[Evaluation:] The artifacts are evaluated by a panel of domain experts who assess the quality of their responses to a curated set of real-world queries. The evaluation is based on predefined metrics for truthfulness, performance, and cost.
        \end{description}

    \section{Context and Problem Statement}

        \subsection{Context}

            As established in the Introduction, this research is situated within the oil and gas industry, a sector characterized by complex, expensive operations. This experiment was carried out specifically within the well construction department of a major oil company. In this environment, engineers and technical staff frequently need to access specialized information from a variety of internal data sources, including operational reports, learned lessons, and safety alerts. The efficiency and accuracy of this information retrieval process directly impact operational decision-making, safety, and cost-effectiveness.

            The set of queries used to test the systems, listed in Appendix~\ref{app:dataset}, provides a concrete exemplification of the problem space.

        \subsection{Problem}

            The central problem addressed in this experiment is the inefficiency of technical knowledge management and data analysis in the well construction domain. Specialists often struggle to find precise answers to their queries, which are typically buried in large volumes of unstructured or semi-structured documents. This leads to time-consuming manual searches and the risk of overlooking critical information.

            This experiment investigates two primary task categories that exemplify this problem, as described in Section~\ref{sec:nlp-review} and summarized here:
            
            \begin{itemize}
                \item \textbf{Q\&A Tasks:} Require the system to answer complex technical questions by synthesizing information from documents. For example: ``How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures?''
                \item \textbf{Text-to-SQL Tasks:} Require the system to query structured databases using natural language. For example: ``What was the longest-lasting NPT on rig number 05?''
            \end{itemize}
            
            The set of queries used to test the systems, listed in Appendix A, provides a concrete exemplification of the problem space.

    \section{Proposed Artifacts}

            To address the problem, we designed, built, and tested two distinct artifacts: a single-agent solution and a multi-agent solution. Both are goal-based agents designed to accurately respond to user queries by leveraging a suite of tools.

        \subsection{Artifact 1: Single-Agent Architecture.}

            In this work, a goal-based agent \citep{Russell2020} was implemented with the goal of accurately responding to various queries. 
            The agent operates within an environment equipped with multiple tools for task-specific operations, as shown in Figure~\ref{fig:agent_environment}, and interfaces with users to receive queries.
            
            \begin{figure}[h]
                \centering
                \includegraphics[width=0.75\textwidth]{images/agent_environment_4.png}
                \caption{Schematic of the LLM-based agent interacting with an environment containing tools for task-specific operations, and the Human Agent interface for user interaction and feedback.}
                \label{fig:agent_environment}
            \end{figure}           
            
            Initially, a configuration of agents was implemented as described in Figure~\ref{fig:agent_config_1} using AutoGen Framework \citep{Wu2023} with an architecture that allows information retrieval and user interaction. This system consists of two agentic setups:

            \begin{figure}[h]
                \centering
                \includegraphics[width=.5\textwidth]{images/agent_config_1.png}
                \caption{Chat setup with one User Proxy \citep{Wu2023} and one Assistant.}
                \label{fig:agent_config_1}
            \end{figure}

            \begin{itemize}        
                        
                \item \textbf{User Proxy:} represents the interface with the user and with tools to access external databases. The modular nature of the tools allows the User Proxy to be customized and expanded based on the variety of data sources and the specific requirements of the application domain.

                \item \textbf{Agent:} powered by LLMs such as GPT-4 and GPT-3 (the specific model is configurable), is the analytical engine of the system. This agent interprets the queries received from the User Proxy and formulates responses.
                                    
            \end{itemize}

            
            For each question in the data set, the agent's decision-making process is executed as described in Figure~\ref{fig:diagrama_agente_1}, initially selecting the appropriate tool to respond to a query and, finally, compiling the retrieved information to provide a final answer.

            \begin{figure}[h]
                \centering
                \includegraphics[width=0.75\textwidth]{images/agent_diagram_1.png}
                \caption{Decision process of the agent.}
                \label{fig:diagrama_agente_1}
            \end{figure}

        \subsection{Artifact 2: Multi-Agent Architecture}

            The second artifact is a multi-agent system where responsibility is distributed among several specialized agents, coordinated by a Chat Manager, as shown in Figure~\ref{fig:agent_config_2}. This architecture is designed to handle queries by routing them to the agent best equipped for the task. As depicted in the decision process in Figure~\ref{fig:diagrama_agente_MultiAgente_2}, a \enquote{speaker selection} step determines the most suitable agent to act at each turn, promoting a more focused and contextualized approach to problem-solving.

            % (Your Figure agent\_config\_2 would be Figure 3.3 here)            
            \begin{figure}[h]
                \centering
                \includegraphics[width=.75\textwidth]{images/agent_config_2.png}
                \caption{Chat setup with one Chat Manager and a group of LLM agents.}
                \label{fig:agent_config_2}
            \end{figure}
            
            % (Your Figure diagrama\_agente\_MultiAgente\_2 would be Figure 3.4 here)
            \begin{figure}[h]
                \centering
                \includegraphics[width=1\textwidth]{images/agent_diagram_2.png}
                \caption{Multi-agent decision process.}
                \label{fig:diagrama_agente_MultiAgente_2}
            \end{figure}

                

        \subsection{Agent's Tools}
            
            In this experiment, three tools were considered in the decision-making process:

            \begin{itemize}            
                
                \item \label{Tool1} \textbf{Tool 1 - Learned Lessons Search:} a tool to search for learned lessons that may be relevant to the query. 
                
        
                \item \label{Tool2} \textbf{Tool 2 - Employee Search:} functionality that allows the search for information related to collaborators of an organization.
        
                \item \label{Tool3} \textbf{Tool 3 - NPT SQL Query (text-to-sql):} Interface for executing SQL queries on a database of operational NPTs.    
                
            \end{itemize}

            There is also a pathway that allows the agent to provide a direct response, without the need to resort to other tools, presumably used when the LLM already possesses the necessary information.

    \section{Evaluation}

        The evaluation phase was designed to assess and compare the performance of the two proposed artifacts. This section details the methodology, the data set creation process, the metrics used, and the final results.

        \subsection{Evaluation Methodology}
        
            The evaluation was conducted by presenting a standardized set of questions to both the single-agent and multi-agent systems, using both GPT-3.5-turbo and GPT-4 models. The responses generated by each configuration were then collected and anonymized.

            A panel of three specialist engineers from the well construction department was tasked with analyzing the generated answers. Each specialist independently scored the responses based on the metrics described in Section~\ref{sec:evaluation_metrics}. The final score for each response was calculated by averaging the scores from the three experts, ensuring a robust and comprehensive assessment.
            
            \xexeo{Aqui seria bom fazer um BPMN do passo a passo do seu experimento, veja a figura 4.1 de\url{https://www.cos.ufrj.br/uploadfile/publicacao/3172.pdf}}
            \vitor{Feito}

            To provide a clear visual representation of the experimental workflow, a Business Process Model and Notation (BPMN) diagram is presented in Figure~\ref{fig:experimental_workflow}. This diagram illustrates the step-by-step process, from query submission to expert evaluation.

            \begin{landscape}
            \begin{figure}[p]
                \centering
                \includegraphics[width=1.5\textwidth]{images/bpmn_experimento_1.png}
                \caption{Experimental workflow.}
                \label{fig:experimental_workflow}
            \end{figure}
            \end{landscape}

            
        \subsection{Dataset Creation}

            A critical component of this evaluation is the test dataset. The dataset was meticulously created to reflect authentic information needs within the well construction domain. The process was as follows:

            \begin{description}
                \item[Source Selection] We identified three primary internal data sources: a database of Operational Learned Lessons, a structured database of Non-Productive Time (NPT) incidents, and a Collaborator Finder tool, as described in Section~\ref{sec:information-sources}.
                \item[Document Sampling] A random sample of documents and records was selected from each data source to ensure broad coverage of topics and scenarios.
                \item[Query Formulation] This process was performed by the author, leveraging domain expertise and collaboration with colleagues to ensure the questions were realistic, relevant, and challenging.
                \item[Dataset Composition] In total, a dataset of 33 unique queries was created. 
            \end{description}

            This approach to dataset creation, grounded in author experience and real-world documents, provides a valid basis for evaluating the artifacts. Table~\ref{table:question_examples} presents a sample of the queries formulated for the experiment.

            
            \begin{table}[h]
                \centering
                % \scriptsize
                \footnotesize
                \sloppy
                \begin{tabular}{|p{.1\linewidth}|p{.9\linewidth}|}
                \hline
                \textbf{Task category} & \textbf{Question} \\   \hline
                \multirow{17}{*}{Q\&A} & How does the presence of silica in the composition of cement 
                paste affect its thermal stability at high temperatures? \\ \cline{2-2}
                & What are the main challenges and risks associated with through tubing plug and abandonment in highly deviated wells? \\ \cline{2-2}
                & Give me all the information about employee BFD1. \\ \cline{2-2}
                & Who are the employees of the POCOS/EP/SASD team? \\ \cline{2-2}
                & How many advisors do we have in the POCOS/SPO department? \\ \cline{2-2}
                & Who are the advisors in the departments belonging to the POCOS/EP department? \\ \cline{2-2}
                & What data sources do you have? \\ \cline{2-2}
                & What functions do you have? \\ \cline{2-2}
                & How does well inclination affect the effectiveness of cementing during through-tubing plugging? \\ \cline{2-2}
                & What can cause difficulty in locking the handling cap of the coiled tubing BOP? \\ \cline{2-2}
                & What can cause anomalous behavior of the AutoTrak with GunDrill during drilling? \\ \cline{2-2}
                & What can be done to optimize the assembly of COP/COI for parallel movement of the JRC/THRT? \\ \cline{2-2}
                & What strategies can be adopted to improve the quality of cementing in highly inclined wells during through-tubing plugging? \\ \cline{2-2}
                & What are the alternatives to accelerate the curing time of cement slurry without compromising its integrity in high-temperature conditions? \\ \cline{2-2}
                & What are the risks associated with the improper substitution of cement with silica for pure cement in surface casing cementations in high-temperature wells? \\ \cline{2-2}
                & What was the strategy adopted to allow the passage of eccentric and/or large-diameter elements through the BOP quickly and without wedging the string with these elements inside the BOP? \\ \cline{2-2}
                \hline                
                \multirow{15}{*}{Text-to-SQL} & What was the longest-lasting NPT on rig number 05? \\ \cline{2-2}
                & How many NPTs occurred on rig number 06 during August 2023? \\ \cline{2-2}
                & What were the 5 most common abnormalities across all rigs? \\ \cline{2-2}
                & What were the abnormalities that occurred on all rigs during the week of September 14th to 20th, 2023? \\ \cline{2-2}
                & Which rigs had the most lost time in 2023? Give me a table with the rigs and the sum of hours. \\ \cline{2-2}
                & Which rigs had the most lost time in the first half of 2023? \\ \cline{2-2}
                & What were the latest abnormalities that occurred on the SS-70 rig? \\ \cline{2-2}
                % & What was the longest-lasting abnormality on the SS-70 rig? \\ \cline{2-2}
                & What was the peak of abnormality occurrences on the NS-52 rig? \\ \cline{2-2}
                & What was the total lost time in hours for abnormalities whose description mentions the term "Coiled Tubing"? \\ \cline{2-2}
                & What was the total lost time in hours on the NS-38 rig in 2023? \\ \cline{2-2}
                & What was the total time lost due to equipment failure on the NS-38 rig in 2023? \\ \cline{2-2}
                % & How many abnormalities occurred on the NS-31 rig during August 2023? \\ \cline{2-2}
                & How many abnormalities occurred on the NS-31 rig during July 2023? \\ \cline{2-2}
                & How many hours of lost time were caused by human error on the NS-47 rig in 2023? \\ \cline{2-2}
                & How many hours of lost time occurred on the MS-20 rig during June 2024? \\ \cline{2-2}
                & How many hours of lost time occurred on the NS-35 rig in 2024? \\
                \hline
                \end{tabular}
                \fussy
                \caption{Queries used in first cycle. }
                \label{table:question_examples}
            \end{table}

        \subsection{Evaluation Metrics} \label{sec:evaluation_metrics}

            To ensure a comprehensive assessment, the expert panel evaluated the artifacts' responses using the following metrics, which are based on the definitions presented in Section~\ref{sec:evaluation-review}:

            \begin{itemize}

                \item \textbf{Truthfulness}: A 1-5 Likert scale score measuring the factual accuracy of the response and the extent of any divergence from the ground truth. A higher score indicates a more factually correct answer with no hallucinations.

                \item \textbf{Performance}: A 1-5 Likert scale score assessing the overall quality of the response, including its linguistic coherence, logical structure, relevance, and conciseness.

                \item \textbf{LLM Cost}: A quantitative metric representing the financial cost in US dollars (USD) to generate a response for a given query using the OpenAI API. This reflects the computational expense and efficiency of each configuration. While other costs exist (development, infrastructure, maintenance), the API cost is a primary operational expenditure that scales directly with usage and is therefore a key metric for evaluating the economic viability of the artifacts, as established in our DSR framework.
            
            \end{itemize}

            To illustrate the application of the first two metrics, an example of an expert evaluation is provided in Table~\ref{tab:tabela_inputs_example}. The table shows the responses of different models to the query: "How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures?". It details the scores for Truthfulness and Performance, along with the specialists' comments that justify the ratings.



        \begin{landscape}
            \begin{table}[h]
            \small
            \centering
            \caption{Query example with inputs, outputs, and human expert evaluations.}
            \label{tab:tabela_inputs_example}
            \begin{tabular}{| m{3cm} | m{1.5cm} | m{1.5cm} |>{\scriptsize}m{9cm} | m{1.3cm} | m{1.2cm} |>{\scriptsize}m{4cm} |}                    
            \toprule
            % \textbf{Input query} & \textbf{Model} & \textbf{Setup} & \textbf{Final output} & \textbf{Truthfulness} & \textbf{Performance} & \textbf{Specialist \ Comments} \\
            Input query & Model & Setup & Final output & Truth. \newline fulness & Perfor- \newline mance & Specialist \ Comments \\
            \midrule
            How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures? & GPT-3.5-turbo & Single-Agent & The presence of silica in the composition of cement paste can affect its thermal stability at high temperatures due to some characteristics of silica. Silica is an inorganic material that has a stable crystalline structure at high temperatures. When exposed to high temperatures, the silica present in the cement paste can undergo structural transformations, resulting in changes in the properties of the paste.

            One of the main changes that can occur is the thermal expansion of the cement paste due to the formation of unstable silica compounds. This thermal expansion can lead to cracks and fissures in the structure, compromising the material's integrity and stability.

            Moreover, the presence of silica in the composition of cement paste can affect the material's thermal resistance. Silica has low thermal conductivity, which can lead to a reduced ability of the cement to withstand high temperatures. This can result in a decrease in the material's strength and, consequently, a reduction in its thermal stability.

            In summary, the presence of silica (...) & 4 & 5 & Created a section that is irrelevant to the question: "Moreover, the presence of silica in the composition of cement paste can affect the material's thermal resistance. (...)" \\
            \cline{3-7}
            &  & Multi-Agent & The presence of silica in the composition of cement paste has a positive effect on thermal stability at high temperatures. Silica is added to cement to mitigate the retrogression of cement strength when exposed to temperatures above 230Â°F (110Â°C). Deep wells or those with a high geothermal gradient may require the use of silica cement to ensure the thermal stability of the paste. The use of silica cement is particularly important in surface casing cementing, where the temperature can exceed the critical limit. \ (...) & 2 & 2 & Retrieved an unrelated Lesson Learned on the subject (about plugs). \\ 
            \cline{2-7}
            & GPT-4 & Single-Agent & The presence of silica in the composition of cement paste increases thermal stability, especially at high temperatures. Silica is used to transform the cement into a refractory material, capable of withstanding extreme conditions. 
            
            The cement (...).

            However, (...). & 3 & 4 & Although it is true, the item on the side is not relevant to the question: "However, it is important to note that (...)." \\
            \cline{3-7}
            &  & Multi-Agent & Silica is added to cement at a ratio of 35\% by weight of cement to mitigate the retrogression (loss of compressive strength) of the cement in the well when subjected to temperatures above 230ÂºF (110ÂºC). \ (...) & 5 & 4 & Cited an unrelated (but true) Lesson Learned about plugs, Lesson Learned LA7005. \\
            \hline
            \end{tabular}
            \end{table}
        \end{landscape}
            
        \subsection{Results}
        
            This section provides an analysis of the data collected during the first experimental cycle. The aggregated results are presented in Table~\ref{tab:tabela_resultados}, followed by a discussion of each evaluation metric established in our DSR framework: Truthfulness, Performance, and LLM Cost.

            \begin{table}[h]
                \small % Reduce the font size
                \centering % Center the table on the page
                \caption{Results on Q\&A and Text-to-SQL tasks, including standard deviation (Std). The best metrics are highlighted with \textbf{\underline{bold and underline}}. The second best are highlighted with \textbf{bold}.}
                \label{tab:tabela_resultados}
                \begin{tabular}{|>{\raggedright\arraybackslash}p{2.0cm}|>{\centering\arraybackslash}p{0.85cm}|>{\centering\arraybackslash}p{0.95cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.85cm}|>{\centering\arraybackslash}p{0.95cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|}
                \hline
                \rowcolor{gray!20}
                \textbf{Task}           & \multicolumn{5}{c|}{\textbf{Single-Agent}}           & \multicolumn{5}{c|}{\textbf{Multi-Agent}} \\ % Merging cells and adding heading
                \textbf{Model}          & \textbf{LLM Cost} & \textbf{Truth.} & \textbf{Std} & \textbf{Perf.} & \textbf{Std} & \textbf{LLM Cost} & \textbf{Truth.} & \textbf{Std} & \textbf{Perf.} & \textbf{Std} \\ \hline
                \cellcolor{gray!20} Q\&A & & & & & & & & & &\\
                GPT-3.5-turbo            & 0.005             & 2.94              & 1.48 & 3.94          & 1.09 & 0.02              & 4.09              & 1.22 & 3.82 & 0.98 \\
                GPT-4                   & 0.12              & \textbf{3.88}     & 1.41 & \textbf{4.06} & 1.30 & 0.45              & \underline{\textbf{4.57}} & 0.79 & \underline{\textbf{4.43}} & 0.79 \\
                \cellcolor{gray!20} Text-to-SQL & & & & & & & & & &\\
                GPT-3.5-turbo            & 0.009             & 4.13              & 1.41 & 4.44          & 1.03 & 0.02              & \textbf{4.29}     & 1.20 & \textbf{4.29} & 1.33 \\
                GPT-4                   & 0.10 & \underline{\textbf{4.56}} & 0.96 & \underline{\textbf{4.63}} & 0.81 & 0.51      & 3.20              & 1.99 & 3.70 & 1.89 \\ \hline
                \end{tabular}
            \end{table}

            The comparative analysis between single and multi-agent setups for RAG, using GPT-3.5-turbo and GPT-4 models, revealed insights regarding the metrics of truthfulness, performance, and costs of the language model.


            \subsubsection{Truthfulness} 

                In assessing the truthfulness metric, significant differences are noted between the single and multi-agent settings in both Q\&A and Text-to-SQL tasks. The results are illustrated in Figures \ref{fig:truthfulness_QA} and \ref{fig:truthfulness_text2sql}.
                For Q\&A tasks, GPT-4 in a multi-agent configuration significantly exceeded the performance of the single-agent with a truthfulness score of 4.57 compared to 3.88. The GPT-3.5-turbo model showed distinct results between the two configurations, with the multi-agent surpassing the single-agent with scores of 4.09 and 2.94, respectively.
                In terms of Text-to-SQL queries, a different outcome was observed. GPT-4 single-agent achieved a score of 4.56, while the same model in the multi-agent configuration obtained 3.20, highlighting a limitation for the multi-agent in this task. Conversely, the GPT-3.5-turbo maintained a more balanced performance between configurations, scoring 4.29 for multi-agent and 4.13 for single-agent.
                
                \begin{figure}[h]
                    \centering
                    \begin{minipage}{.48\textwidth}
                        \centering                
                        \includegraphics[width=1\linewidth]{images/truthfulness_QA.png}
                        \caption{Truthfulness and standard deviation in Q\&A tasks by LLM model and agent configuration.}
                        \label{fig:truthfulness_QA}
                    \end{minipage}%
                    \hspace{0.2cm}
                    \begin{minipage}{.48\textwidth}
                        \centering
                        \includegraphics[width=1\linewidth]{images/truthfulness_text2sql.png}
                        \caption{Truthfulness and standard deviation in Text-to-SQL tasks by LLM model and agent configuration.}
                        \label{fig:truthfulness_text2sql}
                    \end{minipage}
                \end{figure}

                
            \subsubsection{Performance}        

                The evaluation of LLM performance \citep{Li2023} in the tasks of Q\&A and Text-to-SQL reveals trends which are similar to the truthfulness results. 
                % As shown in Figures \ref{fig:performance_QA} and \ref{fig:performance_text2sql} and summarized in \ref{tab:tabela_resultados}, the text performance in single and multi-agent setups was compared using the GPT-3.5-turbo and GPT-4 models.        
                For Q\&A tasks, the multi-agent setup shows a performance boost compared to the single-agent setup. In particular, the multi-agent GPT-4 achieves a performance score of 4.43, which is higher than the single-agent GPT-4 score of 4.06. This pattern is consistent with the GPT-3.5-turbo, where the multi-agent system also surpasses the single-agent system, scoring 3.82 and 3.94, respectively. These findings emphasize the effectiveness of the multi-agent approach in handling technical user queries.
                        
                \begin{figure}[h]
                    \centering
                    \begin{minipage}{.48\textwidth}
                        \centering                
                        \includegraphics[width=1\linewidth]{images/performance_QA.png}
                        \caption{Performance and standard deviation in Q\&A tasks by LLM model and agent configuration.}
                        \label{fig:performance_QA}
                    \end{minipage}
                    \hspace{0.2cm}
                    \begin{minipage}{.48\textwidth}
                        \centering
                        \includegraphics[width=1\linewidth]{images/performance_text2sql.png}
                        \caption{Performance and standard deviation in Text-to-SQL tasks by LLM model and agent configuration.}
                        \label{fig:performance_text2sql}
                    \end{minipage}%
                \end{figure}


            \subsubsection{LLM Cost} 
                Language model services are typically composed by a values per token. For instance, GPT-4 model costs US\$30.00 (input) and US\$60.00 (output) per 1 million tokens received and sent, respectively.        
                The single-agent architecture demonstrated substantially lower costs for both Q\&A and Text-to-SQL tasks compared to the multi-agent setup as shown in Figure~\ref{fig:truthfulness_vs_cost_vs_config_model}. For instance, the average cost of the GPT-4 model \citep{OpenAI2023} for a Q\&A task was \$0.12 per processed question for the single-agent, while the multi-agent recorded an average cost of \$0.45. This trend of higher costs for the multi-agent architecture was also maintained for Text-to-SQL tasks, with an average cost of \$0.51 for the multi-agent architecture in contrast to \$0.10 for the single agent.
                The higher token count and cost for multi-agent setting is due to the inclusion of intermediate calls, for example, when the "Agent Selector" needs to decide which agent to pass the turn to. All the message history is passed to the LLM at this stage, substantially increasing the number of tokens submitted and response time.


                \begin{figure}[h]
                    \centering              
                    % \framebox{
                        \includegraphics[width=0.75\textwidth]{images/truthfulness_vs_cost_vs_config_model.png}
                    % }
                    \caption{Average LLM costs and Truthfulness per completed task according to setup and model.}
                    \label{fig:truthfulness_vs_cost_vs_config_model}
                \end{figure}
                
                

        \subsection{Discussion} 
        
            The comparison between single and multi-agent systems revealed significant differences in terms of performance and cost:
            
            \subsubsection{General Performance.}     
                The results indicate that for Q\&A tasks in the context of O\&G, truthfulness measure was 28\% higher with the multi-agent architecture compared to single. 
                However, for Text-to-SQL tasks, this trend was inverted, where the single-agent scored 15\% higher.

                These findings suggest that for Q\&A tasks, the multi-agent setup may be more advantageous in terms of providing truthful information, particularly when utilizing the more advanced GPT-4 model. 
                Conversely, in Text-to-SQL tasks, the GPT-4 model in a single-agent configuration proved more effective. 
                This might imply that the added complexity of managing multiple agents in some tasks does not necessarily lead to improved performance in responses, underscoring the importance of carefully selecting the agent configuration based on the task type and specific features of the language model used.
                    
            \subsubsection{Cost-Performance Analysis.}
                While the multi-agent system shows higher truthfulness in Q\&A tasks, it is crucial to consider the associated costs. 
                To provide a clearer comparison, let us consider the score/cost ratios. For Q\&A tasks using GPT-4, the single-agent configuration yields a ratio of 32.33 truthfulness points per dollar, compared to 10.16 for the multi-agent setup. This indicates that while the multi-agent system shows a 17.8\% improvement in truthfulness, it comes at a 275\% increase in cost.

                This trade-off highlights an important implication for any organization considering the adoption of these technologies. The optimal architecture is not universal; it is highly dependent on specific task requirements and budget constraints. 
                This reality underscores the necessity of conducting a preliminary, cost-performance evaluation. Rather than simply selecting a model, decision-makers must first perform a targeted analysis to establish a cost-benefit threshold. 
                Our work not only provides initial data for the O\&G domain but also demonstrates a foundational methodology for this evaluation process, which ultimately motivated the more rigorous and quantitative approach of our second experimental cycle.


            \subsubsection{Model Performance Variations.}
                Interestingly, our results show that GPT-3.5-turbo outperforms GPT-4 in certain tasks, particularly in the Text-to-SQL multi-agent configuration, despite GPT-4's larger size and more extensive training. 
                This unexpected performance could be attributed to several factors. 
                First, GPT-3.5-turbo may have undergone more specific fine-tuning for structured query tasks, allowing it to excel in Text-to-SQL scenarios. 
                Additionally, GPT-3.5-turbo's training data might be more recent or more relevant to the specific domain of our study. 
                Another possibility is that the smaller model size of GPT-3.5-turbo allows for faster processing and more efficient handling of the multi-agent setup, resulting in better performance in some contexts. Additionally, given the observed standard deviations, part of this difference may be attributable to statistical noise rather than a systematic advantage.

                However, it is important to note that GPT-4, when used in a multi-agent setup, demonstrated more consistent truthfulness and performance, as evidenced by its reduced standard deviation in results. 
                This consistency can be particularly advantageous in applications where reliability and accuracy are critical. 
                Multi-agent systems have the advantage of maintaining separate contexts for different aspects of a task \citep{Langchain2025blogmultiagent}. 
                This compartmentalization can lead to better handling of complex, multi-faceted queries, as each agent can focus on its specific context without being overwhelmed by irrelevant information. However, this advantage may be offset in tasks like Text-to-SQL, where maintaining a unified context of the database schema and query structure is crucial, possibly explaining the better performance of single-agent setups in this task.
                Furthermore, the multi-agent architecture inherently involves multiple stages of information processing, which can serve as natural filtering mechanisms.
                As information passes from one agent to another, irrelevant or low-quality data may be naturally filtered out, leading to more refined and accurate final outputs. 
                This could explain the superior performance in filtering irrelevant information observed in multi-agent setups.
            
            
            \subsubsection{Economic Efficiency.} 
            
                The multi-agent architecture incurs significantly higher costs compared to the single-agent system, primarily due to additional intermediate calls to the language model and multiple iterations between agents for action planning. 
                Also, the cost differences between using GPT-4 and GPT-3.5-turbo are substantial, with GPT-4 being 20 times more expensive (in early 2024).
                \xexeo{Dizer x vezes mais caro em julho de 2025}.
                \vitor{Feito}

                The average cost per query for each configuration is presented in Table \ref{tab:cost_per_query}. These figures highlight the direct cost implications of the chosen architecture and model.
                
                \begin{table}[h!]
                \centering
                \caption{Average LLM Cost Per Query (USD). Values from early 2024.}
                \label{tab:cost_per_query}
                \begin{tabular}{l r}
                \toprule
                \textbf{Configuration} & \textbf{Cost per Query} \\
                \midrule
                Single-Agent (GPT-3.5-Turbo) & \$0.0068 \\
                Single-Agent (GPT-4) & \$0.1095 \\
                Multi-Agent (GPT-3.5-Turbo) & \$0.0197 \\
                Multi-Agent (GPT-4) & \$0.4896 \\
                \bottomrule
                \end{tabular}
                \end{table}

                To illustrate the financial implications of adopting different models and architectures, we estimate the annual costs for a large company with 40,000 knowledge workers. Our calculations are based on an average of 5 queries per worker per day, over 250 working days per year.
                
                Under these assumptions, the total annual query volume is 50 million (40,000 workers $\times$ 5 queries/day $\times$ 250 days). For a single-agent configuration, this results in an annual cost of approximately \$337,843 for GPT-3.5 and \$5.47 million for GPT-4.
                
                In a multi-agent architecture, the costs increase substantially, escalating to approximately \$986,631 for GPT-3.5 and \$24.48 million for GPT-4. These estimates underscore the significant financial trade-offs when adopting a multi-agent system, which, while potentially offering performance benefits, comes with a considerable increase in LLM operational costs.

                While multi-agent systems and more advanced models like GPT-4 offer improvements in performance, the economic efficiency, as measured by truthfulness per dollar, may favor single-agent systems and less costly models like GPT-3.5-turbo, depending on the specific application and budget constraints.

                It is important to note that, as of July 2025, the landscape of LLMs has evolved substantially. The emergence of more efficient models, has led to a significant decrease in API's costs. This suggests that the financial trade-offs discussed previously may no longer be as pronounced, and that high-performance multi-agent systems could become economically viable much sooner than anticipated.

                \xexeo{In summary Ã© o parÃ¡grafo tÃ­pico das LLMs... Mas Ã© isso mesmo. PorÃ©m tem que colocar um ponto: o custo dos modelos estÃ¡ caindo barbaramente com o aparecimento de novos modelos no topo de desempenho e novas tecnologias tem permitido alcanÃ§ar resultados de Ã³tima qualidade com mÃ¡quinas muito menores, o que tambÃ©m derruba o custo. Pode atÃ© citar o exemplo do DeepSeek (buscando na literatura o desempenho x custo dele)}
                \vitor{Feito}
                
            
            \subsubsection{Challenges and Limitations}     
                During the evaluation of the agents, several challenges and limitations were identified.

                \textbf{\textit{Contextualization and Interpretation.}} 
                    In many cases, the single-agent solution had difficulty understanding the context of the question. For example, a question about cementing was interpreted in the context of the construction industry, a theme to which the language models were more exposed during the training phase. 
                    However, the multi-agent structure, with its well-defined roles, better understood the questions and showed superior performance in Q\&A tasks, corroborating the findings of \citep{Li2024}.
                
                \textbf{\textit{Filtering Irrelevant Information.}} 
                    The agent often receives irrelevant documents along with important ones in the prompt context, and it is up to the LLM to ignore these. 
                    For example, when asked about alternatives to accelerate the curing time of cement paste without compromising its integrity at high temperatures, the RAG system retrieved a document that included information about batch cementing to ensure homogeneity during manufacturing and pumping. 
                    While this information is true, it was not relevant to the specific question asked. 
                    In this aspect, the multi-agent solution performed better at discarding such irrelevant information, focusing more accurately on the task at hand. 
                    Other possible solutions include improving the accuracy of semantic search by adjusting a minimum threshold for similarity measures or through re-ranking techniques such as those proposed by \citep{Carraro2024} and \citep{Sun2023}.
                
                \textbf{\textit{Hallucination.}} 
                    During the evaluation of our system, we encountered instances where the agent produced hallucinated information instead of utilizing the appropriate tool to retrieve accurate data, as in \citep{Bilbao2023}. 
                    For example, when asked, "How many anomalies occurred on rig number 05 during August 2023?" the agent was expected to use the Text-to-SQL tool to query the database. 
                    However, it bypassed this tool and generated a fabricated response, stating that 5 anomalies occurred, along with detailed descriptions of fictional events. The correct answer, as retrieved from the database, was that 7 anomalies occurred. This hallucination likely resulted from the agent's reliance on its internal knowledge rather than external data retrieval. 

                    In terms of hallucination statistics, our analysis revealed that for Q\&A tasks, hallucinations occurred in 9.6\% of cases and 3.8\% for partially hallucinated. 
                    In contrast, Text-to-SQL tasks exhibited a lower hallucination rate, with only 3.6\% of responses containing hallucinated information and 96.4\% being accurate. 
                    These findings highlight the variation of susceptibility to hallucination in different types of tasks, highlighting the need for targeted strategies to mitigate this problem.
                
                \textbf{\textit{Industry Jargon:}}
                    Specifically analyzing the activity of drilling and completion of offshore wells, the main challenge is the inherently complex and technical nature of the data involved. 
                    There were instances of incorrect interpretation of information, likely due to the use of terms, expressions, and themes specific to well construction, to which the language model had little or no exposure during training phase. 
                    A possible solution is the implementation of specialized models, which has been pointed out in gray literature as a trend for the coming years \citep{Shah2024, Meena2023, Ghosh2023}.
                
                \textbf{\textit{Tools vs. Performance:}} 
                    It was identified during the experiments that agents with a high amount of tools showed a decline in overall performance. 
                    This can be attributed to the added context to the prompts. 
                    As the context length increases, the model's ability to accurately interpret and respond diminishes.
                    This is a limitation of current language models, where longer contexts can lead to a dilution of relevant information and increased difficulty in maintaining coherence and accuracy. 
                    This conclusion is currently qualitative, as these metrics were not addressed in this experiment.

                
                \textbf{\textit{Queries Involving Proper Names:}}
                    In queries involving people's names, it was not possible to retrieve relevant documents using semantic search. 
                    For example, when asked to identify the employee associated with a specific key and list knowledge items they registered in the system, the LLM incorrectly attributed such documents to the wrong author. 
                    This highlights the difficulty in accurately retrieving information based on proper names, which can be complicated by variations in accentuation, abbreviation, and formatting.
                    \xexeo{tem evidÃªncias disso em outros artigos?}
                    \vitor{nÃ£o encontrei}
                    A potential solution to be explored is the use of Self-Query Retriever \citep{LangchainSelfQuery2023}, implementing a hybrid search with metadata filters (including proper names) and semantic retrieval of the rest of the query. 
                    It is also suggested, in these cases, to use the \citep{Levenshtein1966} distance to handle possible variations in the spelling of names. 
                    This approach could improve the accuracy of retrieving documents related to specific individuals, ensuring that the correct information is associated with the right person.
                    
            
            % \subsubsection{Practical Implications}
            \subsubsection{Practical Implications.} 

                The findings from our study have significant practical implications for the O\&G sector, and potentially for other industries characterized by complex and technical data environments:
                    
                \begin{itemize}
                
                    \item \textbf{Enhanced Decision-Making Support:}
                        Our results indicate that multi-agent systems provide a 28\% higher truthfulness measure in Q\&A tasks. This can be particularly beneficial for decision-making in well engineering, where accurate and truthful information is critical.
                        Implementing multi-agent systems in decision-making processes can lead to more reliable and informed decisions, thereby reducing the risk of errors and enhancing operational safety and efficiency.
                    
                    \item \textbf{Balancing Performance and Economic Efficiency:}
                        While multi-agent systems offer superior performance in terms of truthfulness, they come with a cost that is 3.7 times higher on average compared to single-agent systems.
                        This highlights the importance of a strategic approach in selecting agent configurations based on specific tasks and budget constraints. 
                        % For instance, single-agent systems might be more cost-effective for Text-to-SQL tasks where they have shown to perform 15\% better. 
                        A detailed cost-benefit analysis reveals that for Q\&A tasks using GPT-4, the single-agent configuration yields a ratio of 32.33 truthfulness points per dollar, compared to 10.16 for the multi-agent setup. While the multi-agent system shows a 17.8\% improvement in truthfulness, this comes at a 275\% increase in cost. The efficiency varies significantly by task type; in Text-to-SQL tasks, the GPT-4 single-agent outperforms the multi-agent by 42.5\% in truthfulness while costing 80.4\% less. 
                        % These quantitative insights emphasize the need for careful consideration of task requirements and budget constraints when choosing between single and multi-agent configurations.
                        
                    \item \textbf{Reflection and Critic Agents:}
                        A promising approach to enhance the performance of these agents is the use of reflection \citep{Shinn2023}, a method where agents verbally reflect on task feedback signals and maintain this reflective text in an episodic memory buffer to improve decision-making in subsequent trials. Critic agents are a way to implement reflection in a multi-agent setup. This type of agent is challenging to apply in Q\&A tasks over private technical data, as commercial LLMs (OpenAI, Google Bard, and others) have not been deeply trained in the domain and struggle to provide relevant and precise critiques, reinforcing the trend toward increased use of domain-specific models \citep{Shah2024, Meena2023, Ghosh2023}.                
                        
                    \item \textbf{Task-Specific Agent Configuration:}
                        The study highlights that the complexity of managing multiple agents does not always lead to better performance. In some cases, a single-agent setup might be more effective.
                        This insight can guide the development and deployment of AI systems, ensuring that the configuration of agents is tailored to the specific requirements of the task, thereby optimizing both performance and cost.           
                        
                    \item \textbf{Potential for Broader Application:}
                        The insights gained from this study are not limited to the O\&G sector but can be applied to other industries with similar technical complexities, such as aerospace, pharmaceuticals, and renewable energy.
                        By adopting multi-agent systems in these industries, organizations can improve decision-making, knowledge management, and operational efficiency, driving innovation and competitiveness.             
                    
                \end{itemize}
                        
                    
            \subsubsection{Future Directions.} 

                This work indicates possible pathways for enhancing RAG architectures in O\&G sector. 
                
                \begin{itemize}
                
                    \item \textbf{Enhancement of IR Semantic Techniques:}
                        There is a critical need to develop more sophisticated semantic search technologies. Future efforts should focus on enhancing the precision of information retrieval by filtering out irrelevant content more effectively. This will ensure that agents can provide more accurate and contextually appropriate responses, crucial for technical domains such as O\&G.
                        
                    \item \textbf{Development of Domain-Specific Models:}
                        Specialized models tailored specifically to the O\&G and other domains, such as biomedical engineering \citep{Pal2024}, could significantly improve the handling of specific jargon and complex technical data, while reducing LLM costs \citep{Arefeen2024}. Future research should aim to develop and train these models to better understand and interpret the unique language and data types found in O\&G, enhancing the overall accuracy of agent responses.
                        
                    \item \textbf{Optimization of Tool Use in Agent Performance:}
                        The relationship between the quantity of tools available to an agent and its performance needs further exploration. Future studies should quantify the impact of tool availability on agent efficacy and efficiency, aiming to optimize tool use without overwhelming the agent or diluting performance quality.
                        
                    \item \textbf{Integration of Advanced Name Recognition Techniques:}
                        Queries involving proper names pose a significant challenge in semantic search. Integrating advanced retrieval techniques, such as Self-Query Retrievers \citep{LangchainSelfQuery2023} and \citep{Levenshtein1966} distance algorithms, could improve the handling of these queries. Future research should focus on enhancing name recognition capabilities to ensure that agents can accurately retrieve and utilize correct information, especially in scenarios where precision is paramount.
                        
                    \item \textbf{Extension to Other Complex Domains:}
                        The potential applications of multi-agent systems are not limited to the O\&G sector. Future research should explore the adaptation and implementation of these systems in other complex and technical domains, such as aerospace, pharmaceuticals, and renewable energy. Investigating how these systems can support decision-making in these areas will provide valuable insights into their versatility and adaptability.
                        
                    \item \textbf{Hybrid Model Experimentation:}
                        Combining the strengths of single and multi-agent systems could yield significant benefits. Future directions should include experimenting with hybrid models that integrate the robustness and depth of multi-agent interactions with the simplicity and efficiency of single-agent systems. This hybrid approach could potentially offer a balanced solution, maximizing performance while managing costs and complexity.
                        

                \end{itemize}
                
                By pursuing these directions, future research can significantly advance the development of multi-agent systems, not only enhancing their application in the O\&G sector but also expanding their utility across various technologically intensive activities.
                    


\chapter{Second Experimental Evaluation Cycle}
\label{chap:second_experiment}

This chapter describes the second experimental cycle of this research, building upon the findings of the first cycle detailed in Chapter 3. The rapid evolution of generative AI frameworks and models, along with the insights gained previously, prompted a more advanced and rigorous evaluation. This second phase employs non-agentic workflows as a baseline, introduces a more quantitative evaluation methodology, and leverages an automated assessment process based on the LLM-as-a-Judge concept \citep{Gu2025}. It's use was driven by the sheer volume of responses requiring evaluation. With four configurations, two models, and three executions for each of the 33 questions, a total of 792 responses were generated. Manually assessing this volume of data would have been impractical. Furthermore, previously used metrics like `truthfulness` had become less critical. This metric was highly relevant when models frequently hallucinated, a problem that is far less prevalent in the current generation of LLMs, shifting the focus to precision and recall of factual information.


\section{Design Science Research Framework}

    This second experimental cycle adheres to the Design Science Research (DSR) methodology, focusing on refining the artifacts and evaluation based on the outcomes of the first cycle.

    \begin{description}
        \item[Context] The operational environment of well construction engineering, where practitioners require efficient and reliable access to vast amounts of technical and ESG-related information.

        \item[Problem] The first experimental cycle revealed several limitations, including the subjective nature and scalability issues of expert-based evaluation, the need to compare agentic systems against simpler non-agentic baselines, and the challenge of ensuring consistent performance. This second cycle addresses the problem of developing a more robust, scalable, and objective method for evaluating and comparing different LLM-based architectures for domain-specific Q\&A.

        \item[Proposed Artifacts] Four distinct architectures were designed and implemented to compare different strategies for information retrieval and reasoning:
        \begin{itemize}
            \item A non-agentic \textbf{Linear-Flow} RAG pipeline.
            \item A non-agentic \textbf{Linear-Flow with a Router} to direct queries.
            \item A \textbf{Single-Agent} architecture, refined from the first experiment.
            \item A \textbf{Multi-Agent Supervisor} architecture for distributed reasoning.
        \end{itemize}

        \item[Evaluation] The artifacts are evaluated using an automated pipeline. An LLM-as-a-Judge assesses the generated answers against a ground-truth dataset. The evaluation is based on quantitative information retrieval metrics: \textbf{Precision}, \textbf{Recall}, and \textbf{F1-Score}.
    \end{description}

\section{Context and Problem Statement}

    \subsection{Context}

    As established in the previous chapters, this research is situated within the oil and gas industry, specifically in the domain of well construction and maintenance. Engineers and specialists in this field must navigate a complex information landscape, drawing from operational reports, ESG alerts, and documented best practices (Learned Lessons) to make critical decisions. The effectiveness of these decisions hinges on the speed and accuracy with which relevant information can be retrieved and synthesized.

    \subsection{Problem}

    The first experimental cycle confirmed the potential of LLM-based agents but also highlighted key challenges. The manual, expert-led evaluation process was time-consuming and difficult to scale. Furthermore, the performance differences between single and multi-agent systems suggested that a more granular analysis was needed, including a comparison with non-agentic RAG workflows to establish a performance baseline. Therefore, the central problem for this second cycle is to design and execute a more rigorous, automated, and scalable evaluation to definitively compare the efficacy of various agentic and non-agentic architectures in this specialized domain.


\section{Proposed Artifacts}

    To address the research problem, four distinct artifacts were developed, representing a spectrum of complexity from simple sequential pipelines to collaborative multi-agent systems. 
    
    \subsection{System Architecture Overview}

    The experimental system was implemented using \citet{Langchain2025} and \citet{Langgraph2025}, frameworks specialized in language model orchestration. This modular design allows for the systematic and reproducible evaluation of different components and workflows. Key layers of the architecture include:

    \begin{itemize}
        \item \textbf{Experiment Orchestration:} Manages the execution loop, iterating through all combinations of questions, models, and setups.
        \item \textbf{Agent Workflow Frameworks:} Defines the logic for each of the four proposed artifacts using LangGraph to create cyclical graphs for agentic behavior.
        \item \textbf{Tool Integration:} A standardized interface providing agents with access to external knowledge sources. This layer enables consistent semantic search over domain-specific vector stores, ensuring that performance differences are attributable to architectural choices rather than variations in data access.
        \item \textbf{Prompt Engineering:} A library of system messages and prompt templates designed to guide the LLM's reasoning process for each specific task within the workflows.
        \item \textbf{State Management and Logging:} Captures the complete execution trace of each run, including intermediate steps, tool calls, and final outputs. This observability is essential for understanding not just the final output, but the process by which each architecture arrived at its answer.
    \end{itemize}


    \subsection{Artifact 1: Linear-Flow}

        The \textbf{Linear-Flow} architecture represents the simplest non-agentic RAG design, serving as a performance baseline. As shown in Figure \ref{fig:diagrama_linear_flow}, user input is processed in a strictly sequential manner. The user's query is handled by a single LLM step, which contains all the instructions needed to generate search queries for every available tool.
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{images_exp2/diagrams/diagrama_linear_flow.png}
            \caption{Linear-Flow architecture. PTn indicates the prompt for Tool n.}
            \label{fig:diagrama_linear_flow}
        \end{figure}

        Because the instruction prompts for all tools are aggregated into a single call, the resulting context for the LLM becomes notably extensive and complex. While this approach is straightforward to implement, its primary drawback is the potential for performance degradation as the context length increases, which can dilute the model's focus and lead to less precise retrieval queries.
        

    \subsection{Artifact 2: Linear-Flow with Router}
    
        The \textbf{Linear-Flow with Router} paradigm (Figure \ref{fig:diagrama_linear_w_router}) extends the basic pipeline by introducing a routing mechanism to create a descentralized, non-agentic workflow. This architecture first directs a user's question to a \textbf{router node}, which is a preliminary LLM call tasked with analyzing the query and determining the most appropriate tool or sequence of tools to use.

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{images_exp2/diagrams/diagrama_linear_w_router.png}
            \caption{Linear-Flow with Router architecture.}
            \label{fig:diagrama_linear_w_router}
        \end{figure}

        This design enables the distribution of complex instruction prompts into smaller, more specialized nodes. Instead of one large prompt, several targeted sub-queries are generated, each dispatched to its respective tool. This approach offers two main advantages:

        \begin{itemize}
            \item \textbf{Specialization:} Each tool receives a query tailored to its specific function, leading to more accurate and relevant retrieval results.
            \item \textbf{Reduced Context:} By breaking down the master prompt, each LLM call operates on a smaller, more focused context, mitigating performance issues associated with long context windows.
        \end{itemize}


    \subsection{Artifact 3: Single-Agent}

        The \textbf{Single-Agent} architecture (Figure \ref{fig:diagrama_single_agent}) embodies a centralized agentic approach, building on the lessons from the first experimental cycle. In this setup, a single LLM agent manages the entire question-answering process. It has access to the full suite of tools and autonomously makes decisions about which to invoke, in what order, and how to synthesize the retrieved information into a final answer.
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{images_exp2/diagrams/diagrama_single_agent.png}
            \caption{Single-Agent architecture.}
            \label{fig:diagrama_single_agent}
        \end{figure}    

        The design emphasizes \textbf{end-to-end reasoning within a unified context}, allowing the model to maintain the same ``thought process'' from start to finish. This artifact tests the capability of a standalone LLM agent to manage a RAG workflow, balancing the tool calling for different knowledge sources, all without the communication overhead required by multi-agent systems.
        

    \subsection{Artifact 4: Multi-Agent Supervisor}
    
        The \textbf{Multi-Agent Supervisor} setup (Figure \ref{fig:diagrama_multiagente_supervisor}) implements a collaborative, hierarchical system to explore the benefits of distributed cognition. This architecture consists of two main components:        

        \begin{enumerate}
            \item \textbf{A Supervisor Agent:} This master agent receives the user's query, analyzes it, and orchestrates the workflow by delegating these tasks to the appropriate specialist agents.
            \item \textbf{Specialist Agents:} A team of agents, each focusing on a specific domain of knowledge or reasoning skill. For this experiment, each specialist was tied to a single tool (e.g., a Learned Lessons Agent, an HSE Alert Agent).
        \end{enumerate}
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{images_exp2/diagrams/diagrama_multiagente_supervisor.png}
            \caption{Multi-Agent Supervisor architecture with four specialist agents.}
            \label{fig:diagrama_multiagente_supervisor}
        \end{figure}

        The supervisor orchestrates the collaboration, integrates the findings from each specialist, and synthesizes the potentially divergent information into a single, coherent final answer. This framework is designed to mimic real-world expert collaboration and tests whether decomposing a problem and assigning its parts to dedicated specialists yields a more accurate result.
        
        
\section{Evaluation}

    The evaluation phase was designed to be automated, scalable, and objective, addressing the limitations of the first experimental cycle.

    \subsection{Evaluation Methodology}

        The core of the evaluation is an automated execution loop (detailed in Algorithm \ref{alg:execution_loop}) that runs each of the 33 questions through every combination of artifact (4 setups) and model (2 models), repeating each run three times to account for stochasticity.

        \begin{algorithm}[h]
        \caption{Experiment Execution Loop}
        \begin{algorithmic}[1]
        \Require questions, setups, models
        \Ensure results
        \Function{RunExperiment}{}
            \State $results \gets \{\}$
            \ForAll{$question \in questions$}
                \State $ground\_truth \gets question.ground\_truth$
                \ForAll{$setup \in setups$}
                    \ForAll{$model \in models$}
                        \For{$i \in 1 \dots 3$} \Comment{Execute 3 times for consistency}
                            \State $agent \gets \text{InitializeAgent}(setup, model)$
                            \State $response \gets agent.\text{ProcessQuestion}(question)$
                            \State $metrics \gets \text{EvaluateResponse}(response, ground\_truth)$
                            \State Store $metrics$ and $response$ in $results$
                        \EndFor
                    \EndFor
                \EndFor
            \EndFor
            \State \Return $\text{AggregateResults}(results)$
        \EndFunction
        \end{algorithmic}
        \label{alg:execution_loop}
        \end{algorithm}

        The quality of each generated response is assessed using the LLM-as-a-Judge approach. A powerful LLM (GPT-4) is prompted to act as an impartial evaluator, comparing the generated answer against the ground-truth answer. The judge decomposes both texts into atomic statements and classifies them to build a confusion matrix, from which the final metrics are calculated. The full prompt for the LLM-as-a-Judge can be found in Appendix \ref{code:llm-judge}.

    \subsection{Data Set Creation}

        The experiment utilizes a curated dataset developed in collaboration with domain experts.
        \begin{itemize}
            \item \textbf{Questions Dataset:} A set of 17 questions (10 questions from the first experimental cycle and 7 new questions) reflecting real-world information needs of well engineers. Each question is paired with a manually created, expert-validated ground-truth answer.
            \item \textbf{Knowledge Bases:} The artifacts were given access to three distinct, pre-processed knowledge sources from within the organization, vectorized for semantic search:
            \begin{itemize}
                \item \textbf{Learned Lessons:} A repository of learned lessons, best practices, and operational alerts.
                \item \textbf{HSE Alerts:} A collection of ESG alerts and incident reports.
                \item \textbf{Operational Reports:} A database of detailed daily operational reports from drilling rigs.
            \end{itemize}
        \end{itemize}

    \subsection{Evaluation Metrics}

        To provide a quantitative and objective assessment, the following information retrieval metrics, detailed in Section~\ref{sec:precision_recall_f1_review}, were calculated for each response based on the LLM-as-a-Judge's analysis:
        \begin{itemize}
            \item \textbf{Precision:} Measures the accuracy of the information presented in the generated answer. It is the ratio of correct statements (True Positives) to the total number of statements made. 
            \item \textbf{Recall:} Measures the completeness of the answer. It is the ratio of correct statements retrieved to the total number of statements available in the ground truth.
            \item \textbf{F1-Score:} The harmonic mean of Precision and Recall, providing a single, balanced measure of overall performance.
        \end{itemize}

    \subsection{Results}


        To ensure a robust evaluation and account for the inherent non-determinism of language models, each of the 17 questions in the dataset was processed three times for every model and configuration combination. This experimental design resulted in a total of 408 executions (17 questions $\times$ 2 models $\times$ 4 configurations $\times$ 3 runs). Each of the 408 generated answers was then compared to a ground truth answer to calculate performance metrics.

        The results presented in this section are derived from this set of runs. For each of the 136 unique combinations of question, model, and configuration, the best-performing run (out of three) was selected based on the F1-Score. The final metrics reported in Table~\ref{tab:performance_metrics} represent the average of these best-run scores across all 17 questions for each of the eight model-configuration pairs. This approach presents a clear view of the potential of each setup, with the F1-Score serving as the primary metric for performance evaluation.

        \begin{landscape}
            \begin{table}[H]
            \centering
            \caption{Detailed performance metrics by model and agent configuration. The best result for each metric is highlighted in bold and underlined. For the inferior model, the best result is only underlined.}
            \label{tab:performance_metrics}
            % \resizebox{\textwidth}{!}{%
            \begin{tabular}{@{}llcccccccccccc@{}}
                \toprule
                \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Configuration}} & \multicolumn{4}{c}{\textbf{F1-Score}} & \multicolumn{4}{c}{\textbf{Precision}} & \multicolumn{4}{c}{\textbf{Recall}} \\
                \cmidrule(l){3-6} \cmidrule(l){7-10} \cmidrule(l){11-14}
                & & Mean & Std. Dev. & Min & Max & Mean & Std. Dev. & Min & Max & Mean & Std. Dev. & Min & Max \\
                \midrule
                \multirow{4}{*}{GPT-4o} & Linear-Flow (Baseline) & 0.581 & 0.204 & 0.000 & 1.000 & 0.656 & 0.262 & 0.000 & 1.000 & 0.548 & 0.201 & 0.000 & 1.000 \\
                & Linear-Flow w/ Router & \textbf{\underline{0.702}} & 0.202 & 0.333 & 1.000 & \textbf{\underline{0.805}} & 0.185 & 0.400 & 1.000 & \textbf{\underline{0.674}} & 0.242 & 0.286 & 1.000 \\
                & Single-Agent & 0.643 & 0.213 & 0.364 & 1.000 & 0.751 & 0.198 & 0.400 & 1.000 & 0.618 & 0.240 & 0.294 & 1.000 \\
                & Multi-Agent & 0.664 & 0.214 & 0.286 & 1.000 & 0.746 & 0.221 & 0.286 & 1.000 & 0.630 & 0.231 & 0.286 & 1.000 \\
                \midrule
                \multirow{4}{*}{GPT-4o-mini} & Linear-Flow (Baseline) & 0.534 & 0.208 & 0.000 & 0.923 & 0.604 & 0.262 & 0.000 & 1.000 & 0.516 & 0.216 & 0.000 & 0.923 \\
                & Linear-Flow w/ Router & \underline{0.604} & 0.155 & 0.333 & 1.000 & 0.676 & 0.196 & 0.300 & 1.000 & \underline{0.602} & 0.206 & 0.267 & 1.000 \\
                & Single-Agent & 0.576 & 0.184 & 0.308 & 1.000 & 0.719 & 0.214 & 0.286 & 1.000 & 0.544 & 0.227 & 0.231 & 1.000 \\
                & Multi-Agent & 0.596 & 0.182 & 0.348 & 1.000 & \underline{0.687} & 0.198 & 0.400 & 1.000 & 0.578 & 0.201 & 0.235 & 1.000 \\
                \bottomrule
            \end{tabular}%
            % }
            \end{table}
        \end{landscape}


        \begin{figure}[h]
            \centering
            \includegraphics[width=0.7\textwidth]{images_exp2/bar_best_f1_by_model_and_configuration.png}
            \caption{Best F1-Score by model and configuration.}
            \label{fig:best_f1_by_model_and_configuration}
        \end{figure}

        
        The analysis of the results presented in Table~\ref{tab:performance_metrics} and visualized in Figure~\ref{fig:best_f1_by_model_and_configuration} reveals several insights into the performance of the different models and configurations. A primary observation is the consistent performance superiority of the GPT-4o model over its counterpart, GPT-4o-mini, across all tested configurations. The most capable configuration for GPT-4o, \textit{Linear-Flow w/ Router}, achieved a mean F1-Score of 0.702. This represents a significant performance uplift of approximately 16.2\% compared to the best score achieved by GPT-4o-mini (0.604), which was also with the \textit{Linear-Flow w/ Router} configuration. This gap underscores the impact that the model's reasoning and instruction-following capabilities have on the overall performance of the system.

        Furthermore, the results show that more complex configurations brought a notable improvement over the \textit{Linear-Flow (Baseline)} for both models. The \textit{Linear-Flow w/ Router} configuration emerged as the most effective architecture overall. For the superior GPT-4o model, this configuration boosted the mean F1-Score by a relative 20.8\% over the baseline (from 0.581 to 0.702). It also increased the mean precision by 22.7\% (from 0.656 to 0.805), indicating that the router is highly effective at selecting the correct reasoning path or tool, thereby reducing incorrect or irrelevant responses.

        While the \textit{Single-Agent} and \textit{Multi-Agent} configurations also outperformed the baseline, they did not reach the performance level of the router-enhanced linear flow. For GPT-4o, the \textit{Multi-Agent} setup (F1-Score 0.664) slightly outperformed the \textit{Single-Agent} (F1-Score 0.643), but both fell short of the \textit{Linear-Flow w/ Router}. This suggests that for the tasks in this experiment, the added complexity of reflective agent loops or multi-agent collaboration did not yield a proportional benefit over a more direct, intelligent tool-routing approach.

        A crucial aspect of the results is the high standard deviation observed across all configurations, typically around 0.20 for the F1-Score. The wide range between minimum and maximum scores indicates that performance is highly variable and question-dependent. This suggests that even the best-performing systems can fail completely on certain queries.



    \subsection{Discussion} \label{sec:exp2-discussion}

        % [DISCUSSION WILL ENTER HERE]
        The results from this second experimental cycle present a series of compelling, and in some aspects, counter-intuitive insights into the application of LLM-based architectures in specialized technical domains. The most significant finding, which stands in contrast to the prevailing trends in agentic AI and some initial findings from our first experiment, is the superior performance of a non-agentic configuration (\textit{Linear-Flow w/ Router}) over its more complex, cyclical agentic counterparts. This outcome challenges the assumption that increased agent complexity, with its capacity for reflection and iterative refinement, universally leads to better performance. This discussion will explore the primary hypothesis for this phenomenon, explore other contributing factors, and consider the broader implications for designing AI systems in niche domains.

        \subsubsection{The Domain Knowledge Deficit: Why Agentic Reflection Fails}

            The core benefit of an agentic architecture, whether single or multi-agent, lies in its ability to perform cyclical reasoning. An agent can call a tool, assess the output, reflect on its progress, and decide on a new course of action, potentially correcting earlier mistakes or refining its strategy. This iterative process is a form of simulated cognition. However, we hypothesize that the effectiveness of this reflective capability is fundamentally contingent on the LLM's pre-existing, foundational knowledge of the subject matter.
            
            For an LLM to effectively judge the output of a tool or the partial answer from a sub-agent, it must have a robust internal model of what constitutes a \textit{good} or \textit{correct} answer in that domain. Consider, for example, the domain of software engineering. LLMs like GPT-4o are extensively trained on vast repositories of code, documentation, and programming discussions. When an agent generates a piece of code, the LLM can \textit{read} it, understand its logic, identify bugs, and suggest improvements because it has been trained on countless similar examples. In this context, a cyclical, reflective flow is highly effective because the LLM is a competent judge of its own (or its peers') output.
            
            The domain of this study, well construction engineering, presents a starkly different scenario. The knowledge is highly specialized, filled with niche terminology, and often contained within proprietary corporate documents that do not form a significant part of the public web crawl used to train general-purpose LLMs. Consequently, when an agent in our experiment retrieves a technical snippet from a lessons-learned document, the LLM lacks the deep, specialized knowledge required to effectively critique it. It cannot reliably discern subtle inaccuracies, determine if the context is fully appropriate, or judge whether a sub-agent's reasoning is sound from an engineering perspective.
            
            In this context of a \textit{domain knowledge deficit}, the cyclical flow of an agentic system becomes a liability rather than an asset. The reflective loop introduces computational and cost overhead (more LLM calls, more complex state management) without a corresponding improvement in the quality of reasoning. The agent may cycle, but it does so without true insight, making the additional complexity ineffective. This leads to the observed result: the more straightforward, non-agentic approach outperforms it.

            It is important to stress that this does not dismiss the value of multi-agent architectures in general. They are plausibly advantageous for: (i) questions with inherent uncertainty or subjectivity that benefit from debate (e.g., investment decisions) and (ii) complex, multi-phase reasoning workflows where specialized agents contribute distinct skills across stages. Our dataset, centered on retrieve-and-synthesize questions over technical documents, may not fully elicit these strengths, which suggests that task selection strongly mediates the benefits of agentic reasoning.

        \subsubsection{The Unsurprising Efficacy of Intelligent Routing}
            
            While the agentic systems underperformed, the success of the \textit{Linear-Flow w/ Router} configuration is, in itself, a significant finding. Its superior performance can be attributed to its focused efficiency. Rather than engaging in a complex, multi-step reasoning process, this architecture excels at a single, critical task: intent classification and tool selection.
            
            The initial ``router'' call is a highly targeted use of the LLM's reasoning power. Its sole purpose is to analyze the user's query and map it to the most appropriate knowledge base (tool). This is a task that even a general-purpose LLM can perform well, as it relies on semantic understanding rather than deep domain expertise. By correctly identifying the right tool from the outset, the router ensures that the subsequent retrieval step is already on the right path.
            
            This approach of ``decide once, execute well'' proved more effective for this dataset than the agents' ``execute, reflect, re-execute'' loop. It avoids the risk of error propagation inherent in cyclical systems. In an agentic loop, a minor misinterpretation in an early step can be amplified in subsequent cycles as the agent doubles down on a flawed path. The linear flow of the router configuration is immune to this, as there are no subsequent cycles to compound an error. The simplicity of its prompt and execution logic also reduces the chance of ``meta-errors'', where the LLM becomes confused by the complex state and instructions of a multi-turn agentic conversation.
        
        \subsubsection{Other Plausible Contributing Factors}
        
            Beyond the primary hypothesis of the domain knowledge deficit, several other factors likely contributed to the observed results:
            
            \begin{itemize}
                \item \textbf{Nature of the Task}: The questions in the experimental dataset, while technically complex, are primarily information retrieval tasks. They demand finding the correct facts from the knowledge base and synthesizing them into an answer. They do not necessarily require the kind of complex, multi-step deliberation or creative problem-solving where a reflective agent might theoretically excel. For such ``retrieve-and-synthesize'' queries, optimizing the retrieval step, as the router does, yields the greatest performance gain.
                
                \item \textbf{Model Capabilities (GPT-4o vs. GPT-4o-mini)}: The consistent and significant performance gap between GPT-4o and GPT-4o-mini across all configurations underscores the critical importance of the underlying model's reasoning and instruction-following capabilities. The superior performance of the \textit{Linear-Flow w/ Router} on GPT-4o (F1-Score 0.702) compared to the same configuration on GPT-4o-mini (F1-Score 0.604) highlights that even the most effective architecture is gated by the power of the engine driving it.
                
                \item \textbf{High Performance Variance}: A crucial observation is the high standard deviation across all configurations. This indicates that performance is highly dependent on the specific question being asked. Even the best-performing system failed completely on some queries (minimum F1-Score of 0.333), while succeeding perfectly on others (maximum F1-Score of 1.000). This suggests that no single architecture is universally optimal. Certain query structures or topics may inherently favor one approach over another, reinforcing the conclusion that the optimal architecture is task- and domain-dependent.
            \end{itemize}
            
            In conclusion, this experiment provides strong evidence that for specialized domains where LLMs lack deep pre-existing knowledge, the architectural focus should be on optimizing the interface between the query and the external knowledge tools. A simple, intelligent routing mechanism that accurately directs a query to the correct source can be more robust and effective than a complex, cyclical agentic system that attempts to ``reason'' in a domain it does not truly understand. The assumption that more complexity equals better performance is a fallacy; true effectiveness comes from aligning the architectural design with both the nature of the task and the inherent capabilities and limitations of the language model itself.
\chapter{Conclusion}
\label{chap:conclusion}

This dissertation embarked on a systematic investigation into the effectiveness, efficiency, and practical viability of various LLM architectures for domain-specific information retrieval in well construction engineering. Motivated by the transformative potential of Generative AI, this research sought to move beyond generalized benchmarks and provide empirical, actionable insights for the oil and gas industry. Through two distinct experimental cycles, detailed in Chapters \ref{chap:first_experiment} and \ref{chap:second_experiment}, this study compared non-agentic, single-agent, and multi-agent systems, culminating in a series of findings that both validate and challenge prevailing assumptions about the application of agentic AI in specialized technical domains.

This concluding chapter synthesizes the results of the research by directly addressing the research questions posed in the Introduction (Section~\ref{sec:research_questions}). It will then summarize the primary contributions of this work, acknowledge its limitations, and propose promising directions for future research.

\section{Answering the Research Questions}
\label{sec:conclusion_rq}

    The core of this investigation was structured around three central research questions. The evidence gathered from the two experimental cycles provides the following answers.

    \vspace{\baselineskip}
    \begin{tcolorbox}[colback=gray!10, colframe=gray!40, title=\textbf{RQ1: Performance and Task-Dependency}]
    Which architecture (non-agentic, single-agent, or multi-agent) provides the highest factual accuracy and overall performance for different types of domain-specific tasks, specifically complex Q\&A and structured Text-to-SQL generation?
    \end{tcolorbox}
    \vspace{\baselineskip}

    The answer to this question evolved significantly between the two experiments, highlighting the importance of rigorous, quantitative evaluation.

    \begin{itemize}
        \item For \textbf{complex Q\&A tasks}, the most effective architecture was decisively a \textbf{non-agentic \texttt{Linear-Flow with Router}}. The second, more rigorous experiment  (Chapter~\ref{chap:second_experiment}) demonstrated that this setup achieved the highest F1-Score (0.702 with GPT-4o), as shown in Table~\ref{tab:performance_metrics}, outperforming both single-agent (0.643) and multi-agent (0.664) systems. While the first experiment suggested that a multi-agent system yielded higher \textit{truthfulness} (Table~\ref{tab:tabela_resultados}), the more precise metrics of the second cycle revealed that accurately routing the query to the correct knowledge source from the outset was more effective than relying on complex, cyclical agentic reasoning.

        \item For \textbf{Text-to-SQL tasks}, the first experiment revealed that a \textbf{single-agent architecture was surprisingly more effective} than a multi-agent one, as detailed in the results of Chapter~\ref{chap:first_experiment}. This suggests that for more structured, less ambiguous tasks, the communication and coordination overhead of a multi-agent system can be detrimental, introducing unnecessary complexity without a corresponding performance benefit.
    \end{itemize}

    \vspace{\baselineskip}
    \begin{tcolorbox}[colback=gray!10, colframe=gray!40, title=\textbf{RQ2: Cost-Effectiveness}]
    What is the relationship between architectural complexity and economic cost? How do the performance benefits of more complex systems (e.g., multi-agent) weigh against their significantly increased computational (API) costs, and what are the implications for practical deployment?
    \end{tcolorbox}
    \vspace{\baselineskip}

    There is a \textbf{direct and significant relationship between architectural complexity and economic cost}.

    The first experiment quantified this trade-off clearly: the multi-agent architecture, while offering an increase from 3.88 to 4.57 in truthfulness for Q\&A tasks, was on average \textbf{3.7 times more expensive} in terms of LLM API costs (see Figure~\ref{fig:truthfulness_vs_cost_vs_config_model}). This increase is driven by the multiple intermediate LLM calls required for inter-agent communication, deliberation, and coordination.

    The most critical finding for practical deployment comes from the second experiment: the \textbf{most cost-effective architecture (\texttt{Linear-Flow with Router}) was also the highest-performing}. This discovery, evidenced in Table~\ref{tab:performance_metrics}, resolves the cost-benefit dilemma in this specific context. There is no need to pay a premium for a complex agentic system when a simpler, more direct, and cheaper non-agentic architecture yields superior results. For organizations in the O\&G sector, the implication is clear: practical deployment should prioritize efficient, well-directed RAG pipelines over theoretically powerful but ultimately less effective agentic systems. While the absolute cost of LLM APIs continues to fall, architectural efficiency remains a dominant factor in the total cost of ownership.

    \vspace{\baselineskip}
    \begin{tcolorbox}[colback=gray!10, colframe=gray!40, title=\textbf{RQ3: Agentic Systems and Domain Specificity}]
    Under what conditions do agentic architectures, with their capacity for cyclical reasoning and reflection, offer a tangible performance advantage over simpler, non-agentic RAG workflows in a highly specialized technical domain where the LLM has a significant "knowledge deficit"?
    \end{tcolorbox}
    \vspace{\baselineskip}

    Agentic architectures offer a tangible performance advantage only when the underlying LLM possesses \textbf{sufficient foundational knowledge of the domain} to make its reflective capabilities meaningful.

    This dissertation's central hypothesis, validated by the results of the second experiment (Chapter~\ref{chap:second_experiment}) and detailed in the discussion in Section~\ref{sec:exp2-discussion}, is the concept of the \textbf{"domain knowledge deficit"}. The primary strength of an agentic system is its ability to self-critique and iteratively refine its approach. However, this capability is fundamentally compromised when the LLM lacks the specialized, pre-existing knowledge to accurately judge the quality of retrieved information. In the domain of well construction engineering, the LLM cannot reliably discern subtle inaccuracies or assess the contextual relevance of technical data from proprietary documents.

    Consequently, the agent's reflective loop becomes ineffective; it iterates without substantive insight, adding computational cost and complexity without commensurate performance gains. Therefore, the condition under which agentic systems are likely to excel is the \textbf{absence of a severe knowledge deficit}. In domains where the LLM is already a competent "junior analyst" (e.g., general programming, finance), agentic reflection is powerful. In highly niche industrial domains, a streamlined, non-agentic workflow that focuses on perfecting the retrieval step is the superior strategy.

\section{Summary of Contributions}
\label{sec:conclusion_contributions}

    This dissertation makes several contributions to the field of applied AI:

    \begin{enumerate}
        \item \textbf{Primary Theoretical Contribution:} The most significant contribution is the empirical evidence that, contrary to the prevailing hype, \textbf{non-agentic architectures can outperform more complex agentic systems in specialized domains}. This work introduces and validates the \textbf{"domain knowledge deficit"} as a key explanatory framework (Section~\ref{sec:exp2-discussion}) for why the reflective capabilities of current LLM agents fail in niche technical contexts.

        \item \textbf{Methodological Contribution:} This research developed and executed a \textbf{robust, automated evaluation pipeline using an LLM-as-a-judge}, a concept explored by \citet{Zheng2023}. This methodology, detailed in Chapter~\ref{chap:second_experiment}, provides a scalable and objective alternative to manual expert evaluation, enabling the calculation of quantitative metrics (Precision, Recall, F1-Score) and facilitating more rigorous comparisons of RAG systems.

        \item \textbf{Practical Contribution:} The findings provide \textbf{clear, evidence-based guidance for the O\&G industry}. Instead of defaulting to complex and costly agentic frameworks, organizations should focus their efforts on building highly efficient retrieval and routing mechanisms. The \texttt{Linear-Flow with Router} architecture, described in Chapter~\ref{chap:second_experiment}, serves as a powerful and practical template for developing high-performing, cost-effective information retrieval solutions.

        \item \textbf{Scholarly Contribution:} Portions of this work have been disseminated as a \textbf{peer-reviewed journal publication} in the petroleum engineering literature, helping to position these results within the broader scientific discourse and to support their reproducibility and impact \citep{Sabbagh2024}.
    \end{enumerate}

\section{Limitations and Future Work}
\label{sec:conclusion_future_work}

    No research is without limitations. The findings of this study are based on a specific dataset and a set of knowledge bases within the well construction domain. While the "domain knowledge deficit" is a generalizable concept, its specific impact may vary in other domains. Furthermore, the LLM-as-a-Judge approach, while scalable, has its own potential biases, as noted by \citet{Gu2025}. Finally, the LLM landscape is evolving at an extraordinary pace; the specific performance of the models used is a snapshot in time, though the architectural insights are likely more enduring.

    These limitations pave the way for several exciting avenues for future research:

    \begin{itemize}
        \item \textbf{Mitigating the Knowledge Deficit:} Future work should focus on strategies to "upskill" the LLM, such as extensive fine-tuning on proprietary corporate data. This aligns with the trend toward specialized models \citep{Shah2024, Meena2023, Ghosh2023} and could determine if the knowledge deficit can be reduced to a point where agentic reflection becomes effective.
        \item \textbf{Dataset Scale and Statistical Power:} Expanding the size and diversity of the evaluation dataset would increase statistical representativeness and enable stronger inference (e.g., narrower confidence intervals, more reliable significance testing, stratified analyses by question type). A larger dataset would also help assess robustness across rare but practically important query categories.
        \item \textbf{Advanced Routing:} Research into more sophisticated, dynamic routing mechanisms that can handle multi-tool dependencies and conditional logic without the full overhead of an agentic loop could yield further performance gains.
        \item \textbf{Hybrid Architectures:} Exploring hybrid systems that use an efficient router for initial tool selection but grant selective, minimal agentic capabilities for specific, well-defined sub-tasks could offer a compelling balance of efficiency and power.
        \item \textbf{Generalizability:} Applying this dissertation's evaluation framework to other specialized domains (e.g., legal, aerospace, pharmaceuticals) would be invaluable for testing the generalizability of the "domain knowledge deficit" hypothesis.
    \end{itemize}

\section{Final Remarks}
\label{sec:conclusion_final}

    The journey to effectively harness the power of LLMs within specialized industries is just beginning. This dissertation demonstrates that the path to success is not necessarily paved with greater complexity. For the unique challenges of well construction engineering, and likely many other technical fields, the most effective AI systems are not those that attempt to replicate human cognition in a domain they do not understand, but those that are engineered for maximal efficiency at the crucial interface between the user's query and the organization's knowledge. The counter-intuitive success of a simple, non-agentic router over its more complex agentic cousins, as demonstrated in Chapter~\ref{chap:second_experiment}, provides a crucial lesson: in the world of applied AI, architectural choices must be guided by a sober assessment of a model's real-world capabilities, where well-directed simplicity often triumphs over unguided complexity.

\chapter{Experiment 2}
    
    \section{Artifacts}

        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{images_exp2/diagrams/diagrama_todos.png}
            \caption{Comparison between the implemented RAG configurations.}
            \label{fig:diagrama_todos}
        \end{figure}

        \FloatBarrier

    \section{Dataset}
    \label{app:dataset}

    \begin{landscape}
    \scriptsize
        \begin{longtable}{|p{3cm}|p{22cm}|}
        \caption{Dataset used in the thesis experiments (Landscape).}\label{tab:dataset_landscape} \\
        \hline \textbf{Question} & \textbf{Ground Truth} \\ \hline
        \endfirsthead
        \multicolumn{2}{c}%
        {{\tablename\ \thetable{} -- Continued from previous page}} \\
        \hline \textbf{Question} & \textbf{Ground Truth} \\ \hline
        \endhead
        \hline \multicolumn{2}{|r|}{{Continued on next page}} \\ \hline
        \endfoot
        \hline \hline
        \endlastfoot
        % --- Data Starts Here ---
        Cite acidentes envolvendo colisÃ£o entre top drive e outros equipamentos. & 
        1. ColisÃ£o do Top Drive com Coluna de ProduÃ§Ã£o na Mesa Auxiliar
        Durante a montagem de coluna de produÃ§Ã£o 6 5/8" na mesa auxiliar, o top drive colidiu com o Ãºltimo tubo acunhado na mesa rotativa, causando empenamento do tubo. A causa foi falha de planejamento e avaliaÃ§Ã£o de risco, pois o top drive nÃ£o foi retraÃ­do totalmente antes da descida. 
        \newline 2. ColisÃ£o entre o Top Drive e a Hydraracker
        Durante manobra de descida de coluna de Drill Pipe 5", o top drive colidiu com o braÃ§o principal da Hydra Racker que estava sendo estendido. A colisÃ£o provocou o desprendimento entre o braÃ§o principal e o braÃ§o inferior da Hydra Racker, com queda de uma peÃ§a de 2,7kg de altura de 27m. Causas incluÃ­ram operaÃ§Ã£o em modo manual e configuraÃ§Ã£o inadequada do sistema anticolisÃ£o.
        \newline 3. ColisÃ£o do Top Drive com SeÃ§Ã£o de Drill Pipe
        Durante manobra de retirada do BHA de 8 7/8", o top drive colidiu com a seÃ§Ã£o de Drill Pipe sustentada pelo PRS (Pipe Racking System). Com o impacto, a carenagem frontal do top drive foi cisalhada e caiu no piso do rigfloor de uma altura de 20m, pesando aproximadamente 85kg. 
        \newline 4. InteraÃ§Ã£o do Top Drive Auxiliar com Equipamentos de SeguranÃ§a
        Durante conexÃ£o de mangueira MPD, o cabo da trava quedas de um plataformista ficou preso no arranjo de iÃ§amento do top drive auxiliar, provocando a suspensÃ£o do trabalhador. O incidente ocorreu por falha na anÃ¡lise de risco e comunicaÃ§Ã£o inadequada. \\ \hline        
        Quais as causas e falhas tÃ­picas de acidentes ou incidentes envolvendo guarda corpo?  Cite alertas que retratem isso. & 
        Os acidentes ou incidentes envolvendo guarda-corpo geralmente ocorrem devido a falhas de projeto, gestÃ£o de processos, especificaÃ§Ã£o inadequada do serviÃ§o, e falta de procedimentos especÃ­ficos. Aqui estÃ£o algumas causas e falhas tÃ­picas.
        \newline 1. Falha de Equipamentos e Projeto de Engenharia Deficiente: O projeto do guarda-corpo pode nÃ£o ser adequado para a operaÃ§Ã£o, levando a deslocamentos inesperados e acidentes. (Fonte: Definitivo - LesÃ£o no dorso do pÃ© direito devido ao choque com guarda corpo\_ POCOS SM \_Abrange+2023-000154)         
        \newline 2. Falha na GestÃ£o de Processos e Pessoas: Inclui falhas no processo de qualificaÃ§Ã£o da empresa contratada, violaÃ§Ã£o de procedimentos por supervisores, e falta de documentos especÃ­ficos para avaliaÃ§Ã£o de operaÃ§Ãµes. (Fonte: Definitivo - LesÃ£o no dorso do pÃ© direito devido ao choque com guarda corpo\_ POCOS SM \_Abrange+2023-000154)         
        \newline 3. Falha de IdentificaÃ§Ã£o de Risco Adicional de Queda: NÃ£o identificar riscos adicionais, como vÃ£os entre a gaiola da escada e o guarda-corpo, pode resultar em quedas fatais. (Fonte: Acidente fatal-Queda de altura - REVAP-MA-PM\_ Abrange 2023-000279)         
        \newline 4. Movimento Irregular da Carga e Falha no Freio do Guindaste: Durante movimentaÃ§Ãµes de carga, falhas no equipamento podem causar impactos que danificam o guarda-corpo e resultam em quedas de objetos. (Fonte: Alerta Definitivo - Vermelho - Descida descontrolada de carga - DivulgaÃ§Ã£o equipe movimentaÃ§Ã£o carga) \\ \hline
Quais foram as principais falhas identificadas em acidentes com pÃ©s, de acordo com os Alertas de SMS? & As principais falhas identificadas em acidentes envolvendo pÃ©s incluem:        
        \newline 1. Planejamento inadequado e falta de anÃ¡lise de risco: Muitos acidentes ocorreram devido Ã  falta de planejamento adequado e anÃ¡lise de risco antes de realizar atividades, como movimentaÃ§Ã£o de cargas e iÃ§amento de peÃ§as. Isso foi observado em acidentes como o de fratura em trÃªs metatarsos do pÃ© esquerdo durante a fabricaÃ§Ã£o de um corrimÃ£o na oficina de solda ("Preliminar AMARELO - Acidente com LesÃ£o - Fratura - Geral - POCOS SM 055 2022").        
        \newline 2. Uso inadequado de Equipamentos de ProteÃ§Ã£o Individual (EPI): Em alguns casos, o uso inadequado ou a falta de EPI apropriado, como botas de seguranÃ§a com proteÃ§Ã£o para metatarso, contribuiu para a ocorrÃªncia de acidentes. Isso foi destacado em acidentes como o corte contuso no pÃ© esquerdo durante a manutenÃ§Ã£o de um gerador ("Definitivo AMARELO - Corte contuso no pÃ©\_ Geral\_ POCOS SM 019\_rev01").        
        \newline 3. Falta de percepÃ§Ã£o de riscos e consciÃªncia situacional: A falta de percepÃ§Ã£o de riscos e a escolha de caminhos mais curtos sem avaliar o ambiente contribuÃ­ram para acidentes, como a torÃ§Ã£o no tornozelo ao pisar em uma madeira de apoio de carga ("Definitivo -TorÃ§Ã£o no tornozelo ao pisar sobre uma peÃ§a de madeira - POCOS SM 025 2023").        
        \newline 4. Falhas na supervisÃ£o e no cumprimento de diretrizes: A supervisÃ£o inadequada e o nÃ£o cumprimento de diretrizes e procedimentos de seguranÃ§a foram fatores em acidentes, como o de fratura no tornozelo ao tropeÃ§ar em um umbilical no convÃ©s ("Definitivo VERMELHO - Fratura no tornozelo\_Geral\_ POCOS SM 016 2021").
        \newline 5. EPI Inadequado: Uso de botas sem caracterÃ­sticas anti-torÃ§Ã£o e solado adequado para ambientes offshore, como mencionado no acidente de torÃ§Ã£o de tornozelo ao descer escada (POCOS SM 009 2021). \\ \hline  
Como a presenÃ§a de sÃ­lica na composiÃ§Ã£o da pasta de cimento afeta sua estabilidade tÃ©rmica em altas temperaturas, de acordo com as liÃ§Ãµes aprendidas? & A sÃ­lica Ã© adicionada ao cimento na proporÃ§Ã£o de 35\% por peso de cimento com o objetivo de mitigar a retrogressÃ£o (perda da resistÃªncia compressiva) do cimento no poÃ§o quando submetido a temperaturas superiores a 230Â°F (110Â°C). Esta prÃ¡tica Ã© comum em poÃ§os profundos ou com gradiente geotÃ©rmico elevado que, em suas fases finais, ultrapassam a temperatura limite. A presenÃ§a de sÃ­lica na pasta de cimento tambÃ©m pode ser necessÃ¡ria em situaÃ§Ãµes onde a temperatura nos anulares dos revestimentos durante a produÃ§Ã£o do poÃ§o seja bastante elevada, mesmo em menores profundidades.
        \newline        A ausÃªncia de sÃ­lica no cimento sujeito a temperaturas superiores a 230Â°F pode resultar numa perda de resistÃªncia compressiva no cimento, levando a problemas de isolamento.
        \newline        Entretanto, Ã© importante observar que a mÃ¡ preparaÃ§Ã£o da pasta de cimento pode resultar em propriedades da pasta inhomogÃªneas durante a fabricaÃ§Ã£o e bombeamento. Isso pode levar a gelificaÃ§Ã£o prematura e possÃ­vel prisÃ£o da coluna. Em caso de anormalidades na preparaÃ§Ã£o da pasta, recomenda-se a realizaÃ§Ã£o da cimentaÃ§Ã£o em bateladas.
        
        ReferÃªncias:
        \newline        Alerta TÃ©cnico AT5533: UtilizaÃ§Ã£o de cimento com sÃ­lica na cimentaÃ§Ã£o de revestimentos de superfÃ­cie. Autor: Jan Fernandes Aslan. Data: 16/11/2020.
        \newline        Alerta TÃ©cnico AT10479: SubstituiÃ§Ã£o indevida de cimento com sÃ­lica por cimento puro em cimentaÃ§Ã£o de revestimento de superfÃ­cie em poÃ§o com temperatura elevada. Autor: Rafael Peralta Muniz Moreira. Data: 14/02/2021.
        \\ \hline

De acordo com as liÃ§Ãµes aprendidas, como a inclinaÃ§Ã£o do poÃ§o afeta a eficÃ¡cia da cimentaÃ§Ã£o durante o tamponamento through tubing? & Durante o tamponamento through tubing em poÃ§os de alta inclinaÃ§Ã£o (maior que 45 graus), a eficÃ¡cia da cimentaÃ§Ã£o pode ser afetada por fatores advindos da inclinaÃ§Ã£o do poÃ§o. Segundo um Alerta TÃ©cnico (AT9613), essa situaÃ§Ã£o aumenta a complexidade das operaÃ§Ãµes de tamponamento devido ao fato de a coluna de produÃ§Ã£o/injeÃ§Ã£o estar descentralizada dentro do revestimento de produÃ§Ã£o. 
        \newline        AlÃ©m disso, outras dificuldades surgem, como:
        \newline        - Dificuldade no assentamento de cement retainer inflÃ¡vel
        - Dificuldade na descida de ferramentas de perfilagem e plugs para isolamento preliminar abaixo do trecho do poÃ§o a ser tamponado
        \newline        Estes fatores podem resultar em poÃ§os com qualidade mÃ©dia ou falha na cimentaÃ§Ã£o. Recomenda-se por agora, atÃ© que estudos mais aprofundados sejam realizados, uma anÃ¡lise criteriosa de cada cenÃ¡rio e a escolha de alternativas mais robustas para o abandono de poÃ§os de alta inclinaÃ§Ã£o e a seleÃ§Ã£o de escopo de projeto executivo de abandono, detalhamento operacional e estimativa dos riscos e tempos da intervenÃ§Ã£o.
        \newline        Por outro lado, existem algumas alternativas como o Bombeio Direto de Cimento (BDC) para abandono permanente through tubing (TT). Isso permite o bombeio de pasta de cimento diretamente pelo riser dual bore, passando pela COP/COI atÃ© ser posicionada em anular â€œAâ€ (COP x revestimento). No entanto, essa tÃ©cnica tambÃ©m tem suas restriÃ§Ãµes e premissas, como necessidade de emprego de Riser Dual Bore e menor tolerÃ¢ncia ao erro no deslocamento de pasta.
        \newline        Em resumo, a inclinaÃ§Ã£o do poÃ§o pode afetar a eficÃ¡cia da cimentaÃ§Ã£o durante o tamponamento atravÃ©s do tubo, tornando a operaÃ§Ã£o mais complexa e aumentando o risco de falhas na cimentaÃ§Ã£o. Diferentes abordagens e tecnologias estÃ£o sendo estudadas para otimizar essas operaÃ§Ãµes e minimizar os riscos. \\ \hline

De acordo com as liÃ§Ãµes aprendidas, quais sÃ£o os riscos associados Ã  substituiÃ§Ã£o de cimento com sÃ­lica por cimento puro em cimentaÃ§Ãµes de revestimento de superfÃ­cie em poÃ§os com temperatura elevada? & Estes sÃ£o os principais riscos associados Ã  substituiÃ§Ã£o indevida de cimento com sÃ­lica por cimento puro em poÃ§os de alta temperatura:
        \newline        1. O cimento puro, quando submetido a altas temperaturas (acima de 110Â°C), sofre uma regressÃ£o de sua resistÃªncia compressiva ao longo do tempo, podendo comprometer sua funÃ§Ã£o em poÃ§os de petrÃ³leo como elemento de barreira.
        \newline        2. Falhas na cimentaÃ§Ã£o do revestimento de superfÃ­cie podem levar a custos e impactos adicionais, como o vazamento de fluidos de perfuraÃ§Ã£o para o fundo do mar e o aumento das cargas hidrostÃ¡ticas sobre o revestimento de superfÃ­cie e os revestimentos subsequentes instalados.
        \newline        3. Em alguns casos, a bainha de cimento pode ser exposta a temperaturas acima de 110Â°C em algum momento do ciclo de vida do poÃ§o. Nesses casos, o uso de cimento com sÃ­lica deve ser mantido para garantir a estabilidade tÃ©rmica da pasta de cimento.
        \newline        4. A substituiÃ§Ã£o indevida pode levar a outros riscos como escape de fluidos de perfuraÃ§Ã£o do anular C para o fundo do mar, alteraÃ§Ãµes no dimensionamento do revestimento, alteraÃ§Ãµes no desenho de projeto de inÃ­cio de poÃ§o, entre outros.
        \newline        Deve-se ressaltar que a intenÃ§Ã£o de reduzir o tempo de aguardo de cura do cimento nÃ£o justifica a substituiÃ§Ã£o do cimento com sÃ­lica por cimento puro, pois existem outros mÃ©todos para alcanÃ§ar uma cura mais rÃ¡pida, como o aumento da densidade da pasta ou o uso de aditivos.
        \newline        Esse fenÃ´meno Ã© conhecido na indÃºstria do petrÃ³leo desde 1954 e foi mitigado pela introduÃ§Ã£o de sÃ­lica cristalina na composiÃ§Ã£o da pasta de cimento para garantir a estabilidade tÃ©rmica. \\ \hline

Quais estratÃ©gias podem ser adotadas para melhorar a qualidade da cimentaÃ§Ã£o em poÃ§os de alta inclinaÃ§Ã£o durante o tamponamento through tubing, de acordo com as liÃ§Ãµes aprendidas? & Com base nos itens de conhecimento obtidos, algumas estratÃ©gias para melhorar a qualidade da cimentaÃ§Ã£o em poÃ§os de alta inclinaÃ§Ã£o durante o tamponamento through tubing incluem:
        \newline        1. Aumento das vazÃµes no deslocamento quando a cimentaÃ§Ã£o Ã© executada com flexitubo. Este procedimento auxilia na melhor distribuiÃ§Ã£o do cimento no espaÃ§o anular e contribui para uma melhor qualidade de cimentaÃ§Ã£o.
        \newline        2. UtilizaÃ§Ã£o de dispositivos para vibraÃ§Ã£o da coluna durante a cimentaÃ§Ã£o. A vibraÃ§Ã£o pode ajudar a melhorar a aderÃªncia do cimento no espaÃ§o anular tornando a cimentaÃ§Ã£o mais eficaz.
        \newline        3. UtilizaÃ§Ã£o de volumes de excesso de pasta de cimento compatÃ­veis com uma operaÃ§Ã£o que busque robustez mÃ¡xima. Este procedimento garante que haja cimento suficiente para preencher o espaÃ§o anular ao longo de todo o comprimento da coluna.
        \newline        4. AnÃ¡lise criteriosa de cada cenÃ¡rio e seleÃ§Ã£o de uma alternativa mais robusta para o abandono (preferencialmente com a retirada da coluna). Isso Ã© particularmente importante para poÃ§os de alta inclinaÃ§Ã£o e outros cenÃ¡rios que adicionem complexidade ao tamponamento.
        \newline        5. PrevisÃ£o no cronograma da intervenÃ§Ã£o da perfilagem de avaliaÃ§Ã£o da cimentaÃ§Ã£o como mÃ©todo de verificaÃ§Ã£o. Isso permite a identificaÃ§Ã£o de possÃ­veis falhas ou Ã¡reas de cimentaÃ§Ã£o insatisfatÃ³ria para um possÃ­vel retoque da cimentaÃ§Ã£o.
        \newline        6. Uso do Bombeio Direto de Cimento (BDC) para abandono permanente through tubing, uma tÃ©cnica que pode otimizar o tempo e reduzir os riscos operacionais, embora com restriÃ§Ãµes especÃ­ficas.
        \newline        Por Ãºltimo, Ã© importante lembrar que toda situaÃ§Ã£o Ã© Ãºnica e deve ser avaliada individualmente de acordo com as suas caracterÃ­sticas especÃ­ficas. \\ \hline

Quais sÃ£o as alternativas para acelerar o tempo de cura da pasta de cimento sem comprometer sua integridade em condiÃ§Ãµes de temperatura elevada, de acordo com as liÃ§Ãµes aprendidas? & As melhores prÃ¡ticas incluem o aumento da densidade da pasta, o uso de aditivos e a consideraÃ§Ã£o da rampa de cura, mas Ã© importante manter a utilizaÃ§Ã£o de cimento com sÃ­lica para garantir a integridade da cimentaÃ§Ã£o em condiÃ§Ãµes de temperatura elevada. \\ \hline

Quais sÃ£o os principais desafios e riscos associados ao tamponamento through tubing em poÃ§os de alta inclinaÃ§Ã£o, de acordo com as liÃ§Ãµes aprendidas? & Existem vÃ¡rios desafios e riscos associados ao tamponamento through tubing em poÃ§os de alta inclinaÃ§Ã£o, conforme revelado no Alerta TÃ©cnico "Risco de falha de cimentaÃ§Ã£o em tamponamento through tubing em poÃ§os de alta inclinaÃ§Ã£o (45 graus)". Os principais problemas incluem:
        \newline        - Maior complexidade nas operaÃ§Ãµes de tamponamento devido Ã  descentralizaÃ§Ã£o da coluna de produÃ§Ã£o/injeÃ§Ã£o dentro do revestimento de produÃ§Ã£o em poÃ§os com alta inclinaÃ§Ã£o.
        - Dificuldade no assentamento de cement retainer inflÃ¡vel em poÃ§os inclinados.
        - Dificuldade na descida de ferramentas de perfilagem e plugs para isolamento preliminar abaixo do trecho do poÃ§o a ser tamponado.
        \newline        AlÃ©m disso, foi observado que em poÃ§os de alta inclinaÃ§Ã£o (> 45 graus), a qualidade da cimentaÃ§Ã£o Ã©, muitas vezes, apenas mÃ©dia, cumprindo apenas os requisitos mÃ­nimos e, em alguns casos, levando a falhas na cimentaÃ§Ã£o.
        \newline        No atual cenÃ¡rio, para melhorar os resultados, Ã© sugerido o uso de vazÃµes no deslocamento superiores quando a cimentaÃ§Ã£o Ã© realizada com flexitubo, a realizaÃ§Ã£o do uso de dispositivos para vibraÃ§Ã£o da coluna durante a cimentaÃ§Ã£o e o uso de volumes excedentes de pasta compatÃ­veis com uma operaÃ§Ã£o da maior robustez possÃ­vel.
        \newline        Destaca-se tambÃ©m a importÃ¢ncia do controle de circulaÃ§Ã£o e perda de fluido durante a perfuraÃ§Ã£o da fase do reservatÃ³rio. Em situaÃ§Ãµes de perda de circulaÃ§Ã£o severa, a perfuraÃ§Ã£o com Ã¡gua do mar e tampÃµes viscosos pode ser uma alternativa segura para evitar custos e riscos adicionais.
        \newline        Ã‰ relevante mencionar tambÃ©m o uso de mÃ©todos sustentÃ¡veis e eficientes de combate Ã  perda de fluido pÃ³s-teste de formaÃ§Ã£o, em que um tampÃ£o especial de filtrado relaxado pode ser uma soluÃ§Ã£o viÃ¡vel, trazendo economia e eficiÃªncia operacional.
        \newline        No entanto, atÃ© que um estudo mais aprofundado sobre as causas desses problemas seja realizado e medidas de melhoria sejam implementadas, recomenda-se a seleÃ§Ã£o cuidadosa do plano de abandono e a anÃ¡lise prÃ©via de cada cenÃ¡rio para poÃ§os de alta inclinaÃ§Ã£o. \\ \hline
Cite problemas operacionais ocorridos na sonda NS-48 em 2024, de acordo com o SITOP? Liste todas as ocorrÃªncias que encontrar. & 1. Falha no Sistema de RotaÃ§Ã£o do Top Drive: Houve uma investigaÃ§Ã£o contÃ­nua sobre uma falha no sistema de rotaÃ§Ã£o do top drive, que impossibilitou a conexÃ£o Ã  coluna. Durante o perÃ­odo de espera, o ponto de acunhamento da cunha foi alterado duas vezes. Fonte: <a href="https://wellsom.petrobras.com.br/sitopi/sonda/NS-48/2024-07-19/M" target="blank">Documento da sonda NS-48 - 2024-07-19 11:50</a>
        \newline        2. CondiÃ§Ãµes Ambientais Adversas: Em vÃ¡rias ocasiÃµes, as operaÃ§Ãµes foram interrompidas devido a condiÃ§Ãµes ambientais adversas, como ventos fortes, que impediram a montagem de equipamentos de arame e a retirada de plugues. Fonte: <a href="https://wellsom.petrobras.com.br/sitopi/sonda/NS-48/2024-05-09/M" target="blank">Documento da sonda NS-48 - 2024-05-09 07:30</a>
        \newline        3. Atrasos na InstalaÃ§Ã£o de Equipamentos: Houve atrasos na instalaÃ§Ã£o de equipamentos, como a ANM CCB-462, devido Ã  espera por embarcaÃ§Ãµes de apoio, o que impactou o cronograma das operaÃ§Ãµes. Fonte: <a href="https://wellsom.petrobras.com.br/sitopi/sonda/NS-48/2024-05-21/M" target="blank">Documento da sonda NS-48 - 2024-05-21 05:24</a>
        \newline        4. Problemas com o WSSV Blue Marlim: A sonda enfrentou problemas operacionais devido Ã  indisponibilidade do WSSV Blue Marlim para operar conectado, causada por condiÃ§Ãµes de mar adversas e problemas no gerador do barco. Fonte: <a href="https://wellsom.petrobras.com.br/sitopi/sonda/NS-48/2024-04-13/T" target="blank">Documento da sonda NS-48 - 2024-04-13 13:29</a>
        \newline        5. Falha no sistema de rotaÃ§Ã£o do top drive: Houve uma investigaÃ§Ã£o sobre a falha no sistema de rotaÃ§Ã£o do top drive, que impossibilitou a conexÃ£o Ã  coluna. Durante o perÃ­odo de aguardo, o ponto de acunhamento da cunha foi alterado duas vezes. Fonte: <a href="https://wellsom.petrobras.com.br/sitopi/sonda/NS-48/2024-07-19/M" target="blank">NS-48 - 2024-07-19</a>
        \newline        6. Aguardando troca de turma: Houve um perÃ­odo de espera para troca de turma do WSSV Blue Marlim, durante o qual foram realizadas operaÃ§Ãµes de mini-TI e recalcamento de fluidos. Fonte: <a href="https://wellsom.petrobras.com.br/sitopi/sonda/NS-48/2024-04-12/M" target="blank">NS-48 - 2024-04-12</a> \\ \hline

Em quais poÃ§os foi utilizado Well Stroker para abertura e fechamento de VIF, de acordo com o SITOP? Liste todas as ocorrÃªncias que encontrar. & Foram encontrados 6 poÃ§os com ocorrÃªncia de uso de Well Stroker para abertura e fechamento de VIF
        1. 8-BUZ-55-RJS
        2. 7-BAZ-3-ESS 
        3. 8-RO-146HP-RJS
        4. 7-CRT-52-RJS
        5. 8-BUZ-58D-RJS
        6. 7-JUB-61D-ESS
        7. 8-ATP-7D-RJS
        8. 3-RJS-688A \\ \hline

Em quais poÃ§os haverÃ¡ cimentaÃ§Ã£o de revestimento entre as prÃ³ximas operaÃ§Ãµes, de acordo com o SITOP do dia 13/05/2025? Informe somente a lista de nomes dos poÃ§os. Liste todos os poÃ§os que encontrar, nÃ£o tente resumir. & Nos prÃ³ximos dias, haverÃ¡ operaÃ§Ãµes de cimentaÃ§Ã£o de revestimento nos seguintes poÃ§os:
        \newline        1. PoÃ§o 7-BUZ-95-RJS
        \newline        2. PoÃ§o 7-JUB-81H-ESS
        \newline        3. PoÃ§o 7-BUZ-94D-RJS
        \newline        4. PoÃ§o 8-MRO-36-RJS
        \newline        5. PoÃ§o 8-BUZ-101-RJS
        \newline        6. PoÃ§o 9-AB-138D-RJS
        \newline        7. PoÃ§o Buena Suerte-1
        \newline        8. PoÃ§o 7-BUZ-94D-RJS \\ \hline

Em qual poÃ§o estÃ¡ a sonda NS-39, de acordo com o SITOP? Considere que a data de hoje Ã© 05/01/2025.  & A sonda NS-39 estÃ¡ operando no poÃ§o 7-MRO-37-RJS. \\ \hline

Quais as Ãºltimas operaÃ§Ãµes efetuadas no poÃ§o 7-MRL-244H-RJS, de acordo com o SITOP do dia 05/01/2025? & OperaÃ§Ã£o Principal:
        MP: Circulado FPBNA 9,4 ppg via coluna a 8 bpm / 270 psi.
        MP: Bombeado 50 bbl de colchÃ£o espaÃ§ador 11,0 ppg via bomba da sonda 6 bpm / 230 psi e deslocado com 10 bbl de FPBNA 9,4 ppg a 6 bpm / 230 psi pela bomba da sonda.
        MP: Bombeado 96 bbl de pasta de cimento 15,8 ppg a 4 bpm e deslocado com 6,9 bbl de colchÃ£o espaÃ§ador 11 ppg e com 59 bbl de FPBNA 9,4 ppg.
        MP: Retirada coluna atÃ© 939 m (mais de 200 m acima do TOC), circulado para limpeza da coluna com FPBNA 9,4 ppg e fechado BOP anular.
        MP: Bombeado via UC 68 bbl de FPBNA 9,4 ppg pela coluna e efetuando squeeze do cimento no overlap liner 10,75" x revestimento 13,626" a 1 bpm/ 260 psi, 2 bpm/ 330 psi, 3 bpm/ 340 psi e a 4 bpm/ 410 psi.  
        MP: Iniciado aguardo de pega de 11 h do cimento Ã s 21:30 h.
        \newline        OperaÃ§Ã£o Paralela:
        MA: Continuada montagem e estaleiramento do BHA 8 1/2". 
        MA: Iniciada montagem do Djar e de ferramenta de condicionamento do topo do liner.  \\ \hline

Quais foram as operaÃ§Ãµes executadas na NS-42 em 22/06/2024, de acordo com o SITOP?  & 
        Descido conjunto FDR/TRT com coluna de DPR 6 5/8" atÃ© 936 m.
        Efetuado dummy run com coluna de DPR e ajustado balanceio.
        Conectada coflex na SFT.
        Efetuada plumbagem da SFT.
        Verticalizado arranjo de superfÃ­cie com SFF, EBE, links articulados e conectado DPR 6 5/8" da SFT na coluna.
        Em paralelo, limpado TMF da ANM pelo ROV.
        Montado BOP de arame sobre a SFT.
        \newline        Finalizada montagem das linhas de superfÃ­cie e manifold auxiliar.
        Efetuado teste de estanqueidade das linhas de superfÃ­cie com 300 psi / 4500 psi por 5 min / 10 min.
        Efetuado teste de estanqueidade do DPR atÃ© as valvulas VG5 e XO da FDR com 3500 psi por 10 min. Hold Point aprovado pelo CSD-EQSB
        Efetuado flush do DPR e HCR para limpeza.
        Posicionado FCBA inibido com MEG 10% no interior do DPR e HCR.
        \newline        \\ \hline

Qual  o SITOP de hoje da NS-45? Considere que a data de hoje Ã© 05/01/2025. & A situaÃ§Ã£o operacional da sonda NS-45 em 05/01/2025 Ã© a seguinte:
        \newline         OperaÃ§Ã£o Principal
        - **ManutenÃ§Ã£o do BOP**: As manutenÃ§Ãµes do BOP estÃ£o 63% concluÃ­das. As atividades incluem a limpeza dos receptÃ¡culos, stabs, troca das vedaÃ§Ãµes da interface LMRP/Stack, troca dos packers seals do POD azul, manutenÃ§Ã£o das reguladoras do POD azul e amarelo, e lubrificaÃ§Ã£o do riser connector.
        - **SituaÃ§Ã£o Atual**: A operaÃ§Ã£o estÃ¡ aguardando a substituiÃ§Ã£o do conector para 16 3/4" e a troca da gaveta de tubos fixa do BOP, alÃ©m de manutenÃ§Ãµes e testes. A previsÃ£o de conclusÃ£o dessas atividades Ã© para 07/01.
        - **PrÃ³ximas OperaÃ§Ãµes**: Prosseguir com a substituiÃ§Ã£o do conector e gaveta de tubos do BOP, manutenÃ§Ãµes e testes (em andamento). EstÃ¡ previsto descer o BOP em 07/01.
        \newline         OperaÃ§Ã£o Paralela
        - **Atividades**: Atualmente, nÃ£o hÃ¡ operaÃ§Ãµes em andamento. EstÃ£o sendo programadas ferramentas MLWD para montagem do BHA e fabricado fluido FPBA para perfuraÃ§Ã£o BHD.
        - **PrÃ³ximas OperaÃ§Ãµes**: EstÃ¡ prevista a montagem do BHA de perfuraÃ§Ã£o com Broca PDC 8 1/2", RSS, Ferramentas MLWDs e Alargador 9 1/2" para 06/01.
        \\ \hline

Qual sonda completou o poÃ§o 7-JUB-62DA-ESS, de acordo com o SITOP?  & A sonda que completou o poÃ§o 7-JUB-62DA-ESS foi a NS-40 \\ \hline

 
        \end{longtable}
    \end{landscape}

    \section{Evaluation Prompt}

\begin{lstlisting}[style=mystyle, language=Python, caption={C\'{o}digo para LLM-as-a-Judge}, label={code:llm-judge}]
class Confusion_Matrix(TypedDict):  # type: ignore
    true_positive: list[str]
    false_positive: list[str]
    true_negative: list[str]
    false_negative: list[str]

def calculate_metrics(llm, question, history):
    if type(question['Ground Truth']) == str:
        prompt_confusion_matrix = f""                
            Voce recebera os seguintes parametros:
            Pergunta: a pergunta do usuario
            Resposta Ideal: a resposta considerada correta por um humano
            Resposta do sistema: a resposta fornecida pelo sistema baseado em IA

            Pegue a resposta do sistema, separe em afirmacoes e classifique cada afirmacao entre as opcoes abaixo:
            True Positive (TP): as afirmacoes corretas feitas pelo sistema, ou seja, que estao presentes na resposta ideal.
            False Positive (FP): as afirmacoes incorretas ou irrelevantes feitas pelo sistema, ou seja, que nao estao presentes na resposta ideal.

            Pegue a resposta ideal, separe em afirmacoes e classifique cada afirmacao entre as opcoes abaixo:
            True Negative (TN): Nao se aplica, deixar vazio.
            False Negative (FN): as afirmacoes que constam na resposta ideal, mas nao foram feitas pelo sistema.

            Importante:
            - Voce deve gerar listas de afirmacoes para cada categoria. 
            - Voce deve quebrar as respostas do sistema e a ideal em afirmacoes objetivas.
            - Se as respostas do sistema ou a ideal contiverem frases grandes com mtas afirmacoes, analisar cada afirmacao separadamente.
            - Uma afirmacao nao pode estar em mais de uma categoria.
            - Ignore coisas na resposta do sistema que nao sao afirmacoes objetivas, como por exemplo citacoes de fontes e links.

            Vamos la!

            ############

            Pergunta: {{{{
            {question['Question']}
            }}}}

            Resposta ideal: {{{{
            {question['Ground Truth']}
            }}}}

            Resposta do sistema: {{{{
            {history[-1].content}
            }}}}"
        response = llm.with_structured_output(Confusion_Matrix).invoke(prompt_confusion_matrix)

        true_positive_count = len(response.get('true_positive', []))
        false_positive_count = len(response.get('false_positive', []))
        true_negative_count = len(response.get('true_negative', []))
        false_negative_count = len(response.get('false_negative', []))

        try:
            precision = true_positive_count / (true_positive_count + false_positive_count)
        except ZeroDivisionError:
            precision = 0
        
        try:
            recall = true_positive_count / (true_positive_count + false_negative_count)
        except ZeroDivisionError:
            recall = 0

        try:
            f1_score = 2 * (precision * recall) / (precision + recall)
        except ZeroDivisionError:
            f1_score = 0
            
        print("\\n\\nPrecision: "+str(precision))
        print("Recall: "+str(recall))
        print("F1 Score: "+str(f1_score))

        print("\\nAnswer Size Ratio: "+str(question['Answer Size (% of GT)']))

        state.precision = precision
        state.recall = recall
        state.f1_score = f1_score
        state.ground_truth = question['Ground Truth']
        state.statements = response
\end{lstlisting}

    \section{Results}
    \label{sec:exp2_appendix}

        \subsection{Precision}


            \subsubsection{Best Precision by Model and Configuration}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/precision/bar_best_precision_by_model_and_configuration.png}
                \caption{Best precision by model and configuration.}
                \label{fig:bar_best_precision_by_model_and_configuration}
            \end{figure}

            \subsubsection{Best Precision by Question Index and Configuration}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/precision/best_precision_by_question_index_and_configuration.png}
                \caption{Best precision by question index and configuration.}
                \label{fig:best_precision_by_question_index_and_configuration}
            \end{figure}

            \subsubsection{Best Precision by Question Index and Model}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/precision/best_precision_by_question_index_and_model.png}
                \caption{Best precision by question index and model.}
                \label{fig:best_precision_by_question_index_and_model}
            \end{figure}

            \subsubsection{Facet Histogram of Precision by Model}
            \begin{figure}[H]
                \centering
                \includegraphics[width=\textwidth]{images_exp2/precision/facet_hist_precision_by_model.png}
                \caption{Facet histogram of precision by model.}
                \label{fig:facet_hist_precision_by_model}
            \end{figure}


            \subsubsection{Histogram of All Precisions}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/precision/hist_precision_all.png}
                \caption{Histogram of all precisions.}
                \label{fig:hist_precision_all}
            \end{figure}

            \subsubsection{Line Plot of Precision by Question Index and Model}
            \begin{figure}[H]
                \centering
                \includegraphics[width=\textwidth]{images_exp2/precision/line_precision_by_question_index_and_model.png}
                \caption{Line plot of precision by question index and model.}
                \label{fig:line_precision_by_question_index_and_model}
            \end{figure}


            \subsubsection{Precision by Model and Configuration}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/precision/precision_by_model_and_configuration.png}
                \caption{Precision by model and configuration.}
                \label{fig:precision_by_model_and_configuration}
            \end{figure}

            \subsubsection{Line Plot of Precision by Question Index and Configuration}
            \begin{figure}[H]
                \centering
                \includegraphics[width=\textwidth]{images_exp2/precision/precision_lineplot_by_question_index_and_configuration.png}
                \caption{Line plot of precision by question index and configuration.}
                \label{fig:precision_lineplot_by_question_index_and_configuration}
            \end{figure}

            \subsubsection{Scatter Plot of Precision vs. Total Time}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/precision/scatter_precision_vs_total_time.png}
                \caption{Scatter plot of precision vs. total time.}
                \label{fig:scatter_precision_vs_total_time}
            \end{figure}

            \subsubsection{Scatter Plot of Precision vs. Total Token Count Input}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/precision/scatter_precision_vs_total_token_count_input.png}
                \caption{Scatter plot of precision vs. total token count input.}
                \label{fig:scatter_precision_vs_total_token_count_input}
            \end{figure}

            
        \subsection{Recall}


            \subsubsection{Best Recall by Model and Configuration}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/recall/bar_best_recall_by_model_and_configuration.png}
                \caption{Best recall by model and configuration.}
                \label{fig:bar_best_recall_by_model_and_configuration}
            \end{figure}

            \subsubsection{Best Recall by Question Index and Configuration}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/recall/best_recall_by_question_index_and_configuration.png}
                \caption{Best recall by question index and configuration.}
                \label{fig:best_recall_by_question_index_and_configuration}
            \end{figure}

            \subsubsection{Best Recall by Question Index and Model}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/recall/best_recall_by_question_index_and_model.png}
                \caption{Best recall by question index and model.}
                \label{fig:best_recall_by_question_index_and_model}
            \end{figure}

            \subsubsection{Facet Histogram of Recall by Model}
            \begin{figure}[H]
                \centering
                \includegraphics[width=\textwidth]{images_exp2/recall/facet_hist_recall_by_model.png}
                \caption{Facet histogram of recall by model.}
                \label{fig:facet_hist_recall_by_model}
            \end{figure}


            \subsubsection{Histogram of All Recalls}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/recall/hist_recall_all.png}
                \caption{Histogram of all recalls.}
                \label{fig:hist_recall_all}
            \end{figure}

            \subsubsection{Line Plot of Recall by Question Index and Model}
            \begin{figure}[H]
                \centering
                \includegraphics[width=\textwidth]{images_exp2/recall/line_recall_by_question_index_and_model.png}
                \caption{Line plot of recall by question index and model.}
                \label{fig:line_recall_by_question_index_and_model}
            \end{figure}


            % \subsubsection{Recall by Model and Configuration}
            % \begin{figure}[H]
            %     \centering
            %     \includegraphics[scale=0.75]{images_exp2/recall/recall_by_model_and_configuration.png}
            %     \caption{Recall by model and configuration.}
            %     \label{fig:recall_by_model_and_configuration}
            % \end{figure}

            \subsubsection{Line Plot of Recall by Question Index and Configuration}
            \begin{figure}[H]
                \centering
                \includegraphics[width=\textwidth]{images_exp2/recall/recall_lineplot_by_question_index_and_configuration.png}
                \caption{Line plot of recall by question index and configuration.}
                \label{fig:recall_lineplot_by_question_index_and_configuration}
            \end{figure}

            \subsubsection{Scatter Plot of Recall vs. Total Time}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/recall/scatter_recall_vs_total_time.png}
                \caption{Scatter plot of recall vs. total time.}
                \label{fig:scatter_recall_vs_total_time}
            \end{figure}

            \subsubsection{Scatter Plot of Recall vs. Total Token Count Input}
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{images_exp2/recall/scatter_recall_vs_total_token_count_input.png}
                \caption{Scatter plot of recall vs. total token count input.}
                \label{fig:scatter_recall_vs_total_token_count_input}
            \end{figure}


        
