\documentclass[msc,english]{coppe}

\usepackage{booktabs}% tabelas mais bonitas
\usepackage{rotating}% rodando coisas, como tabelas
\usepackage{longtable} % tabelas longas
\usepackage{rotating}
\usepackage[most]{tcolorbox} % caixas de texto
\usepackage{amsmath,amssymb}

\usepackage[editing]{coop-writing}
% \usepackage[publish]{coop-writing}

\usepackage{marvosym}
\cwsetcommwarn{\Lightning}
\cwnamedef{xexeo}{red}{X}
\cwnamedef{vitor}{blue}{V}
\usepackage{xurl}
\usepackage{hyperref}

\usepackage{multirow}
\usepackage{changepage} 

\usepackage{adjustbox} 

\usepackage{xcolor, colortbl}
\usepackage{hhline} % For double lines

\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}

\usepackage{lmodern}
\usepackage[T1]{fontenc}

\usepackage{silence}
% \WarningFilter{latex}{Overfull}
\WarningFilter{latex}{Underfull}
\WarningFilter{latex}{empty journal}

\usepackage{float}
\usepackage{pdflscape} % in preamble

\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}

\usepackage{quoting}


\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{keywordgreen}{rgb}{0.1,0.4,0.2}

\lstdefinestyle{mystyle}{ 
    commentstyle=\color{codegray},
    keywordstyle=\color{keywordgreen}\bfseries,
    numberstyle=\color{codegray},
    stringstyle=\color{purple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=10pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

% \usepackage[a4paper,margin=1.5cm]{geometry}


\makelosymbols
\makeloabbreviations

\begin{document}


\title{Comparative Analysis of Single and Multi-Agent Large Language Model Architectures for Domain-Specific Tasks in Well Construction}
\foreigntitle{Comparative Analysis of Single and Multi-Agent Large Language Model Architectures for Domain-Specific Tasks in Well Construction}
\author{Vitor}{BrandÃ£o Sabbagh}
\advisor{Prof.}{Geraldo}{Bonorino XexÃ©o}{D.Sc.}

\examiner{Prof.}{Geraldo Bonorino XexÃ©o}{D.Sc.}
\examiner{Prof.}{Jano Moreira de Souza}{Ph.D.}
\examiner{Prof.}{Arnaldo CÃ¢ndido JÃºnior}{D.Sc.}
\department{PESC}
\date{07}{2025}

\keyword{Large Language Models}
\keyword{Agents}
\keyword{Oil Well Construction}

\maketitle

\frontmatter
\dedication{To Carolina, my life partner.}

\chapter*{Acknowledgements}

To my daughter, Marina, who came into the world just two months ago, bringing a new light and a new purpose to my life. I dedicate every page of this work to you, with the hope of building a bright future for you.

To my parents, Vera and Nicolau, for all the love, unconditional support, and for always believing in me. Your faith in my abilities was the foundation for this achievement.

To my beloved wife, Carolina, my gratitude for all the patience, understanding, and love, especially during the most challenging moments of this journey. Without your support, this work would not have been possible.

To my stepson, Filipe, thank you for the moments of joy and relaxation that helped me maintain balance, especially during our Minecraft adventures. May our friendship continue to grow.

I express my deep gratitude to my mentor, Claudio, for his unwavering support and trust since the beginning of my career in digital transformation. His mentorship was fundamental to my professional development.

To my advisor, XexÃ©o, thank you for the wise guidance, academic rigor, and patience throughout this entire process. Your teachings were crucial to the quality of this work.

I extend my gratitude to the well construction engineering experts, Marcelo Grimberg, Rafael Peralta, and Lorenzo Simonassi, whose expertise and dedication significantly contributed to this research.

I also want to thank Ashish Vaswani. His work on "Attention Is All You Need" paved the way for the Large Language Models that were not only the subject of this dissertation but also an invaluable tool that helped me put ideas into words.

Finally, a special thanks to my colleagues from Petrobras and the Tecgraf Institute. Our daily discussions about Gen-AI were an inexhaustible source of inspiration and knowledge, immensely enriching this dissertation.

\begin{abstract}

    Esta dissertaÃ§Ã£o apresenta a aplicaÃ§Ã£o de modelos de linguagem (LLM) no setor de petrÃ³leo e gÃ¡s, especificamente em tarefas de construÃ§Ã£o e manutenÃ§Ã£o de poÃ§os. O estudo avalia o desempenho de uma arquitetura baseada em LLM de agente Ãºnico e de mÃºltiplos agentes no processamento de diferentes tarefas, oferecendo uma perspectiva comparativa sobre sua precisÃ£o e as implicaÃ§Ãµes de custo de sua implementaÃ§Ã£o. Os resultados indicam que sistemas multiagentes oferecem desempenho melhorado em tarefas de perguntas e respostas, com uma medida de veracidade 28\% maior do que os sistemas de agente Ãºnico, mas a um custo financeiro mais alto. Especificamente, a arquitetura multiagente incorre em custos que sÃ£o, em mÃ©dia, 3,7 vezes maiores do que os da configuraÃ§Ã£o de agente Ãºnico, devido ao aumento do nÃºmero de tokens processados. Por outro lado, os sistemas de agente Ãºnico se destacam em tarefas de texto para SQL (Linguagem de Consulta Estruturada), especialmente ao usar o Transformador PrÃ©-Treinado Generativo 4 (GPT-4), alcanÃ§ando uma pontuaÃ§Ã£o 15\% maior em comparaÃ§Ã£o com as configuraÃ§Ãµes multiagentes, sugerindo que arquiteturas mais simples podem, Ã s vezes, superar a complexidade. A novidade deste trabalho reside em seu exame original dos desafios especÃ­ficos apresentados pelos dados complexos, tÃ©cnicos e nÃ£o estruturados inerentes Ã s operaÃ§Ãµes de construÃ§Ã£o de poÃ§os, contribuindo para o planejamento estratÃ©gico da adoÃ§Ã£o de aplicaÃ§Ãµes de IA generativa, fornecendo uma base para otimizar soluÃ§Ãµes contra parÃ¢metros econÃ´micos e tecnolÃ³gicos.
    
    \end{abstract}
    
\begin{foreignabstract}


    This article explores the application of large language models (LLM) in the oil and gas  sector, specifically within well construction and maintenance tasks. The study evaluates the performances of a single-agent and a multi-agent LLM-based architecture in processing different tasks, offering a comparative perspective on their accuracy and the cost implications of their implementation. The results indicate that multi-agent systems offer improved performance in question and answer tasks, with a truthfulness measure 28\% higher than single-agent systems, but at a higher financial cost. Specifically, the multi-agent architecture incurs costs that are, on average, 3.7 times higher than those of the single-agent setup due to the increased number of tokens processed. Conversely, single-agent systems excel in text-to-SQL (Structured Query Language) tasks, particularly when using Generative Pre-Trained Transformer 4 (GPT-4), achieving a 15\% higher score compared to multi-agent configurations, suggesting that simpler architectures can sometimes outpace complexity. The novelty of this work lies in its original examination of the specific challenges presented by the complex, technical, unstructured data inherent in well construction operations, contributing to strategic planning for adopting generative AI applications, providing a basis for optimizing solutions against economic and technological parameters. 
    
\end{foreignabstract}


\tableofcontents
\listoffigures
\listoftables
\printlosymbols
\printloabbreviations

\mainmatter


\input{part01_introducao.tex}

\input{part02_revisao.tex}

\input{part03_experimento1.tex}

\input{part04_experimento2.tex}

\input{part05_conclusao.tex}

% \input{part05a_conclusao2.tex}

\backmatter
\bibliographystyle{en-coppe-unsrt}
\bibliography{bib}

\appendix

\input{part10_apendice.tex}

\renewcommand{\appendixname}{Appendix}
\appendix

% \input{part11_anexo.tex}

\listofcomments

\end{document}

%% 
%%
%% End of file `example.tex'.

\chapter{Introduction}


% \section{Contextualization}


    % [IA-GEN NA INDUSTRIA] 
    In the dynamic and ever-changing oil and gas (O\&G) industry,
    \xexeo{eu acho que aqui falta alto. TEm um pouco hÃ¡ ver com o uso do "in the dynamic changing of the" nÃ£o ser realmente muito adequado a "oil and gas industry", falta algo, um qualificador a mais (mercado? . Minha IA sugeriu mudar para "In the dynamic and ever-changing oil and gas (O\&G) industry", o que eu achei muito melhor}
    \vitor{feito}
    \abbrev{O\&G}{oil and gas} 
    digital transformation has emerged as a key element to achieve operational efficiency, sustainability, and competitiveness. 
    At the forefront of this transformation are Large Language Models (LLMs), which have the potential to process unstructured queries, map out alternatives, and advise users on possible actions \citep{Kar2023}. 
    We also note the advantage of increased engagement, cooperation, accessibility, and ultimately profitability. 
    These models redefine paradigms in knowledge management and information retrieval and impact a variety of other areas \citep{Eckroth2023}, making it crucial to adopt these technologies to remain competitive.    
    
    % [ESTUDO AUMENTO PRODUTIVIDADE] 
    A study conducted by \citet{Dellacqua2023}, in collaboration with the Boston Consulting Group, shows that in knowledge-intensive tasks, consultants equipped with access to LLMs such as GPT-4 not only completed tasks more efficiently (25.1\% more quickly on average) but also with substantially higher quality, achieving results more than 40\% better compared to those without AI assistance \citep{Dellacqua2023}.
    Increase in productivity of knowledge workers was 12\% on average.    
    A major oil company spent in 2023 \$2.8B with employee compensation \citep{Petrobras2024}.
    A potential increase of 12\% in knowledge workers productivity, given they represent 60\% of all employee, could represent \$204M annual savings in this scenario. 
    \xexeo{Aqui estÃ¡ um problema clÃ¡ssico. VocÃª quer fornecer um dado mas provavelmente nÃ£o quer dizer que Ã© a Petrobras. Mas provavelmente esse dado estÃ¡ em algum relatÃ³rio pÃºblico. NÃ£o consegue ele de algum lugar e aÃ­ pode citar?} 
    \vitor{feito}   
    
    % [AUMENTO DO PIB DEVIDO A GEN AI] 
    Broader economic indicators predict significant transformations due to generative AI (Gen-AI) across various industries.
    A report from Goldman Sachs \citep{Hatzius2023} highlights that Gen-AI is poised to increase global GDP by nearly 7\%, increasing productivity growth by 1.5 percentage points over the next decade. 
    This economic uplift is expected due to AI's ability to automate complex workflows and create new business opportunities, significantly impacting employment and productivity sectors worldwide.


            
    % [PROBLEMA DE DADOS NA INDUSTRIA EM GERAL] 
    Expanding on the broader discussion on data utilization within organizations, an important issue is the challenge of extracting relevant information from extensive databases \citep{Singh2023}. 
    Initially, the challenge of knowing, finding, and accessing data poses a significant obstacle to decision-making processes. 
    Collaborators at O\&G companies often face the intensive task of manually searching large data repositories to find useful information.


    
    % [PROBLEMA DE DADOS NO O\&G] 
    Focusing specifically on the activities of drilling and completion of offshore and onshore wells, a major challenge lies in the inherently complex and technical nature of the data involved, which can be from various types: operations, projects, technologies, supply chains, and others. 
    Inefficiency in leveraging large volumes of unstructured data worsens these challenges, as observed by \citet{Singh2023}. 
    A significant amount of the data generated and collected in this sector is unstructured, ranging from text reports and emails to images and videos of exploration and production activities. 
    Examples include hundreds of daily operational reports from drilling rigs, well execution projects, nonproductive time (NPT) reports, and operational lessons learned documents, as illustrated in Figure \ref{fig:report_example}. 
    As a result, valuable information can remain untapped, and the potential to find insights, informed decision-making, and innovation is significantly compromised.
    \citet{Singh2023} showcases the capabilities and potential of Generative AI-enabled chatbots for the O\&G sector, particularly in enhancing drilling and production analytics to achieve better business results. The author concludes that companies that adopt these technologies in the coming years will see clear advantages.     
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{images/report_example.png}
        \caption{Sample of drilling \& completion learned lesson partial document. (translated from Portuguese)}
        \label{fig:report_example}
    \end{figure}           
    
    However, the deployment of such technologies presents limitations and introduces challenges, including biased data, hallucinations, lack of explainability, and logical reasoning errors, among others \citep{Hadi2023}, which require a balanced approach to harness their potential in a responsible manner.    
    Although previous research has focused mainly on the broader applications of AI in industry, the novelty of our research lies in its original examination of the specific challenges and solutions presented by the complex, technical and unstructured data inherent in O\&G operations. 
    By comparing single- and multi-agent systems, this study fills a knowledge gap, providing empirical insights into the effectiveness of different Gen-AI architectures in a domain where such studies are scarce. 
    
    The adoption of these technologies by a major oil company underscores their potential to revolutionize data analysis and management, presenting an opportunity for deeper exploration and application.

\section{Business Scope Delimitation}

    To contextualize the scope of this study, it is necessary to understand the life cycle of an oil field, which begins with Exploration and progresses to the Development of Production, followed by effective production, and culminates in decommissioning \citep{Badiru2016}. Gen-AI has the potential to impact each of these phases, but the focus of this work lies in the operations of the development and maintenance stages.
            
    Well construction is a highly specialized activity that involves drilling and completion of wells for hydrocarbon extraction \citep{Thomas2004}. In this context, Gen-AI can be applied in various ways. 
    For example, a chatbot could manage knowledge by answering queries about operations and well projects by retrieving information from the organization's databases. 
    Additionally, LLM-based agents could be used in executive project review to ensure that drilling or completion operations comply with the organization's standards and adhere to best operational practices. 
    Moreover, Gen-AI could perform inference in unstructured databases to extract specific information from text reports and obtain structured data. This business scope emphasizes the importance of Gen-AI in the construction and maintenance of wells.

    \subsection{Key Information Sources in Well Engineering} \label{sec:information-sources}

        To fully appreciate the challenges in this domain, it is important to understand the primary data sources that specialists interact with daily. The following sources, used in this research's experiments, exemplify the complex information landscape of well engineering:

        \paragraph{Operational Learned Lessons} During drilling, completion, and workover interventions, documents called Knowledge Items are written by specialists, as depicted in Fig~\ref{fig:report_example}. These can be of four types: Technical Alert, Learned Lesson, Good Practice, and Well Observation. This system serves as a critical tool for knowledge management, considering the large number and variety of specialists involved and well operations performed.

        \paragraph{Operational NPTs (Non-Productive Time)} This data source contains structured records of anomalies that occurred during well interventions, detailing the title, description, location, operation type, responsible sector, rig involved, time lost, and event dates. These data are critical for the industry, as NPTs represent periods when operations are interrupted. The identification and analysis of these events are essential for continuous process improvement, cost reduction, and increased operational efficiency.

        \paragraph{Collaborator Finder} The third data source is a collaborator finder, an important internal tool for consulting and managing employee data. This system allows for the quick identification of employees through information such as name, workplace, and role. The importance of this tool lies in the ability to cross-reference employee data with operational events, enabling a more complete analysis by an intelligent agent.

\section{Objectives}

    This research directly addresses the challenges facing major oil companies. 
    By investigating the comparative advantages and limitations of various Gen-AI architectures, including single and multi-agent systems, for Q\&A and Text-to-SQL tasks, this study aims to identify the most efficient and cost-effective solutions.
    
    The specific objectives of this research are to assess the suitability and effectiveness of multi-agent systems based on LLMs for complex, domain-specific tasks in well engineering, aiming to streamline information access and decision-making.
    
    The study will compare single-agent and multi-agent AI systems in terms of their ability to address well engineering queries. Finally, it will map the potential obstacles and limitations associated with deploying Gen-AI applications.
            
    The insights gained from this research will directly contribute to O\&G companies strategic goals by improving access to well engineering information and automated data analysis tasks. 
    A comprehensive understanding of the challenges and limitations associated with Gen-AI will enable informed decisions about its adoption, maximizing the return on investment. 

    To achieve these objectives, this research was conducted through two distinct experimental phases. The first, carried out in 2024, focused on a foundational comparison between single and multi-agent architectures, which revealed that, although the multi-agent architecture achieved 28\% higher truthfulness in Q\&A tasks, its cost was on average 3.7 times higher. Furthermore, the single-agent architecture proved to be surprisingly more effective in Text-to-SQL tasks. The rapid evolution of generative AI frameworks and models prompted a second, more advanced experiment in 2025. This second phase built upon the initial findings, also employing non-agentic workflows as baseline and a more rigorous, quantitative evaluation methodology to address the challenges identified in the first experiment and automated evaluation based on the concept commonly reffered to as "LLM-as-judge" (\citep{Gu2025}).
    
    \xexeo{Acho que pode aumentar um pouco isso, jÃ¡ falar dos resultados (dissertaÃ§Ã£o tem spoiler), motivar o segundo a partir dos resutlados do primeiro e botar depois das questÃµes de pesquisa (leia o todo), sempre alinhando tudo (questÃ£o de pesquisa - experimento - conclusÃµes}
    \vitor{IncluÃ­do spoiler e ponte do 1o justificando o 2o experimento.}

    <<<<PENDENTE>>>>

    FAZER ALINHAMENTO FINAL (QUESTÃƒO DE PESQUISA - EXPERIMENTO - CONCLUSÃ•ES)
    
    DETALHAMENTO LLM: "BOTAR DEPOIS DAS QUESTÃ•ES DE PESQUISA" E "SEMPRE ALINHANDO TUDO"
    ESTA Ã‰ A "LINHA DE OURO" (GOLDEN THREAD) DE UMA DISSERTAÃ‡ÃƒO. TUDO PRECISA ESTAR CONECTADO. A ESTRUTURA QUE ELE SUGERE CRIA UM FLUXO MUITO LÃ“GICO PARA O LEITOR:
    
    OBJETIVOS: O QUE VOCÃŠ QUER ALCANÃ‡AR (VISÃƒO GERAL).
    
    QUESTÃ•ES DE PESQUISA (A SEREM ADICIONADAS): AS PERGUNTAS ESPECÃFICAS E FOCADAS QUE SUA PESQUISA VAI RESPONDER.
    
    PARÃGRAFO COM "SPOILER" (O QUE ESTAMOS DISCUTINDO): UM RESUMO DE COMO VOCÃŠ RESPONDEU A ESSAS PERGUNTAS E O QUE ENCONTROU.
    
    CONCLUSÃ•ES (NO FINAL DA DISSERTAÃ‡ÃƒO): ONDE VOCÃŠ RESPONDE FORMALMENTE Ã€S QUESTÃ•ES DE PESQUISA, USANDO OS RESULTADOS DETALHADOS DOS EXPERIMENTOS.

    AO COLOCAR O PARÃGRAFO DE "SPOILER" DEPOIS DAS QUESTÃ•ES DE PESQUISA, VOCÃŠ CRIA UMA CONEXÃƒO DIRETA: "PARA RESPONDER A ESTAS PERGUNTAS (QUESTÃƒO 1, QUESTÃƒO 2...), EU CONDUZI ESTES EXPERIMENTOS (EXPERIMENTO 1, EXPERIMENTO 2), QUE ME LEVARAM A ESTAS DESCOBERTAS PRINCIPAIS (RESULTADO A, RESULTADO B)."

    ESSA ESTRUTURA AMARRA TODA A SUA DISSERTAÃ‡ÃƒO, TORNANDO-A COESA, LÃ“GICA E FÃCIL DE ACOMPANHAR.
    <<<<FIM>>>>

    \todo[inline]{Ok, estÃ¡ bom, porÃ©m seria melhor para dissertaÃ§Ã£o se agora vocÃª definisse questÃµes de pesquisa. Essas questÃµes de pesquisa serÃ£o respondidas na conclusÃ£o, a partir do que vocÃª fez. Eu sÃ³ li atÃ© aqui, entÃ£o nÃ£o tenho sugestÃµes fortes agora, mas as questÃµes podem ser coisas coisa: Qual a eficÃ¡cia e eficiÃªncia de LLMs para extrair dados de bases .... Como sistemas single e multi agentes se comparam... Elas podem ser bem melhores e bem mais objetivas e ao longo do texto, se eu detectar alguma , escrevo. A questÃ£o Ã© perguntar aqui no fim da introduÃ§Ã£o e responder na conclusÃ£o, caracterizando a colaboraÃ§Ã£o
    \newline \newline SERÃ FEITO NO FIM}


% \todo[inline]{\textbf{VITOR: CONCLUÃDO} \newline \newline 
%     Ok, faltou a metodologia e tem que ter. Tem que explicar a metodologia genÃ©rica de sua pesquisa. Eu agora nÃ£o sei bem o que se ajeita, mas vejo que talvez possa descrever como DSR ou outro mÃ©todo que implique em aÃ§Ãµes reais em um local. Ã‰ importante notar que vocÃª descreve uma metodologia nos seus experimentos que Ã© um processo de pesquisa (e isso nÃ£o estÃ¡ errado), porÃ©m aqui hÃ¡ deve ser usado um conceito de metodologia mais amplo, ligado a epistemologia. Argumentos em funÃ§Ã£o da DSR: Design Science Research (DSR) [sim, pedi para o ChatGPT, mas eu jÃ¡ meio que concordava]
%     A Design Science Research Ã© a classificaÃ§Ã£o mais apropriada para a metodologia empregada, pelos seguintes motivos:
%     \textbf{Artefato proposto}: A pesquisa visa comparar e melhorar arquiteturas de sistemas baseadas em LLMs aplicadas a tarefas especÃ­ficas de engenharia de poÃ§os, o que se alinha Ã  criaÃ§Ã£o e avaliaÃ§Ã£o de artefatos â€” elemento central da DSR.
%     \textbf{AvaliaÃ§Ã£o empÃ­rica}: Foram realizados dois ciclos de experimentos com diferentes configuraÃ§Ãµes de agente Ãºnico e multiagente, com dados reais da indÃºstria de O\&G e validaÃ§Ã£o por especialistas.
%     \textbf{Ciclos iterativos}: O segundo experimento se baseia explicitamente nos resultados do primeiro, sugerindo um processo iterativo de refinamento de artefatos â€” outro elemento tÃ­pico de DSR.
%     \textbf{RelevÃ¢ncia prÃ¡tica + rigor cientÃ­fico}: O trabalho visa resolver um problema concreto de empresas do setor de petrÃ³leo e gÃ¡s, mas com critÃ©rios e mÃ©tricas de avaliaÃ§Ã£o quantitativas rigorosas.
%     \textbf{ConclusÃ£o}: Mesmo sem ter sido explicitamente rotulada como DSR na dissertaÃ§Ã£o, a estrutura e objetivos da pesquisa indicam fortemente que ela segue os princÃ­pios dessa metodologia.
%     \newline \newline \textbf{VITOR: CONCLUÃDO}}

\section{Research Methodology}
  
    This research follows the Design Science Research (DSR) methodology, a framework particularly suited for studies that develop and evaluate technological artifacts to address specific organizational problems. DSR provides a structured approach for creating innovative solutions while maintaining scientific rigor through empirical validation \citep{hevner2007three}.
    
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dsr-model.png}
    \caption{Main elements of DSR-Model, translated from \citet{Oswald2023}.}
    \label{fig:dsr-model}
    \end{figure}
    
    \subsection{Design Science Research Framework}
    
        The DSR methodology employed in this study consists of four interconnected elements, as illustrated in Figure~\ref{fig:dsr-model}:
        
        \begin{enumerate}
        \item \textbf{Problem in Context}: Identifying and defining a relevant organizational challenge within its specific environment
        \item \textbf{Artifact}: Designing and developing a technological solution to address the identified problem
        \item \textbf{Behavioral Conjectures}: Formulating hypotheses about how the artifact will function and impact the problem space
        \item \textbf{Empirical Evaluation}: Systematically testing the artifact to validate its effectiveness and the underlying conjectures
        \end{enumerate}
        
        This cyclical framework guides both the research design and execution, ensuring that the developed artifacts are not only technically sound but also practically relevant.
    
    \subsection{Application of DSR in This Research} \label{sec:dsr-application}
        
        \subsubsection{Problem in Context}
        
        This study addresses the challenge of efficiently extracting relevant information from extensive technical databases in the oil and gas industry, specifically in well construction and maintenance operations. 
        
        \begin{table}[h]
            \centering
            \caption{Characteristics of the Problem Context}
            \begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
            \hline
            \textbf{Challenge Aspect} & \textbf{Description} \\
            \hline
            Data Structure & Large volumes of unstructured data (operational reports, lessons learned documents, NPT reports) \\
            \hline
            Technical Complexity & Domain-specific terminology and complex relationships \\
            \hline
            Business Impact & Significant potential economic impact from improved knowledge access \\
            \hline
            \end{tabular}
            \label{tab:problem-context}
        \end{table}
        
        \subsubsection{Artifacts}
        
        Two primary artifacts were designed and implemented, illustrated in Figure~\ref{fig:artifacts}, using state-of-the-art language models (GPT-3.5-turbo and GPT-4) and integrated with domain-specific knowledge bases through various retrieval mechanisms.
        
        \begin{figure}[h]
        \centering
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\begin{tabular}{c}
            \textbf{Single-Agent LLM System} \\
            \small A centralized architecture where one \\
            \small language model agent handles the entire \\
            \small question-answering process with \\
            \small access to multiple tools
            \end{tabular}}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \fbox{\begin{tabular}{c}
            \textbf{Multi-Agent LLM System} \\
            \small A collaborative architecture where \\
            \small multiple specialized agents work together \\
            \small under coordination to process queries \\
            \small  
            \end{tabular}}
        \end{minipage}
        \caption{Primary Artifacts Developed in This Research}
        \label{fig:artifacts}
        \end{figure}
        
        
        \subsubsection{Behavioral Conjectures}
        
        The research was guided by several key conjectures:
        
        \begin{tcolorbox}[colback=gray!10, colframe=gray!40, title=Key Research Conjectures]
            \begin{itemize}
            \item Multi-agent systems will demonstrate higher accuracy in complex technical queries due to their ability to distribute cognitive load and specialize in different aspects of the problem
            \item The performance advantages of multi-agent systems will vary by task type (Q\&A vs. Text-to-SQL)
            \item More advanced language models will yield better performance but at significantly higher LLM financial costs
            \item The economic efficiency (performance-to-cost ratio) will be a critical factor in determining practical implementation viability
            \end{itemize}
        \end{tcolorbox}
        
        \subsubsection{Empirical Evaluation}
        
        The evaluation was conducted through two distinct experimental phases (summarized in Table~\ref{tab:experiments}), allowing for iterative refinement of both the artifacts and the evaluation methodology, addressing limitations identified in the first experiment while adapting to the rapid evolution of language model capabilities.
        
        \begin{table}[h]
        \centering
        \caption{Comparison of Experimental Phases}
        \begin{tabular}{|p{0.15\textwidth}|p{0.38\textwidth}|p{0.38\textwidth}|}
        \hline
        \textbf{Aspect} & \textbf{First Experiment (2024)} & \textbf{Second Experiment (2025)} \\
        \hline
        Focus & Comparative analysis of single and multi-agent architectures & Extended evaluation incorporating non-agentic workflows as baseline \\
        \hline
        Evaluation Methods & Expert validation by domain specialists & Automated assessment using "LLM-as-judge" approach \\
        \hline
        Metrics & Truthfulness, performance, and LLM cost & Precision, recall, and F1-score \\
        \hline
        Outcomes & Identification of key challenges and limitations & More rigorous quantitative evaluation methodology \\
        \hline
        \end{tabular}
        \label{tab:experiments}
        \end{table}

    
    % \subsection{Research Quality and Validity}
    
    %     To ensure research quality and validity, several measures were implemented, as shown in Figure~\ref{fig:research-quality}. 
    %     By adhering to the DSR methodology, this research maintains a balance between practical utility and scientific rigor, producing artifacts that address real organizational needs while contributing to the theoretical understanding of multi-agent LLM systems in specialized technical domains.
        
    %     \begin{figure}[h]
    %         \centering
    %         \begin{tikzpicture}
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (a) at (0,0) {\textbf{Triangulation}\\\small Using multiple data sources, evaluation methods, and expert perspectives};
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (b) at (6,0) {\textbf{Reproducibility}\\\small Detailed documentation of experimental procedures, prompts, and configurations};
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (c) at (0,-4) {\textbf{Practical Relevance}\\\small Direct application to real-world operational challenges};
    %         \node[draw, rounded corners, fill=blue!10, text width=0.3\textwidth, align=center] (d) at (6,-4) {\textbf{Theoretical Contribution}\\\small Insights into comparative advantages of different agent architectures};
    %         \draw[->] (a) -- (b);
    %         \draw[->] (b) -- (d);
    %         \draw[->] (a) -- (c);
    %         \draw[->] (c) -- (d);
    %         \end{tikzpicture}
    %         \caption{Research Quality Assurance Framework}
    %         \label{fig:research-quality}
    %     \end{figure}
        
        

\section{Thesis Structure}



    ****SERÃ FEITO POR ÃšLTIMO****
   
\chapter{Literature Review} 

% \todo[inline]{Todo capÃ­tulo deve ter uma introduÃ§Ã£o explanatÃ³ria. "This chapter describes"}

    This chapter provides a comprehensive literature review of the key technologies and concepts that form the foundation of this dissertation. It begins with an overview of the applications of Artificial Intelligence (AI) in the Exploration and Production (E\&P) industry. The focus then narrows to Large Language Models (LLMs), discussing their architecture and impact. Subsequently, the chapter delves into the Retrieval-Augmented Generation (RAG) technique, which enhances LLMs with external knowledge. It also explores the use of single and multi-agent setups. Finally, the chapter concludes by examining the 'LLM-as-judge' paradigm for evaluating the performance of generative models.


    \section{AI in the Exploration and Production (E\&P) industry}

        The use of AI in the Exploration and Production (E\&P) industry has been extensive. 
        In the last decades the majority of AI applications in the industry involved data mining and neural networks \citep{Bravo2014}. 
        An example is the work by \citep{Gudala2021} on optimization of the properties of the heavy oil flow, through the use of neural networks to optimize flow-influencing parameters.
        Another development was a deep learning workflow proposed by \citep{Gohari2024}, with the generation of synthetic graphic well logs through the application of transfer learning. 
        These developments illustrate the potential of AI to improve processes and the accuracy and efficiency of data analysis \citep{Rahmani2021}.
    
        Natural Language Processing (NLP) stands at the intersection of computer science and linguistics, representing a domain within artificial intelligence aimed at enabling computers to understand and process human language in a way that is both meaningful and effective \citep{Liddy2001}. 
        This field integrates a diverse range of computational techniques to analyze and represent text at various levels of linguistic detail, striving to emulate human-like language understanding. 
        As an active area of research, traditionally NLP employs multiple layers of language analysis, each contributing uniquely to the interpretation and generation of language, which finds practical applications in various sectors \citep{Liddy2001}.      
        In the O\&G industry, the management of unstructured data, such as texts, images, and documents, is crucial, with Natural Language Processing (NLP) and Machine Learning playing key roles.
        Research by \citet{Antoniak2016} and \citet{Castineira2018} has explored the use of NLP to analyze risks and drilling reports.           
    
    \section{Natural Language Processing} \label{sec:nlp-review}

        NLP (Natural Language Processing) is a broad field that covers various tasks to enable computers to process and understand human language. These tasks, which represent specific problems or applications, have been the focus of research for decades, predating the recent surge in Large Language Models. They range from fundamental challenges like part-of-speech tagging to complex applications like machine translation. This section explores two tasks particularly relevant to this dissertation: Question Answering (Q\&A) and Text-to-SQL, both of which have been significantly advanced by recent developments in the field.

        \subsection{Q\&A tasks}     

            Question and Answer (Q\&A) represent a method to facilitate knowledge transfer between individuals within organizations \citep{Iske2005}. 
            \xexeo{As tasks vem antes das LLMs, elas sempre existiram como problemas da Ã¡rea de NLP. Inclusive acho que na seÃ§Ã£o de NLP vocÃª pode fazer um parÃ¡grafo sobre a existÃªncia de vÃ¡rias tasks e usar essas como subseÃ§Ãµes}
            \vitor{Feito}
            Conceptually, Q\&A systems are designed to connect individuals who possess specific knowledge with those seeking that knowledge through a structured question-and-answer format. 
            The role of Q\&A in the documentation landscape, as exemplified by platforms such as Stack Overflow, highlights their significance in technical disciplines \citep{Treude2011}. 
            This understanding can guide organizations in making more informed decisions about implementing such systems to enhance knowledge transfer and organizational learning \citep{Iske2005}.

        \subsection{Text-to-SQL tasks} 

            Text-to-SQL tasks in the context of artificial intelligence involve the automatic translation of natural language questions or commands into structured SQL (Structured Query Language) queries \citep{Qin2022}. This is an important area in natural language processing (NLP), allowing users to interact with databases using plain language rather than needing to know how to write complex SQL queries.         
                
            The arrival of advanced language models like GPT-3 and GPT-4 \citep{OpenAImodels} has marked a significant leap in Text-to-SQL applications \citep{Singh2023}, demonstrating remarkable capabilities in handling these tasks. This can be attributed to their extensive training on diverse datasets \citep{Deng2021}, which include not only large amounts of text but also structured data like tables and code, enabling the model to understand the intricate relationships between language and data structures. The study by \citep{Deng2023} introduces a pre-training framework for text to SQL translation, emphasizing the alignment between text and tables in Text-to-SQL tasks.






    \section{Intelligent Agents}         
        % \xexeo{Agentes existem antes das LLMs, logo essa seÃ§Ã£o deve vir antes, inclusive jÃ¡ transformei em seÃ§Ã£o}
        % \vitor{feito}
        According to \citet{Russell2020}, an agent is something that performs actions. When it comes to computerized agents (in our case, AI-based), these agents are expected to do more: operate autonomously, perceive the environment, persist over time, adapt to changes, create, and strive to achieve goals.
        The agent program implements the agent function.
        There is a variety of basic agent program designs that vary in efficiency, compactness, and flexibility. The appropriate design of the agent program depends on the nature of the environment. In this work, a goal-based agent design was implemented, which acts to achieve defined goals \citep{Russell2020}.
        Other possible types include simple reflex agents, which directly respond to perceptions, while model-based reflex agents maintain an internal state to track aspects of the world that are not evident in the current perception. Finally, there are utility-based agents, which try to maximize their expected "happiness" \citep{Russell2020}.


        \subsection{Multi-Agent Systems}

            A Multi-Agent System (MAS) extends the concept of a single agent to a collection of agents that interact within a shared environment \citep{Gokulan2010}. A MAS is defined as a loosely coupled network of autonomous problem-solving entities that collaborate to find solutions to problems that are beyond the individual capabilities or knowledge of any single entity \citep{FloresMendez1999}. 
            % These systems are characterized by having no global system control, decentralized data, and asynchronous computation, with each agent possessing only incomplete information or capabilities to solve the overall problem \citep{FloresMendez1999}. This distributed nature provides several advantages, including increased speed and efficiency through parallel computation, enhanced reliability and robustness due to graceful degradation if an agent fails, and greater scalability and flexibility, as new agents can be added to the system when necessary \citep{Gokulan2010}.
            
            % Despite these benefits, 
            % The design of a MAS presents significant challenges, with coordination being the central issue \citep{Gokulan2010}. In a multi-agent environment, the action of one agent can modify the environment for others, necessitating that each agent attempts to predict the actions of its neighbors to make optimal, goal-directed decisions. This creates a complex dynamic where coordination is essential to prevent chaos, manage conflicts arising from limited individual perspectives, and meet global constraints \citep{Gokulan2010}. Effective interaction is therefore critical and typically requires mechanisms for agents to find each other, such as facilitators or brokers, and the use of a common agent communication language (ACL) and shared ontologies to ensure mutual understanding \citep{FloresMendez1999}.
            
            The structure of a MAS can vary, with different organizational paradigms such as hierarchical structures or coalitions being employed depending on the application \citep{Gokulan2010}. A practical example of a MAS architecture is demonstrated in power system restoration, where a system can be composed of multiple "bus agents" and a single "facilitator agent" \citep{Nagata2002}. In this setup, each bus agent works to restore its local area by negotiating with neighboring agents based on locally available information, while the facilitator agent manages the overall decision-making process, showcasing how a collection of agents with simple, local strategies can cooperate to achieve a complex, global goal \citep{Nagata2002}.



    \section{Large Language Models}         

        Large Language Models (LLMs) are advanced neural network-based models designed to understand and generate human-like text. 
        They leverage the Transformer architecture introduced in the seminal paper \enquote{Attention is All You Need} by \citet{Vaswani2017}. 
        This architecture relies on self-attention mechanisms, allowing the model to weigh the importance of different words in a sentence effectively. 

        The emergence of LLMs has made it possible to understand and produce textual information. 
        These systems are expected to revolutionize various industries by supporting complex decision-making processes. GPT models \citep{OpenAI2023}, in particular, take advantage of its vast training data to provide human-like responses \citep{Mosser2024}, which can be highly beneficial in contexts requiring natural language understanding and generation. The exponential growth in the size and capability of LLMs in recent years has been remarkable. Models like OpenAI's GPT series have shown significant advancements, moving from millions to hundreds of billions of parameters, which gives them increasingly sophisticated natural language understanding and generation. This advancement is illustrated in Figure~\ref{fig:llm_evolution}. For new models (released after jan/2025), including OpenAI's o3 series and GPT-4.5, Anthropic's Claude 3.7 and 4, and Google's Gemini 2.5 Pro, the exact parameter counts have not been publicly disclosed. 

        \xexeo{Acho que aqui merecia um grÃ¡fico do crescimento do tamanho das LLMs e um parÃ¡grafo sobre esse crescimento}
        \vitor{Feito}

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\textwidth]{images/llm_evolution.png}
            \caption{The evolution of LLMs.}
            \label{fig:llm_evolution}
        \end{figure}
                
        However, the trajectory of LLM development in 2025 has signaled a shift in focus. While previous advancements were often marked by an exponential increase in parameter counts, the latest generation of models emphasizes sophisticated reasoning capabilities over sheer size. 
        This move away from parameter size as the primary metric of progress underscores a new trend: enhancing the models' ability to perform complex, multi-step reasoning. 
        This is evident in features like the private chain-of-thought mechanisms in OpenAI's models and the "extended thinking" mode in Anthropic's Claude series, indicating that language models are advancing through more intricate cognitive architectures rather than just scaled-up data processing.

        As highlighted by \citet{Singh2023}, the integration of LLM-based solutions, such as conversational chatbots, offers an approach to optimizing operations across various business segments, including drilling, completion, and production.
        \citet{Singh2023} uses LLMs models to extract, analyze, and interpret datasets, enabling generation of insights and recommendations. 

        Despite its widespread impact, language models are not without its limitations. 
        In many industry-specific applications, the critical information required is often proprietary, not shared with third parties, and thus absent from the training data of these LLMs \citep{Mosser2024}. 
        This gap means that GPT models might not have access to the most up-to-date or sensitive information needed for certain tasks. 
        Moreover, due to their probabilistic nature, LLMs can experience hallucinations, producing confident yet incorrect or nonsensical responses based on user input \citep{OpenAI2023}. 
    
    
        \subsection{LLM applications}

        \xexeo{Precisa de um texto aqui}
        \vitor{Feito}

            % LLM applications have witnessed a dramatic surge in development and adoption, reshaping the landscape of AI. 
            % This growth is fueled by continuous advancements in model architectures, training techniques, and the availability of vast datasets. 
            The proliferation of LLMs has led to a diverse array of applications that leverage their ability to understand, generate, and process human language.

            The expansion of the LLM application ecosystem is evident in the significant market growth projections. For instance, one report projects the global LLM market to grow from \$5.62 billion in 2024 to \$35.43 billion by 2030, with a compound annual growth rate (CAGR) of 36.9\% \citep{GrandViewResearch2025}. This rapid expansion is indicative of the immense value and potential that organizations across industries see in these technologies. The applications themselves are becoming increasingly sophisticated, evolving from simple text generation to complex, multimodal systems capable of processing and integrating text, images, and other data formats \citep{Kaddour2023}.
            
            The spectrum of LLM-based applications is broad and continually expanding. Early applications focused on tasks such as text summarization, translation, and sentiment analysis. However, the current generation of LLMs powers a much wider range of tools. These can be broadly categorized into several key areas. Conversational AI, in the form of advanced chatbots and virtual assistants, represents a significant segment of the market, enhancing customer service and user engagement \citep{GrandViewResearch2025}. Content creation is another major application area, where LLMs are employed to generate a variety of materials, from marketing copy and social media posts to technical documentation and even creative writing \citep{V7Labs2025}.            
            
            Furthermore, LLMs are being integrated into more specialized and high-stakes domains. In the legal field, they assist with tasks like contract analysis and legal research. The financial sector utilizes them for fraud detection and market analysis \citep{V7Labs2025}. In software development, LLM-powered tools for code generation and debugging are becoming increasingly prevalent, accelerating development cycles and improving programmer productivity. A key innovation driving the utility of these applications is the advent of techniques like Retrieval-Augmented Generation (RAG), which allows LLMs to retrieve and incorporate information from external knowledge bases, thereby improving the accuracy and relevance of their outputs \citep{KeywordsAI2025}. The ongoing development of multimodal LLMs is further pushing the boundaries of what is possible, enabling applications that can understand and reason about the world in a more holistic manner \citep{Kaddour2023}.
        
        \subsection{Retrieval-Augmented Generation (RAG)} 

            Retrieval-Augmented Generation (RAG) technique combines LLMs with information retrieval to generate accurate and up-to-date responses, as introduced by \citet{Lewis2020}. 
            \xexeo{Aqui merece um desenho ilustrativo, atÃ© para quebrar tanto texto}
            \vitor{Feito}
            It employs a search in a database to find relevant information, overcoming the inherent limitations of LLMs that rely solely on the prior knowledge embedded in the language model during the training phase. 
            With the ongoing evolution of information retrieval, which has moved from term-based methods to more semantic approaches leveraging deep learning and large datasets to tackle more complex challenges.
            
            A RAG consists of two main components: a retriever and a generator, as illustrated in Figure~\ref{fig:rag_diagram}. The retriever is responsible for finding relevant information from a knowledge base, and the generator uses that information to create a human-like response. 
            
            \begin{figure}[h!]
                \centering
                \includegraphics[width=0.4\textwidth]{images/rag_diagram_vertical.png}
                \caption{A diagram illustrating the RAG process.}
                \label{fig:rag_diagram}
            \end{figure}         

            As elucidated by \citet{Lewis2020}, RAG unites the strengths of pre-trained parametric and non-parametric memory, using a dense vector index and a semantic retriever. 
            As demonstrated by \citet{Li2022} in their analysis, RAG is surpassing traditional generative models in terms of performance across a variety of tasks. The study provides a detailed survey on this topic, emphasizing the fundamental concepts and its applicability in specific contexts.

            New tools have been developed to facilitate the implementation of RAG solutions. \citet{Liu2023} present a toolkit that integrates augmented retrieval techniques into LLMs, including modules for query rewriting, document retrieval, passage extraction, response generation, and fact-checking, enabling the creation of more factual and specific responses. The recent study by \citet{Zhao2023} extends this horizon by examining the incorporation of multimodal knowledge into generative models, exploring the integration of diverse external sources such as images, code, tables, graphs, and audio, to enhance the grounding context and improve usability. It also explores potential future trajectories in this emerging field, marking a relevant contribution to the evolving narrative of RAG and its applications.

            
        \subsection{Multi-Agent Setup} 

            \xexeo{Isso aqui seria uma seÃ§Ã£o de multi agentes dentro da LLM, mas vocÃª precisa escrever pelo menos um parÃ¡grafo de multi agentes gerais na seÃ§Ã£o agentes}      
            \vitor{Feito. Inclui uma subseÃ§Ã£o sobre MAS.}

            As demonstrated by \citet{xi2023rise}, the pursuit of Artificial General Intelligence (AGI) has significantly benefited from the development of LLM-based agents, capable of sensing, decision-making, and acting across diverse scenarios.  
            His study outline a foundational framework for such agents, consisting of brain, perception, and action components, which can be customized for various applications including single-agent scenarios, multi-agent systems, and human-agent collaboration . 
            The comprehensive survey underscores the crucial role of LLMs in moving towards AGI, suggesting a promising horizon for operational efficiency and decision-making processes in complex organizational settings \citep{xi2023rise}.

            \citet{Li2024} demonstrated that, through a sampling and voting method, the performance of LLMs scales with the number of instantiated agents.
            Another open-source framework is AutoGen \citep{Wu2023}, that enables the creation of LLM multi-agent applications, allowing for customization across various modes including. It supports diverse applications in fields such as mathematics, coding, and operations research, demonstrating its effectiveness through empirical studies \citep{Wu2023}.

            
        \section{Evaluation} \label{sec:evaluation-review}

            \subsection{Truthfulness}

                In the evaluation of RAG systems, ensuring the truthfulness of the generated output is a primary concern. \citet{Lin2022} introduces a framework for this purpose. The authors define a truthful answer as one that aligns with literal truth about the real world. This is particularly relevant for RAG systems, which can retrieve and incorporate information from vast and varied sources. An answer is considered truthful if it does not assert any false statements, and informative if it provides relevant information that addresses the user's query.
                
                In \citet{Li2023}, the authors conducted an evaluation to determine the effectiveness of their proposed prompts on the performance of various LLMs. The evaluation employed both automated standard experiments and human studies to assess the impact of emotional stimuli on task performance, truthfulness, and responsibility.

                In the first experiment of this study, human experts assessed each Q\&A pair based on the definitions:

                \begin{quoting}[font={small,itshape},indentfirst=false]
                    \begin{itemize}
                    \item \textbf{Truthfulness}: a metric to gauge the extent of divergence from factual accuracy, otherwise referred to as hallucination \citep{Lin2021}.
                        \subitem 1=â€œThe response promulgates incorrect information, detrimentally influencing the ultimate interpretationâ€
                        \subitem 2=â€œA segment of the response deviates from factual accuracy; however,this deviation does not materially affect the ultimate interpretationâ€
                        \subitem 3=â€œThe response predominantly adheres to factual accuracy, with potential for minor discrepancies that do not substantially influence the final interpretationâ€
                        \subitem 4=â€œThe response is largely in consonance with factual evidence, albeit with insignificant deviations that remain inconsequential to the final interpretationâ€
                        \subitem 5=â€œThe response is in meticulous alignment with the facts, exhibiting no deviationsâ€
                                
                    \item \textbf{Performance}: encompasses the overall quality of responses, considering linguistic coherence, logical reasoning, diversity, and the presence of corroborative evidence.
                        \subitem 1 = â€œThe response fails to address the question adequatelyâ€
                        \subitem 2 =â€œThe response addresses the question; however, its linguistic articulation is sub-optimal, and the logical structure is ambiguousâ€
                        \subitem 3 = â€œThe response sufficiently addresses the question, demonstrating clear logical coherenceâ€
                        \subitem 4 = â€œBeyond merely addressing the question, the response exhibits superior linguistic clarity and robust logical reasoningâ€
                        \subitem 5 = â€œThe response adeptly addresses the question, characterized by proficient linguistic expression, lucid logic, and bolstered by illustrative examplesâ€\citep{Lin2021}.         
                    \end{itemize}
                \end{quoting}

            \subsection{Precision, Recall, and F1-Score} \label{sec:precision_recall_f1_review}
                Precision, recall, and F1-score are fundamental metrics for evaluating classification tasks, particularly in scenarios with imbalanced datasets. These metrics provide a more nuanced understanding of a model's performance than accuracy alone.

                \textbf{Precision} measures the accuracy of positive predictions. It is the ratio of correctly predicted positive observations to the total predicted positive observations. A high precision relates to a low false positive rate.
                \begin{equation}
                    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                    \label{eq:precision}
                \end{equation}

                \textbf{Recall} (or Sensitivity) measures the ability of the model to find all the relevant cases within a dataset. It is the ratio of correctly predicted positive observations to all observations in the actual class.
                \begin{equation}
                    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                    \label{eq:recall}
                \end{equation}

                The \textbf{F1-score} is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is the harmonic mean of the two and is a good way to show that a model has a good performance on both metrics.
                \begin{equation}
                    \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                    \label{eq:f1-score}
                \end{equation}


        \subsection{LLM-as-judge}

            \xexeo{Ã’k, essa seÃ§Ã£o eu nÃ£o gostei muito, apesar de nÃ£o ter nenhum erro. Primeiro tem que fazer uma seÃ§Ã£o de avaliaÃ§Ã£o com as medidas, onde devem estar todas as medidas que vocÃª usar na dissertaÃ§Ã£o (nÃ£o li ainda, estou indo na ordem). AÃ­ entÃ£o vocÃª pode falar disso, mas apenas se usou}
            \vitor{Enxuguei esta seÃ§Ã£o e fiz a inclusÃ£o das demais mÃ©tricas utilizadas antes desta seÃ§Ã£o. Veja se estÃ¡ melhor.}

            The LLM-as-Judge paradigm represents a significant shift in the evaluation of NLP systems in general, using a language model as a scalable proxy for human evaluators (\citep{li2024llmsasjudgescomprehensivesurveyllmbased}). 
            This approach was developed to overcome the semantic shallowness of traditional metrics like BLEU or ROUGE and the logistical challenges of extensive human annotation (\citep{Zheng2023}). 
            By providing a "judge" LLM with a clear rubric and context, it can perform assessments of qualities like coherence, relevance, and factual accuracy (\citep{li2024llmsasjudgescomprehensivesurveyllmbased}).
            This method has proven effective for complex, open-ended tasks where simple string matching is insufficient, with models like GPT-4 demonstrating over 80\% agreement with human preferences in benchmarking studies \citep{Zheng2023}.

            For evaluating Retrieval-Augmented Generation (RAG) systems, the LLM-as-Judge framework can be adapted to produce structured, quantitative assessments. 
            In this application, the judge LLM is tasked with comparing the RAG-generated answer against a ground-truth dataset.
            By using a crafted prompt that defines the classification criteria, the judge can systematically categorize each output into classes such as True Positive (TP) (factually consistent with the ground truth), False Positive (FP) (introduces unsupported information), True Negative (TN) (a correct refusal to answer), or False Negative (FN) (missing relevant information). This approach moves beyond subjective scoring towards a more objective evaluation. The prompt used in this work is presented in the code in Appendix~\ref{code:llm-judge}.

            The advantage of this methodology is its ability to translate qualitative judgments directly into a confusion matrix, allowing the calculation of standard metrics such as precision (Equation~\ref{eq:precision}), recall (Equation~\ref{eq:recall}), and F1-score (Equation~\ref{eq:f1-score}). This process establishes a replicable pipeline for benchmarking the factual accuracy of a RAG system at scale. While it is important to acknowledge the potential for inherent biases in LLM judges (\citep{Gu2025}), studies show high correlation with human-expert evaluations (\citep{li2024llmsasjudgescomprehensivesurveyllmbased}), making it a useful tool for iterative development and system comparison.

\todo[inline]{Como vocÃª criou um dataset de teste para uma tarefa, seria bom falar disso. Mas essa Ã© uma coisa adicional a outras mudanÃ§as que pedi, seria bom, mas se nÃ£o der tempo, nÃ£o deu.
\newline \newline VITOR: O DATASET ENTRARIA NO CAP. 3 E 4 OU AQUI MESMO?}

\xexeo{VocÃª usou no prmeiro experimento outras mÃ©tricas, tem que descreve-las aqui: Truthfulness, Performance, LLMCost}
\vitor{Feito}

\chapter{First Experimental Evaluation Cycle}

    \xexeo{Todo capÃ­tulo deve ter uma introduÃ§Ã£o explanatÃ³ria. "This chapter describes"}
    \vitor{Feito.}

    % This chapter describes the first experimental cycle of this research, structured according to the DSR methodology, as detailed in Section~\ref{sec:dsr-application}. It situates the experiment within the established \textbf{context} of knowledge management in the well construction and maintenance domain of a major oil company. The core \textbf{problem} this experiment addresses is the need for an effective mechanism to query complex technical and operational information within large volumes of unstructured data. As a solution, this experiment proposes and implements two distinct \textbf{artifacts}: a single-agent system and a multi-agent architecture, both designed to leverage LLMs for responding to specialized user queries. Finally, the chapter details the \textbf{evaluation} of these artifacts, outlining the methodology, the dataset creation process, and the metrics (Truthfulness, Performance, and LLM Cost) used to assess their capabilities and limitations.

    \xexeo{Acho que pode atÃ© ser Primeiro Ciclo, ou pode ter um nome como ``Efetividada das LLMs na SoluÃ§Ã£o..''  vai ter que quebrar esse capÃ­tulo, que tem muita informaÃ§Ã£o nos conceitos da DSR: no mÃ­nimo nos quatro principais: Contexto, Problema, Artefato, AvaliaÃ§Ã£o. Grande parte do contexto e problema jÃ¡ devem ser descritos no capitulo que eu pedi para criar e aqui sÃ³ faz referÃªncia.}
    \vitor{O CapÃ­tulo foi refatorado para ficar alinhado com a DSR.}

    \xexeo{Ok, vocÃª foi direto para o experimento mas nÃ£o disse o que ia fazer. Aqui exatamente cabe o quadro da DSR que eu te mandei: qual o contexto, qual o problema, qual a suposiÃ§Ã£o (de utilidade ou de mundanÃ§a de contexto), qual os quadro teÃ³ricos, qual o(s) artefato(s) proposto(s) e como serÃ£o avaliados, vai fechar muito bem.}
    \vitor{O CapÃ­tulo foi refatorado para ficar alinhado com a DSR.}

    
    \xexeo{isso aqui Ã© a validaÃ§Ã£o, mas qual Ã© o problema, qual a proposta, sÃ£o essas informaÃ§Ãµes que faltam para ficar bem organizado} 
    \vitor{O CapÃ­tulo foi refatorado para ficar alinhado com a DSR.}

    This chapter describes the first experimental cycle of this research, as introduced in Section~\ref{sec:dsr-application}, conducted to investigate the effectiveness of different LLM based agent architectures. The primary objective is to address complex, domain-specific queries within the field of well construction and maintenance. This initial cycle serves as a foundational study, comparing single-agent and multi-agent systems to generate empirical insights into their performance, cost, and inherent limitations. The findings from this cycle will inform the more advanced, quantitative evaluation performed in the second experiment.

    Following the principles of DSR, this chapter is structured to clearly present the research components. We will begin by defining the business context and the specific problem this experiment aims to solve. Subsequently, we will describe the design of the proposed technological solutions, referred to as artifacts. Finally, we will detail the evaluation methodology, including the process for data set creation, the metrics used for assessment, and a thorough analysis of the results.
    
    \section{Design Science Research Framework}
    
        To provide a clear and organized structure for this experiment, we adopt the DSR framework. The key components of this research cycle are outlined as follows:

        \begin{description}
            \item[Context] The operational environment of the well construction department within a major oil company, where efficient access to technical knowledge is critical.

            \item[Problem] The challenge faced by engineers and specialists in effectively querying and retrieving accurate information from vast, unstructured, and domain-specific knowledge bases (e.g., operational reports, lessons learned).

            \item[Supposition] Our core supposition is that LLM-based agent systems can improve the efficiency and accuracy of information retrieval for specialized tasks, but that the choice of architecture (single-agent vs. multi-agent) will have a measurable impact on performance and cost.

            \item[Theoretical Frameworks] This work is grounded in the theories of Intelligent Agents, Retrieval-Augmented Generation (RAG), and multi-agent systems, as detailed in the Literature Review.

            \item[Proposed Artifacts] Two distinct LLM-based agent systems are proposed and built:
            \begin{itemize}
                \item A Single-Agent Architecture.
                \item A Multi-Agent Architecture.
            \end{itemize}

            \item[Evaluation] The artifacts are evaluated by a panel of domain experts who assess the quality of their responses to a curated set of real-world queries. The evaluation is based on predefined metrics for truthfulness, performance, and cost.
        \end{description}

    \section{Context and Problem Statement}

        \subsection{Context}

            As established in the Introduction, this research is situated within the oil and gas industry, a sector characterized by complex, expensive operations. This experiment was carried out specifically within the well construction department of a major oil company. In this environment, engineers and technical staff frequently need to access specialized information from a variety of internal data sources, including operational reports, learned lessons, and safety alerts. The efficiency and accuracy of this information retrieval process directly impact operational decision-making, safety, and cost-effectiveness.

            The set of queries used to test the systems
            % , listed in Appendix~\ref{app:dataset}, 
            provides a concrete exemplification of the problem space.

        \subsection{Problem}

            The central problem addressed in this experiment is the inefficiency of technical knowledge management and data analysis in the well construction domain. Specialists often struggle to find precise answers to their queries, which are typically buried in large volumes of unstructured or semi-structured documents. This leads to time-consuming manual searches and the risk of overlooking critical information.

            This experiment investigates two primary task categories that exemplify this problem, as described in Section~\ref{sec:nlp-review} and summarized here:
            
            \begin{itemize}
                \item \textbf{Q\&A Tasks:} Require the system to answer complex technical questions by synthesizing information from documents. For example: ``How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures?''
                \item \textbf{Text-to-SQL Tasks:} Require the system to query structured databases using natural language. For example: ``What was the longest-lasting NPT on rig number 05?''
            \end{itemize}
            
            The set of queries used to test the systems, listed in Appendix A, provides a concrete exemplification of the problem space.

    \section{Proposed Artifacts}

            To address the problem, we designed, built, and tested two distinct artifacts: a single-agent solution and a multi-agent solution. Both are goal-based agents designed to accurately respond to user queries by leveraging a suite of tools.

        \subsection{Single-Agent Architecture.}

            In this work, a goal-based agent \citep{Russell2020} was implemented with the goal of accurately responding to various queries. 
            The agent operates within an environment equipped with multiple tools for task-specific operations, as shown in Figure~\ref{fig:agent_environment}, and interfaces with users to receive queries.
            
            \begin{figure}[h]
                \centering
                \includegraphics[width=0.75\textwidth]{images/agent_environment_4.png}
                \caption{Schematic of the LLM-based agent interacting with an environment containing tools for task-specific operations, and the Human Agent interface for user interaction and feedback.}
                \label{fig:agent_environment}
            \end{figure}           
            
            Initially, a configuration of agents was implemented as described in Figure~\ref{fig:agent_config_1} using AutoGen Framework \citep{Wu2023} with an architecture that allows information retrieval and user interaction. This system consists of two agentic setups:

            \begin{figure}[h]
                \centering
                \includegraphics[width=.5\textwidth]{images/agent_config_1.png}
                \caption{Chat setup with one User Proxy \citep{Wu2023} and one Assistant.}
                \label{fig:agent_config_1}
            \end{figure}

            \begin{itemize}        
                        
                \item \textbf{User Proxy:} represents the interface with the user and with tools to access external databases. The modular nature of the tools allows the User Proxy to be customized and expanded based on the variety of data sources and the specific requirements of the application domain.

                \item \textbf{Agent:} powered by LLMs such as GPT-4 and GPT-3 (the specific model is configurable), is the analytical engine of the system. This agent interprets the queries received from the User Proxy and formulates responses.
                                    
            \end{itemize}

            
            For each question in the data set, the agent's decision-making process is executed as described in Figure~\ref{fig:diagrama_agente_1}, initially selecting the appropriate tool to respond to a query and, finally, compiling the retrieved information to provide a final answer.

            \begin{figure}[h]
                \centering
                \includegraphics[width=0.75\textwidth]{images/agent_diagram_1.png}
                \caption{Decision process of the agent.}
                \label{fig:diagrama_agente_1}
            \end{figure}

        \subsection{Multi-Agent Architecture}

            The second artifact is a multi-agent system where responsibility is distributed among several specialized agents, coordinated by a Chat Manager, as shown in Figure~\ref{fig:agent_config_2}. This architecture is designed to handle queries by routing them to the agent best equipped for the task. As depicted in the decision process in Figure~\ref{fig:diagrama_agente_MultiAgente_2}, a \enquote{speaker selection} step determines the most suitable agent to act at each turn, promoting a more focused and contextualized approach to problem-solving.

            % (Your Figure agent\_config\_2 would be Figure 3.3 here)            
            \begin{figure}[h]
                \centering
                \includegraphics[width=.75\textwidth]{images/agent_config_2.png}
                \caption{Chat setup with one Chat Manager and a group of LLM agents.}
                \label{fig:agent_config_2}
            \end{figure}
            
            % (Your Figure diagrama\_agente\_MultiAgente\_2 would be Figure 3.4 here)
            \begin{figure}[h]
                \centering
                \includegraphics[width=1\textwidth]{images/agent_diagram_2.png}
                \caption{Multi-agent decision process.}
                \label{fig:diagrama_agente_MultiAgente_2}
            \end{figure}

                

        \subsection{Agent's Tools}
            
            In this experiment, three tools were considered in the decision-making process:

            \begin{itemize}            
                
                \item \textbf{Tool 1 - Learned Lessons Search:} a tool to search for learned lessons that may be relevant to the query. 
                \label{Tool1}
        
                \item \label{Tool2} \textbf{Tool 2 - Employee Search:} functionality that allows the search for information related to collaborators of an organization.
        
                \item \label{Tool3} \textbf{Tool 3 - NPT SQL Query:} Interface for executing SQL queries on a database of operational NPTs.    
                
            \end{itemize}

            There is also a pathway that allows the agent to provide a direct response, without the need to resort to other tools, presumably used when the LLM already possesses the necessary information.

    \section{Evaluation}

        The evaluation phase was designed to assess and compare the performance of the two proposed artifacts. This section details the methodology, the data set creation process, the metrics used, and the final results.

        \subsection{Evaluation Methodology}
        
            The evaluation was conducted by presenting a standardized set of questions to both the single-agent and multi-agent systems, using both GPT-3.5-turbo and GPT-4 models. The responses generated by each configuration were then collected and anonymized.

            A panel of three specialist engineers from the well construction department was tasked with analyzing the generated answers. Each specialist independently scored the responses based on the metrics described in Section~\ref{sec:evaluation_metrics}. The final score for each response was calculated by averaging the scores from the three experts, ensuring a robust and comprehensive assessment.
            
            \xexeo{Aqui seria bom fazer um BPMN do passo a passo do seu experimento, veja a figura 4.1 de\url{https://www.cos.ufrj.br/uploadfile/publicacao/3172.pdf}}
            \vitor{Feito}

            To provide a clear visual representation of the experimental workflow, a Business Process Model and Notation (BPMN) diagram is presented in Figure~\ref{fig:experimental_workflow}. This diagram illustrates the step-by-step process, from query submission to expert evaluation.

            \begin{figure}[h]
                \centering
                \includegraphics[width=\textwidth]{images/bpmn_experimento_1.png}
                \caption{Experimental workflow.}
                \label{fig:experimental_workflow}
            \end{figure}

            
        \subsection{Data Set Creation}

            A critical component of this evaluation is the test dataset. The dataset was meticulously created to reflect authentic information needs within the well construction domain. The process was as follows:

            \begin{description}
                \item[Source Selection] We identified three primary internal data sources: a database of Operational Learned Lessons, a structured database of Non-Productive Time (NPT) incidents, and a Collaborator Finder tool, as described in Section~\ref{sec:information-sources}.
                \item[Document Sampling] A random sample of documents and records was selected from each data source to ensure broad coverage of topics and scenarios.
                \item[Query Formulation] This process was performed by the author, leveraging domain expertise and collaboration with colleagues to ensure the questions were realistic, relevant, and challenging.
                \item[Dataset Composition] In total, a dataset of 33 unique queries was created. 
            \end{description}

            This approach to dataset creation, grounded in author experience and real-world documents, provides a valid basis for evaluating the artifacts. Table~\ref{table:question_examples} presents a sample of the queries formulated for the experiment.

            \xexeo{Coloca todas na tabela! E faz uma seÃ§Ã£o de criaÃ§Ã£o de perguntas, ou subseÃ§Ã£o}
            \vitor{Feito}

            
            \begin{table}[h]
                \centering
                \scriptsize
                \sloppy
                \begin{tabular}{|p{.1\linewidth}|p{.9\linewidth}|}
                \hline
                \textbf{Task category} & \textbf{Question} \\   \hline
                \multirow{17}{*}{Q\&A} & How does the presence of silica in the composition of cement 
                paste affect its thermal stability at high temperatures? \\ \cline{2-2}
                & What are the main challenges and risks associated with through tubing plug and abandonment in highly deviated wells? \\ \cline{2-2}
                % & What can cause hydrate formation in the Tree Running Tool  connector during the HCR (High Collapse Resistance) hose flush  before connecting to the Wet Christmas Tree? \\ \cline{2-2}
                % & What can cause the Down Hole Safety Valve to remain open  due to hydrate formation in the control lines? \\ \cline{2-2}
                % & What can cause damage to thread protectors and sealing  areas of pin ends of pipes stored at the coating yard? \\ \cline{2-2}
                % & What can cause high drag and torque off-bottom during  the drilling of a well with high deviation? \\ \cline{2-2}
                % & What precautions should be taken when performing a top check  of the abandonment plug in wells with higher inclination? \\ \cline{2-2}
                % & What are the critical factors to consider when choosing a base  fluid for manufacturing a viscous support plug? \\ \cline{2-2}
                % & What are the best practices for managing drilling parameters  during cement cutting to avoid premature bit wear? \\ \cline{2-2}
                & Give me all the information about employee BFD1. \\ \cline{2-2}
                & Who are the employees of the POCOS/EP/SASD team? \\ \cline{2-2}
                & How many advisors do we have in the POCOS/SPO department? \\ \cline{2-2}
                & Who are the advisors in the departments belonging to the POCOS/EP department? \\ \cline{2-2}
                & What data sources do you have? \\ \cline{2-2}
                & What functions do you have? \\ \cline{2-2}
                & How does well inclination affect the effectiveness of cementing during through-tubing plugging? \\ \cline{2-2}
                & What can cause difficulty in locking the handling cap of the coiled tubing BOP? \\ \cline{2-2}
                & What can cause anomalous behavior of the AutoTrak with GunDrill during drilling? \\ \cline{2-2}
                & What can be done to optimize the assembly of COP/COI for parallel movement of the JRC/THRT? \\ \cline{2-2}
                & What strategies can be adopted to improve the quality of cementing in highly inclined wells during through-tubing plugging? \\ \cline{2-2}
                & What are the alternatives to accelerate the curing time of cement slurry without compromising its integrity in high-temperature conditions? \\ \cline{2-2}
                & What are the risks associated with the improper substitution of cement with silica for pure cement in surface casing cementations in high-temperature wells? \\ \cline{2-2}
                & What was the strategy adopted to allow the passage of eccentric and/or large-diameter elements through the BOP quickly and without wedging the string with these elements inside the BOP? \\ \cline{2-2}
                \hline                
                \multirow{15}{*}{Text-to-SQL} & What was the longest-lasting NPT on rig number 05? \\ \cline{2-2}
                & How many NPTs occurred on rig number 06 during August 2023? \\ \cline{2-2}
                & What were the 5 most common abnormalities across all rigs? \\ \cline{2-2}
                & What were the abnormalities that occurred on all rigs during the week of September 14th to 20th, 2023? \\ \cline{2-2}
                & Which rigs had the most lost time in 2023? Give me a table with the rigs and the sum of hours. \\ \cline{2-2}
                & Which rigs had the most lost time in the first half of 2023? \\ \cline{2-2}
                & What were the latest abnormalities that occurred on the SS-70 rig? \\ \cline{2-2}
                % & What was the longest-lasting abnormality on the SS-70 rig? \\ \cline{2-2}
                & What was the peak of abnormality occurrences on the NS-52 rig? \\ \cline{2-2}
                & What was the total lost time in hours for abnormalities whose description mentions the term "Coiled Tubing"? \\ \cline{2-2}
                & What was the total lost time in hours on the NS-38 rig in 2023? \\ \cline{2-2}
                & What was the total time lost due to equipment failure on the NS-38 rig in 2023? \\ \cline{2-2}
                % & How many abnormalities occurred on the NS-31 rig during August 2023? \\ \cline{2-2}
                & How many abnormalities occurred on the NS-31 rig during July 2023? \\ \cline{2-2}
                & How many hours of lost time were caused by human error on the NS-47 rig in 2023? \\ \cline{2-2}
                & How many hours of lost time occurred on the MS-20 rig during June 2024? \\ \cline{2-2}
                & How many hours of lost time occurred on the NS-35 rig in 2024? \\
                \hline
                \end{tabular}
                \fussy
                \caption{Queries used in first cycle. }
                \label{table:question_examples}
            \end{table}

        \subsection{Evaluation Metrics} \label{sec:evaluation_metrics}

            To ensure a comprehensive assessment, the expert panel evaluated the artifacts' responses using the following metrics, which are based on the definitions presented in Section~\ref{sec:evaluation-review}:

            \begin{itemize}

                \item \textbf{Truthfulness}: A 1-5 Likert scale score measuring the factual accuracy of the response and the extent of any divergence from the ground truth. A higher score indicates a more factually correct answer with no hallucinations.

                \item \textbf{Performance}: A 1-5 Likert scale score assessing the overall quality of the response, including its linguistic coherence, logical structure, relevance, and conciseness.

                \item \textbf{LLM Cost}: A quantitative metric representing the financial cost in US dollars (USD) to generate a response for a given query using the OpenAI API. This reflects the computational expense and efficiency of each configuration. While other costs exist (development, infrastructure, maintenance), the API cost is a primary operational expenditure that scales directly with usage and is therefore a key metric for evaluating the economic viability of the artifacts, as established in our DSR framework.
            
            \end{itemize}

            To illustrate the application of the first two metrics, an example of an expert evaluation is provided in Table~\ref{tab:tabela_inputs_example}. The table shows the responses of different models to the query: "How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures?". It details the scores for Truthfulness and Performance, along with the specialists' comments that justify the ratings.

            \xexeo{Por que essa pergunta? Isso Ã© um exemplo? Como o multi agente entrou aqui se ainda nÃ£o falou de multi agente? Melhor deixar para mais tarde ou mostrar sÃ³ para single-agent?} 
            \vitor{Refatorado pra deixar claro q esse eh um exemplo e inserido apÃ³s a explicaÃ§Ã£o das arquiteturas.}
            % This 
            \xexeorem[comprehensive]{OLHA UMA PALAVRA INÃšTIL TÃPICA DE LLM} 
            % comparison allows for an 
            \xexeorem[in-depth]{Outra} 
            % assessment of the models' capabilities in generating accurate and relevant responses to technical questions.
            \vitor{Essa parte com palavras tÃ­picas de llm foi escrita no artigo, inÃ­cio do ano passado, quando essas expressÃµes ainda nÃ£o eram indesejÃ¡veis kkkk estou retirando tudo. Se ainda tiver passado algo, me avise pfv.}


        \begin{landscape}
            \begin{table}[h]
            \small
            \centering
            \caption{Query example with inputs, outputs, and human expert evaluations.}
            \label{tab:tabela_inputs_example}
            \begin{tabular}{| m{3cm} | m{1.5cm} | m{1.5cm} |>{\scriptsize}m{9cm} | m{1.3cm} | m{1.2cm} |>{\scriptsize}m{4cm} |}                    
            \toprule
            % \textbf{Input query} & \textbf{Model} & \textbf{Setup} & \textbf{Final output} & \textbf{Truthfulness} & \textbf{Performance} & \textbf{Specialist \ Comments} \\
            Input query & Model & Setup & Final output & Truth. \newline fulness & Perfor- \newline mance & Specialist \ Comments \\
            \midrule
            How does the presence of silica in the composition of cement paste affect its thermal stability at high temperatures? & GPT-3.5-turbo & Single-Agent & The presence of silica in the composition of cement paste can affect its thermal stability at high temperatures due to some characteristics of silica. Silica is an inorganic material that has a stable crystalline structure at high temperatures. When exposed to high temperatures, the silica present in the cement paste can undergo structural transformations, resulting in changes in the properties of the paste.

            One of the main changes that can occur is the thermal expansion of the cement paste due to the formation of unstable silica compounds. This thermal expansion can lead to cracks and fissures in the structure, compromising the material's integrity and stability.

            Moreover, the presence of silica in the composition of cement paste can affect the material's thermal resistance. Silica has low thermal conductivity, which can lead to a reduced ability of the cement to withstand high temperatures. This can result in a decrease in the material's strength and, consequently, a reduction in its thermal stability.

            In summary, the presence of silica (...) & 4 & 5 & Created a section that is irrelevant to the question: "Moreover, the presence of silica in the composition of cement paste can affect the material's thermal resistance. (...)" \\
            \cline{3-7}
            &  & Multi-Agent & The presence of silica in the composition of cement paste has a positive effect on thermal stability at high temperatures. Silica is added to cement to mitigate the retrogression of cement strength when exposed to temperatures above 230Â°F (110Â°C). Deep wells or those with a high geothermal gradient may require the use of silica cement to ensure the thermal stability of the paste. The use of silica cement is particularly important in surface casing cementing, where the temperature can exceed the critical limit. \ (...) & 2 & 2 & Retrieved an unrelated Lesson Learned on the subject (about plugs). \\ 
            \cline{2-7}
            & GPT-4 & Single-Agent & The presence of silica in the composition of cement paste increases thermal stability, especially at high temperatures. Silica is used to transform the cement into a refractory material, capable of withstanding extreme conditions. 
            
            The cement (...).

            However, (...). & 3 & 4 & Although it is true, the item on the side is not relevant to the question: "However, it is important to note that (...)." \\
            \cline{3-7}
            &  & Multi-Agent & Silica is added to cement at a ratio of 35\% by weight of cement to mitigate the retrogression (loss of compressive strength) of the cement in the well when subjected to temperatures above 230ÂºF (110ÂºC). \ (...) & 5 & 4 & Cited an unrelated (but true) Lesson Learned about plugs, Lesson Learned LA7005. \\
            \hline
            \end{tabular}
            \end{table}
        \end{landscape}
            

        % \subsection{Results and Discussion}

            % (This section would contain the complete text from your original "Results" and "Discussion" sections. The content is good, but it logically belongs here as the final part of the DSR evaluation phase. You can simply move that text here, ensuring it flows from the description of the metrics.)


        \subsection{Results} % NEW SUBSECTION for your results
            % The results table and the sections analyzing each metric go here.
            % The original \subsection{Truthfulness}, \subsection{Performance},
            % and \subsection{LLM Cost} become \subsubsections.

            % The third metric, LLM Cost
            \xexeo{Isso aqui Ã© uma pergunta de pesquisa tem que entrar de alguma maneira na definiÃ§Ã£o do DSR, lembrando que as avaliaÃ§Ãµes do DSR podem ser mais de uma}
            \vitor{Feito. Movido p/ definiÃ§Ã£o do DSR}
            % , is
            \xexeo{NÃ£o Ã© represents, jÃ¡ que Ã© o custo mesmo, acho que  corresponds to, ou mesmo sÃ³ is }
            \vitor{Feito} 
            % the financial cost associated with using OpenAI's API for the language models in each configuration. This metric is measured in US dollars and reflects the computational resources required for each task.
            \xexeo{Tem que falar alguma coisa que nÃ£o Ã© o Ãºnico custo, e quais sÃ£o os outros e porque esse Ã© importante, isso pode estar descrito no modelo DSR, antes}
            \vitor{Feito}
            \xexeo{Esse parÃ¡grafo tipicamente aparece na revisÃ£o}


            This section provides an analysis of the data collected during the first experimental cycle. The aggregated results are presented in Table~\ref{tab:tabela_resultados}, followed by a discussion of each evaluation metric established in our DSR framework: Truthfulness, Performance, and LLM Cost.

            \begin{table}[h]
                \small % Reduce the font size
                \centering % Center the table on the page
                \caption{Results on Q\&A and Text-to-SQL tasks, including standard deviation (Std). The best metrics are highlighted with \textbf{\underline{bold and underline}}. The second best are highlighted with \textbf{bold}.}
                \label{tab:tabela_resultados}
                \begin{tabular}{|>{\raggedright\arraybackslash}p{2.0cm}|>{\centering\arraybackslash}p{0.85cm}|>{\centering\arraybackslash}p{0.95cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.85cm}|>{\centering\arraybackslash}p{0.95cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{0.8cm}|}
                \hline
                \rowcolor{gray!20}
                \textbf{Task}           & \multicolumn{5}{c|}{\textbf{Single-Agent}}           & \multicolumn{5}{c|}{\textbf{Multi-Agent}} \\ % Merging cells and adding heading
                \textbf{Model}          & \textbf{LLM Cost} & \textbf{Truth.} & \textbf{Std} & \textbf{Perf.} & \textbf{Std} & \textbf{LLM Cost} & \textbf{Truth.} & \textbf{Std} & \textbf{Perf.} & \textbf{Std} \\ \hline
                \cellcolor{gray!20} Q\&A & & & & & & & & & &\\
                GPT-3.5-turbo            & 0.005             & 2.94              & 1.48 & 3.94          & 1.09 & 0.02              & 4.09              & 1.22 & 3.82 & 0.98 \\
                GPT-4                   & 0.12              & \textbf{3.88}     & 1.41 & \textbf{4.06} & 1.30 & 0.45              & \underline{\textbf{4.57}} & 0.79 & \underline{\textbf{4.43}} & 0.79 \\
                \cellcolor{gray!20} Text-to-SQL & & & & & & & & & &\\
                GPT-3.5-turbo            & 0.009             & 4.13              & 1.41 & 4.44          & 1.03 & 0.02              & \textbf{4.29}     & 1.20 & \textbf{4.29} & 1.33 \\
                GPT-4                   & 0.10 & \underline{\textbf{4.56}} & 0.96 & \underline{\textbf{4.63}} & 0.81 & 0.51      & 3.20              & 1.99 & 3.70 & 1.89 \\ \hline
                \end{tabular}
            \end{table}

            The comparative analysis between single and multi-agent setups for RAG, using GPT-3.5-turbo and GPT-4 models, revealed insights regarding the metrics of truthfulness, performance, and costs of the language model.


            \subsubsection{Truthfulness} 

                In assessing the truthfulness metric, significant differences are noted between the single and multi-agent settings in both Q\&A and Text-to-SQL tasks. The results are illustrated in Figures \ref{fig:truthfulness_QA} and \ref{fig:truthfulness_text2sql}.
                For Q\&A tasks, GPT-4 in a multi-agent configuration significantly exceeded the performance of the single-agent with a truthfulness score of 4.57 compared to 3.88. The GPT-3.5-turbo model showed distinct results between the two configurations, with the multi-agent surpassing the single-agent with scores of 4.09 and 2.94, respectively.
                In terms of Text-to-SQL queries, a different outcome was observed. GPT-4 single-agent achieved a score of 4.56, while the same model in the multi-agent configuration obtained 3.20, highlighting a limitation for the multi-agent in this task. Conversely, the GPT-3.5-turbo maintained a more balanced performance between configurations, scoring 4.29 for multi-agent and 4.13 for single-agent.
                
                \begin{figure}[h]
                    \centering
                    \begin{minipage}{.48\textwidth}
                        \centering                
                        \includegraphics[width=1\linewidth]{images/truthfulness_QA.png}
                        \caption{Truthfulness and standard deviation in Q\&A tasks by LLM model and agent configuration.}
                        \label{fig:truthfulness_QA}
                    \end{minipage}%
                    \hspace{0.2cm}
                    \begin{minipage}{.48\textwidth}
                        \centering
                        \includegraphics[width=1\linewidth]{images/truthfulness_text2sql.png}
                        \caption{Truthfulness and standard deviation in Text-to-SQL tasks by LLM model and agent configuration.}
                        \label{fig:truthfulness_text2sql}
                    \end{minipage}
                \end{figure}

                
            \subsubsection{Performance}        

                The evaluation of LLM performance \citep{Li2023} in the tasks of Q\&A and Text-to-SQL reveals trends which are similar to the truthfulness results. 
                % As shown in Figures \ref{fig:performance_QA} and \ref{fig:performance_text2sql} and summarized in \ref{tab:tabela_resultados}, the text performance in single and multi-agent setups was compared using the GPT-3.5-turbo and GPT-4 models.        
                For Q\&A tasks, the multi-agent setup shows a performance boost compared to the single-agent setup. In particular, the multi-agent GPT-4 achieves a performance score of 4.43, which is higher than the single-agent GPT-4 score of 4.06. This pattern is consistent with the GPT-3.5-turbo, where the multi-agent system also surpasses the single-agent system, scoring 3.82 and 3.94, respectively. These findings emphasize the effectiveness of the multi-agent approach in handling technical user queries.
                        
                \begin{figure}[h]
                    \centering
                    \begin{minipage}{.48\textwidth}
                        \centering                
                        % \framebox{
                            \includegraphics[width=1\linewidth]{images/performance_QA.png}
                        % }
                        \caption{Performance and standard deviation in Q\&A tasks by LLM model and agent configuration.}
                        \label{fig:performance_QA}
                    \end{minipage}
                    \hspace{0.2cm}
                    \begin{minipage}{.48\textwidth}
                        \centering
                        % \framebox{
                        % \includegraphics[width=1\linewidth]{images/performance_text2sql.png}
                        % }
                        \caption{Performance and standard deviation in Text-to-SQL tasks by LLM model and agent configuration.}
                        \label{fig:performance_text2sql}
                    \end{minipage}%
                \end{figure}


            \subsubsection{LLM Cost} 
                Language model services are typically composed by a values per token. For instance, GPT-4 model costs US\$30.00 (input) and US\$60.00 (output) per 1 million tokens received and sent, respectively.        
                The single-agent architecture demonstrated substantially lower costs for both Q\&A and Text-to-SQL tasks compared to the multi-agent setup as shown in Figure~\ref{fig:truthfulness_vs_cost_vs_config_model}. For instance, the average cost of the GPT-4 model \citep{OpenAI2023} for a Q\&A task was \$0.12 per processed question for the single-agent, while the multi-agent recorded an average cost of \$0.45. This trend of higher costs for the multi-agent architecture was also maintained for Text-to-SQL tasks, with an average cost of \$0.51 for the multi-agent architecture in contrast to \$0.10 for the single agent.
                The higher token count and cost for multi-agent setting is due to the inclusion of intermediate calls, for example, when the "Agent Selector" needs to decide which agent to pass the turn to. All the message history is passed to the LLM at this stage, substantially increasing the number of tokens submitted and response time.


                \begin{figure}[h]
                    \centering              
                    % \framebox{
                        \includegraphics[width=0.75\textwidth]{images/truthfulness_vs_cost_vs_config_model.png}
                    % }
                    \caption{Average LLM costs and Truthfulness per completed task according to setup and model.}
                    \label{fig:truthfulness_vs_cost_vs_config_model}
                \end{figure}
                
                

        \subsection{Discussion} % NEW SUBSECTION for your discussion
            % The original \section{Discussion} and all its content go here.
            % The original \subsections become \subsubsections.

            
            The comparison between single and multi-agent systems revealed significant differences in terms of performance and cost:
            
            \subsubsection{General Performance.}     
                The results indicate that for Q\&A tasks in the context of O\&G, truthfulness measure was 28\% higher with the multi-agent architecture compared to single. 
                However, for Text-to-SQL tasks, this trend was inverted, where the single-agent scored 15\% higher.

                These findings suggest that for Q\&A tasks, the multi-agent setup may be more advantageous in terms of providing truthful information, particularly when utilizing the more advanced GPT-4 model. 
                Conversely, in Text-to-SQL tasks, the GPT-4 model in a single-agent configuration proved more effective. 
                This might imply that the added complexity of managing multiple agents in some tasks does not necessarily lead to improved performance in responses, underscoring the importance of carefully selecting the agent configuration based on the task type and specific features of the language model used.
                    
            \subsubsection{Cost-Performance Analysis.}
                While the multi-agent system shows higher truthfulness in Q\&A tasks, it is crucial to consider the associated costs. 
                To provide a clearer comparison, let us consider the score/cost ratios. For Q\&A tasks using GPT-4, the single-agent configuration yields a ratio of 32.33 truthfulness points per dollar, compared to 10.16 for the multi-agent setup. This indicates that while the multi-agent system shows a 17.8\% improvement in truthfulness, it comes at a 275\% increase in cost.
                
                % Based on our analysis, we recommend using a multi-agent system for Q\&A tasks when the budget allows for it and accuracy is a critical factor. 
                % However, decision-makers should consider setting a cost-performance threshold to guide the choice of system configuration, ensuring that the benefits justify the expenses involved.
                \xexeo{TEm que deduzir a necessidade de fazer um experimento antes levando essas coisas em consideraÃ§Ã£o}
                \vitor{Feito abaixo.}

                This trade-off highlights an important implication for any organization considering the adoption of these technologies. The optimal architecture is not universal; it is highly dependent on specific task requirements and budget constraints. 
                This reality underscores the necessity of conducting a preliminary, cost-performance evaluation. Rather than simply selecting a model, decision-makers must first perform a targeted analysis to establish a cost-benefit threshold. 
                Our work not only provides initial data for the O\&G domain but also demonstrates a foundational methodology for this evaluation process, which ultimately motivated the more rigorous and quantitative approach of our second experimental cycle.


            \subsubsection{Model Performance Variations.}
                Interestingly, our results show that GPT-3.5-turbo outperforms GPT-4 in certain tasks, particularly in the Text-to-SQL multi-agent configuration, despite GPT-4's larger size and more extensive training. 
                This unexpected performance could be attributed to several factors. 
                First, GPT-3.5-turbo may have undergone more specific fine-tuning for structured query tasks, allowing it to excel in Text-to-SQL scenarios. 
                Additionally, GPT-3.5-turbo's training data might be more recent or more relevant to the specific domain of our study. 
                Another possibility is that the smaller model size of GPT-3.5-turbo allows for faster processing and more efficient handling of the multi-agent setup, resulting in better performance in some contexts.

                However, it is important to note that GPT-4, when used in a multi-agent setup, demonstrated more consistent truthfulness and performance, as evidenced by its reduced standard deviation in results. 
                This consistency can be particularly advantageous in applications where reliability and accuracy are critical. 
                Multi-agent systems have the advantage of maintaining separate contexts for different aspects of a task \citep{Langchain2025blogmultiagent}. 
                \xexeo{VocÃª pode suportar essa afirmaÃ§Ã£o com uma citaÃ§Ã£o?}
                \vitor{Feito.}
                This compartmentalization can lead to better handling of complex, multi-faceted queries, as each agent can focus on its specific context without being overwhelmed by irrelevant information. However, this advantage may be offset in tasks like Text-to-SQL, where maintaining a unified context of the database schema and query structure is crucial, possibly explaining the better performance of single-agent setups in this task.
                Furthermore, the multi-agent architecture inherently involves multiple stages of information processing, which can serve as natural filtering mechanisms.
                As information passes from one agent to another, irrelevant or low-quality data may be naturally filtered out, leading to more refined and accurate final outputs. 
                This could explain the superior performance in filtering irrelevant information observed in multi-agent setups.
            
            
            \subsubsection{Economic Efficiency.} 
            
                The multi-agent architecture incurs significantly higher costs compared to the single-agent system, primarily due to additional intermediate calls to the language model and multiple iterations between agents for action planning. 
                Also, the cost differences between using GPT-4 and GPT-3.5-turbo are substantial, with GPT-4 being 20 times more expensive (in early 2024).
                \xexeo{Dizer x vezes mais caro em julho de 2025}.
                \vitor{Feito}

                The average cost per query for each configuration is presented in Table \ref{tab:cost_per_query}. These figures highlight the direct cost implications of the chosen architecture and model.
                
                \begin{table}[h!]
                \centering
                \caption{Average LLM Cost Per Query (USD). Values from early 2024.}
                \label{tab:cost_per_query}
                \begin{tabular}{l r}
                \toprule
                \textbf{Configuration} & \textbf{Cost per Query} \\
                \midrule
                Single-Agent (GPT-3.5-Turbo) & \$0.0068 \\
                Single-Agent (GPT-4) & \$0.1095 \\
                Multi-Agent (GPT-3.5-Turbo) & \$0.0197 \\
                Multi-Agent (GPT-4) & \$0.4896 \\
                \bottomrule
                \end{tabular}
                \end{table}

                To illustrate the financial implications of adopting different models and architectures, we estimate the annual costs for a large company with 40,000 knowledge workers. Our calculations are based on an average of 5 queries per worker per day, over 250 working days per year.
                
                Under these assumptions, the total annual query volume is 50 million (40,000 workers $\times$ 5 queries/day $\times$ 250 days). For a single-agent configuration, this results in an annual cost of approximately \$337,843 for GPT-3.5 and \$5.47 million for GPT-4.
                
                In a multi-agent architecture, the costs increase substantially, escalating to approximately \$986,631 for GPT-3.5 and \$24.48 million for GPT-4. These estimates underscore the significant financial trade-offs when adopting a multi-agent system, which, while potentially offering performance benefits, comes with a considerable increase in LLM operational costs.

                While multi-agent systems and more advanced models like GPT-4 offer improvements in performance, the economic efficiency, as measured by truthfulness per dollar, may favor single-agent systems and less costly models like GPT-3.5-turbo, depending on the specific application and budget constraints.

                It is important to note that, as of July 2025, the landscape of LLMs has evolved substantially. The emergence of more efficient models, has led to a significant decrease in API's costs. This suggests that the financial trade-offs discussed previously may no longer be as pronounced, and that high-performance multi-agent systems could become economically viable much sooner than anticipated.

                \xexeo{In summary Ã© o parÃ¡grafo tÃ­pico das LLMs... Mas Ã© isso mesmo. PorÃ©m tem que colocar um ponto: o custo dos modelos estÃ¡ caindo barbaramente com o aparecimento de novos modelos no topo de desempenho e novas tecnologias tem permitido alcanÃ§ar resultados de Ã³tima qualidade com mÃ¡quinas muito menores, o que tambÃ©m derruba o custo. Pode atÃ© citar o exemplo do DeepSeek (buscando na literatura o desempenho x custo dele)}
                \vitor{Feito}
                
            
            \subsubsection{Challenges and Limitations}     
                During the evaluation of the agents, several challenges and limitations were identified.

                \textbf{\textit{Contextualization and Interpretation.}} 
                    In many cases, the single-agent solution had difficulty understanding the context of the question. For example, a question about cementing was interpreted in the context of the construction industry, a theme to which the language models were more exposed during the training phase. 
                    However, the multi-agent structure, with its well-defined roles, better understood the questions and showed superior performance in Q\&A tasks, corroborating the findings of \citep{Li2024}.
                
                \textbf{\textit{Filtering Irrelevant Information.}} 
                    The agent often receives irrelevant documents along with important ones in the prompt context, and it is up to the LLM to ignore these. 
                    For example, when asked about alternatives to accelerate the curing time of cement paste without compromising its integrity at high temperatures, the RAG system retrieved a document that included information about batch cementing to ensure homogeneity during manufacturing and pumping. 
                    While this information is true, it was not relevant to the specific question asked. 
                    In this aspect, the multi-agent solution performed better at discarding such irrelevant information, focusing more accurately on the task at hand. 
                    Other possible solutions include improving the accuracy of semantic search by adjusting a minimum threshold for similarity measures or through re-ranking techniques such as those proposed by \citep{Carraro2024} and \citep{Sun2023}.
                
                \textbf{\textit{Hallucination.}} 
                    During the evaluation of our system, we encountered instances where the agent produced hallucinated information instead of utilizing the appropriate tool to retrieve accurate data, as in \citep{Bilbao2023}. 
                    For example, when asked, "How many anomalies occurred on rig number 05 during August 2023?" the agent was expected to use the Text-to-SQL tool to query the database. 
                    However, it bypassed this tool and generated a fabricated response, stating that 5 anomalies occurred, along with detailed descriptions of fictional events. The correct answer, as retrieved from the database, was that 7 anomalies occurred. This hallucination likely resulted from the agent's reliance on its internal knowledge rather than external data retrieval. 

                    In terms of hallucination statistics, our analysis revealed that for Q\&A tasks, hallucinations occurred in 9.6\% of cases and 3.8\% for partially hallucinated. 
                    In contrast, Text-to-SQL tasks exhibited a lower hallucination rate, with only 3.6\% of responses containing hallucinated information and 96.4\% being accurate. 
                    These findings highlight the variation of susceptibility to hallucination in different types of tasks, highlighting the need for targeted strategies to mitigate this problem.
                
                \textbf{\textit{Industry Jargon:}}
                    Specifically analyzing the activity of drilling and completion of offshore wells, the main challenge is the inherently complex and technical nature of the data involved. 
                    There were instances of incorrect interpretation of information, likely due to the use of terms, expressions, and themes specific to well construction, to which the language model had little or no exposure during training phase. 
                    A possible solution is the implementation of specialized models, which has been pointed out in gray literature as a trend for the coming years \citep{Shah2024, Meena2023, Ghosh2023}.
                
                \textbf{\textit{Tools vs. Performance:}} 
                    It was identified during the experiments that agents with a high amount of tools showed a decline in overall performance. 
                    This can be attributed to the added context to the prompts. 
                    As the context length increases, the model's ability to accurately interpret and respond diminishes.
                    This is a limitation of current language models, where longer contexts can lead to a dilution of relevant information and increased difficulty in maintaining coherence and accuracy. 
                    This conclusion is currently qualitative, as these metrics were not addressed in this experiment.

                
                \textbf{\textit{Queries Involving Proper Names:}}
                    In queries involving people's names, it was not possible to retrieve relevant documents using semantic search. 
                    For example, when asked to identify the employee associated with a specific key and list knowledge items they registered in the system, the LLM incorrectly attributed such documents to the wrong author
                    \xexeo{O RAG ou a LLM usando o RAG, nÃ£o ficou claro}
                    \vitor{Corrigido}
                    . 
                    This highlights the difficulty in accurately retrieving information based on proper names, which can be complicated by variations in accentuation, abbreviation, and formatting.
                    \xexeo{tem evidÃªncias disso em outros artigos?}
                    \vitor{nÃ£o encontrei}
                    A potential solution to be explored is the use of Self-Query Retriever \citep{LangchainSelfQuery2023}, implementing a hybrid search with metadata filters (including proper names) and semantic retrieval of the rest of the query. 
                    It is also suggested, in these cases, to use the \citep{Levenshtein1966} distance to handle possible variations in the spelling of names. 
                    This approach could improve the accuracy of retrieving documents related to specific individuals, ensuring that the correct information is associated with the right person.
                    
            
            % \subsubsection{Practical Implications}
            \subsubsection{Practical Implications.} 

                The findings from our study have significant practical implications for the O\&G sector, and potentially for other industries characterized by complex and technical data environments:
                    
                \begin{itemize}
                
                    \item \textbf{Enhanced Decision-Making Support:}
                        Our results indicate that multi-agent systems provide a 28\% higher truthfulness measure in Q\&A tasks. This can be particularly beneficial for decision-making in well engineering, where accurate and truthful information is critical.
                        Implementing multi-agent systems in decision-making processes can lead to more reliable and informed decisions, thereby reducing the risk of errors and enhancing operational safety and efficiency.
                    
                    \item \textbf{Balancing Performance and Economic Efficiency:}
                        While multi-agent systems offer superior performance in terms of truthfulness, they come with a cost that is 3.7 times higher on average compared to single-agent systems.
                        This highlights the importance of a strategic approach in selecting agent configurations based on specific tasks and budget constraints. 
                        % For instance, single-agent systems might be more cost-effective for Text-to-SQL tasks where they have shown to perform 15\% better. 
                        A detailed cost-benefit analysis reveals that for Q\&A tasks using GPT-4, the single-agent configuration yields a ratio of 32.33 truthfulness points per dollar, compared to 10.16 for the multi-agent setup. While the multi-agent system shows a 17.8\% improvement in truthfulness, this comes at a 275\% increase in cost. The efficiency varies significantly by task type; in Text-to-SQL tasks, the GPT-4 single-agent outperforms the multi-agent by 42.5\% in truthfulness while costing 80.4\% less. 
                        % These quantitative insights emphasize the need for careful consideration of task requirements and budget constraints when choosing between single and multi-agent configurations.
                        
                    \item \textbf{Reflection and Critic Agents:}
                        A promising approach to enhance the performance of these agents is the use of reflection \citep{Shinn2023}, a method where agents verbally reflect on task feedback signals and maintain this reflective text in an episodic memory buffer to improve decision-making in subsequent trials. Critic agents are a way to implement reflection in a multi-agent setup. This type of agent is challenging to apply in Q\&A tasks over private technical data, as commercial LLMs (OpenAI, Google Bard, and others) have not been deeply trained in the domain and struggle to provide relevant and precise critiques, reinforcing the trend toward increased use of domain-specific models \citep{Shah2024, Meena2023, Ghosh2023}.                
                        
                    \item \textbf{Task-Specific Agent Configuration:}
                        The study highlights that the complexity of managing multiple agents does not always lead to better performance. In some cases, a single-agent setup might be more effective.
                        This insight can guide the development and deployment of AI systems, ensuring that the configuration of agents is tailored to the specific requirements of the task, thereby optimizing both performance and cost.           
                        
                    \item \textbf{Potential for Broader Application:}
                        The insights gained from this study are not limited to the O\&G sector but can be applied to other industries with similar technical complexities, such as aerospace, pharmaceuticals, and renewable energy.
                        By adopting multi-agent systems in these industries, organizations can improve decision-making, knowledge management, and operational efficiency, driving innovation and competitiveness.             
                    
                \end{itemize}
                        
                    
            \subsubsection{Future Directions.} 

                This work indicates possible pathways for enhancing RAG architectures in O\&G sector. 
                
                \begin{itemize}
                
                    \item \textbf{Enhancement of IR Semantic Techniques:}
                        There is a critical need to develop more sophisticated semantic search technologies. Future efforts should focus on enhancing the precision of information retrieval by filtering out irrelevant content more effectively. This will ensure that agents can provide more accurate and contextually appropriate responses, crucial for technical domains such as O\&G.
                        
                    \item \textbf{Development of Domain-Specific Models:}
                        Specialized models tailored specifically to the O\&G and other domains, such as biomedical engineering \citep{Pal2024}, could significantly improve the handling of specific jargon and complex technical data, while reducing LLM costs \citep{Arefeen2024}. Future research should aim to develop and train these models to better understand and interpret the unique language and data types found in O\&G, enhancing the overall accuracy of agent responses.
                        
                    \item \textbf{Optimization of Tool Use in Agent Performance:}
                        The relationship between the quantity of tools available to an agent and its performance needs further exploration. Future studies should quantify the impact of tool availability on agent efficacy and efficiency, aiming to optimize tool use without overwhelming the agent or diluting performance quality.
                        
                    \item \textbf{Integration of Advanced Name Recognition Techniques:}
                        Queries involving proper names pose a significant challenge in semantic search. Integrating advanced retrieval techniques, such as Self-Query Retrievers \citep{LangchainSelfQuery2023} and \citep{Levenshtein1966} distance algorithms, could improve the handling of these queries. Future research should focus on enhancing name recognition capabilities to ensure that agents can accurately retrieve and utilize correct information, especially in scenarios where precision is paramount.
                        
                    \item \textbf{Extension to Other Complex Domains:}
                        The potential applications of multi-agent systems are not limited to the O\&G sector. Future research should explore the adaptation and implementation of these systems in other complex and technical domains, such as aerospace, pharmaceuticals, and renewable energy. Investigating how these systems can support decision-making in these areas will provide valuable insights into their versatility and adaptability.
                        
                    \item \textbf{Hybrid Model Experimentation:}
                        Combining the strengths of single and multi-agent systems could yield significant benefits. Future directions should include experimenting with hybrid models that integrate the robustness and depth of multi-agent interactions with the simplicity and efficiency of single-agent systems. This hybrid approach could potentially offer a balanced solution, maximizing performance while managing costs and complexity.
                        

                \end{itemize}
                
                By pursuing these directions, future research can significantly advance the development of multi-agent systems, not only enhancing their application in the O\&G sector but also expanding their utility across various technologically intensive activities.
                    


\chapter{Second Experimental Evaluation Cycle}
\label{chap:second_experiment}

This chapter describes the second experimental cycle of this research, building upon the findings of the first cycle detailed in Chapter 3. The rapid evolution of generative AI frameworks and models, along with the insights gained previously, prompted a more advanced and rigorous evaluation. This second phase employs non-agentic workflows as a baseline, introduces a more quantitative evaluation methodology, and leverages an automated assessment process based on the "LLM-as-judge" concept \citep{Gu2025}.

The use of "LLM-as-a-judge" was driven by the sheer volume of responses requiring evaluation. With four configurations, two models, and three executions for each of the 33 [AJUSTAR] questions, a total of 792 [AJUSTAR] responses were generated. Manually assessing this volume of data would have been impractical. Furthermore, previously used metrics like `truthfulness` had become less critical. This metric was highly relevant when models frequently hallucinated, a problem that is far less prevalent in the current generation of LLMs, shifting the focus to precision and recall of factual information.

[TODO: GERAR BPMN!!]

\section{Design Science Research Framework}

    This second experimental cycle adheres to the Design Science Research (DSR) methodology, focusing on refining the artifacts and evaluation based on the outcomes of the first cycle.

    \begin{description}
        \item[Context] The operational environment of well construction engineering, where practitioners require efficient and reliable access to vast amounts of technical and ESG-related information.

        \item[Problem] The first experimental cycle revealed several limitations, including the subjective nature and scalability issues of expert-based evaluation, the need to compare agentic systems against simpler non-agentic baselines, and the challenge of ensuring consistent performance. This second cycle addresses the problem of developing a more robust, scalable, and objective method for evaluating and comparing different LLM-based architectures for domain-specific Q\&A.

        \item[Proposed Artifacts] Four distinct architectures were designed and implemented to compare different strategies for information retrieval and reasoning:
        \begin{itemize}
            \item A non-agentic \textbf{Linear-Flow} RAG pipeline.
            \item A non-agentic \textbf{Linear-Flow with a Router} to direct queries.
            \item A \textbf{Single-Agent} architecture, refined from the first experiment.
            \item A \textbf{Multi-Agent Supervisor} architecture for distributed reasoning.
        \end{itemize}

        \item[Evaluation] The artifacts are evaluated using an automated pipeline. An "LLM-as-a-judge" assesses the generated answers against a ground-truth dataset. The evaluation is based on quantitative information retrieval metrics: \textbf{Precision}, \textbf{Recall}, and \textbf{F1-Score}.
    \end{description}

\section{Context and Problem Statement}

    \subsection{Context}

    As established in the previous chapters, this research is situated within the oil and gas industry, specifically in the domain of well construction and maintenance. Engineers and specialists in this field must navigate a complex information landscape, drawing from operational reports, ESG alerts, and documented best practices (Learned Lessons) to make critical decisions. The effectiveness of these decisions hinges on the speed and accuracy with which relevant information can be retrieved and synthesized.

    \subsection{Problem}

    The first experimental cycle confirmed the potential of LLM-based agents but also highlighted key challenges. The manual, expert-led evaluation process was time-consuming and difficult to scale. Furthermore, the performance differences between single and multi-agent systems suggested that a more granular analysis was needed, including a comparison with non-agentic RAG workflows to establish a performance baseline. Therefore, the central problem for this second cycle is to design and execute a more rigorous, automated, and scalable evaluation to definitively compare the efficacy of various agentic and non-agentic architectures in this specialized domain.


\section{Proposed Artifacts}

    To address the research problem, four distinct artifacts were developed, representing a spectrum of complexity from simple sequential pipelines to collaborative multi-agent systems. 
    
    \subsection{System Architecture Overview}

    The experimental system was implemented using \citet{Langchain2025} and \citet{Langgraph2025} frameworks specialized in language model orchestration. This modular design allows for the systematic and reproducible evaluation of different components and workflows. Key layers of the architecture include:

    \begin{itemize}
        \item \textbf{Experiment Orchestration:} Manages the execution loop, iterating through all combinations of questions, models, and setups.
        \item \textbf{Agent Workflow Frameworks:} Defines the logic for each of the four proposed artifacts using LangGraph to create cyclical graphs for agentic behavior.
        \item \textbf{Tool Integration:} A standardized interface providing agents with access to external knowledge sources. This layer enables consistent semantic search over domain-specific vector stores, ensuring that performance differences are attributable to architectural choices rather than variations in data access.
        \item \textbf{Prompt Engineering:} A library of system messages and prompt templates designed to guide the LLM's reasoning process for each specific task within the workflows.
        \item \textbf{State Management and Logging:} Captures the complete execution trace of each run, including intermediate steps, tool calls, and final outputs. This observability is essential for understanding not just the final output, but the process by which each architecture arrived at its answer.
    \end{itemize}


    \subsection{Artifact 1: Linear-Flow}

        The \textbf{Linear-Flow} architecture represents the simplest non-agentic RAG design, serving as a performance baseline. As shown in Figure \ref{fig:diagrama_linear_flow}, user input is processed in a strictly sequential manner. The user's query is handled by a single LLM step, which contains all the instructions needed to generate search queries for every available tool.
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{images_exp2/diagrams/diagrama_linear_flow.png}
            \caption{Linear-Flow architecture. PTn indicates the prompt for Tool n.}
            \label{fig:diagrama_linear_flow}
        \end{figure}

        Because the instruction prompts for all tools are aggregated into a single call, the resulting context for the LLM becomes notably extensive and complex. While this approach is straightforward to implement, its primary drawback is the potential for performance degradation as the context length increases, which can dilute the model's focus and lead to less precise retrieval queries.
        

    \subsection{Artifact 2: Linear-Flow with Router}
    
        The \textbf{Linear-Flow with Router} paradigm (Figure \ref{fig:diagrama_linear_w_router}) extends the basic pipeline by introducing a routing mechanism to create a descentralized, non-agentic workflow. This architecture first directs a user's question to a "router" node, which is a preliminary LLM call tasked with analyzing the query and determining the most appropriate tool or sequence of tools to use.

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{images_exp2/diagrams/diagrama_linear_w_router.png}
            \caption{Linear-Flow with Router architecture.}
            \label{fig:diagrama_linear_w_router}
        \end{figure}

        This design enables the distribution of complex instruction prompts into smaller, more specialized nodes. Instead of one large prompt, several targeted sub-queries are generated, each dispatched to its respective tool. This approach offers two main advantages:

        \begin{itemize}
            \item \textbf{Specialization:} Each tool receives a query tailored to its specific function, leading to more accurate and relevant retrieval results.
            \item \textbf{Reduced Context:} By breaking down the master prompt, each LLM call operates on a smaller, more focused context, mitigating performance issues associated with long context windows.
        \end{itemize}


    \subsection{Artifact 3: Single-Agent}

        The \textbf{Single-Agent} architecture (Figure \ref{fig:diagrama_single_agent}) embodies a centralized agentic approach, building on the lessons from the first experimental cycle. In this setup, a single LLM agent manages the entire question-answering process. It has access to the full suite of tools and autonomously makes decisions about which to invoke, in what order, and how to synthesize the retrieved information into a final answer.
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{images_exp2/diagrams/diagrama_single_agent.png}
            \caption{Single-Agent architecture.}
            \label{fig:diagrama_single_agent}
        \end{figure}    

        The design emphasizes \textbf{end-to-end reasoning within a unified context}, allowing the model to maintain the same "thought process" from start to finish. This artifact tests the capability of a standalone LLM agent to manage a RAG workflow, balancing the tool calling for different knowledge sources, all without the communication overhead required by multi-agent systems.
        

    \subsection{Artifact 4: Multi-Agent Supervisor}
    
        The \textbf{Multi-Agent Supervisor} setup (Figure \ref{fig:diagrama_multiagente_supervisor}) implements a collaborative, hierarchical system to explore the benefits of distributed cognition. This architecture consists of two main components:        

        \begin{enumerate}
            \item \textbf{A Supervisor Agent:} This master agent receives the user's query, analyzes it, and orchestrates the workflow by delegating these tasks to the appropriate specialist agents.
            \item \textbf{Specialist Agents:} A team of agents, each focusing on a specific domain of knowledge or reasoning skill. For this experiment, each specialist was tied to a single tool (e.g., a "Learned Lessons Agent," an "HSE Alert Agent").
        \end{enumerate}
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{images_exp2/diagrams/diagrama_multiagente_supervisor.png}
            \caption{Multi-Agent Supervisor architecture with four specialist agents.}
            \label{fig:diagrama_multiagente_supervisor}
        \end{figure}

        The supervisor orchestrates the collaboration, integrates the findings from each specialist, and synthesizes the potentially divergent information into a single, coherent final answer. This framework is designed to mimic real-world expert collaboration and tests whether decomposing a problem and assigning its parts to dedicated specialists yields a more accurate result.
        
        
\section{Evaluation}

    The evaluation phase was designed to be automated, scalable, and objective, addressing the limitations of the first experimental cycle.

    \subsection{Evaluation Methodology}

        The core of the evaluation is an automated execution loop (detailed in Algorithm \ref{alg:execution_loop}) that runs each of the 33 questions through every combination of artifact (4 setups) and model (2 models), repeating each run three times to account for stochasticity.

        \begin{algorithm}[h]
        \caption{Experiment Execution Loop}
        \begin{algorithmic}[1]
        \Require questions, setups, models
        \Ensure results
        \Function{RunExperiment}{}
            \State $results \gets \{\}$
            \ForAll{$question \in questions$}
                \State $ground\_truth \gets question.ground\_truth$
                \ForAll{$setup \in setups$}
                    \ForAll{$model \in models$}
                        \For{$i \in 1 \dots 3$} \Comment{Execute 3 times for consistency}
                            \State $agent \gets \text{InitializeAgent}(setup, model)$
                            \State $response \gets agent.\text{ProcessQuestion}(question)$
                            \State $metrics \gets \text{EvaluateResponse}(response, ground\_truth)$
                            \State Store $metrics$ and $response$ in $results$
                        \EndFor
                    \EndFor
                \EndFor
            \EndFor
            \State \Return $\text{AggregateResults}(results)$
        \EndFunction
        \end{algorithmic}
        \label{alg:execution_loop}
        \end{algorithm}

        The quality of each generated response is assessed using the \textbf{LLM-as-a-Judge} approach. A powerful LLM (GPT-4) is prompted to act as an impartial evaluator, comparing the generated answer against the ground-truth answer. The judge decomposes both texts into atomic statements and classifies them to build a confusion matrix, from which the final metrics are calculated. The full prompt for the LLM-as-a-Judge can be found in Appendix \ref{code:llm-judge}.

    \subsection{Data Set Creation}

        The experiment utilizes a curated dataset developed in collaboration with domain experts.
        \begin{itemize}
            \item \textbf{Questions Dataset:} A set of 17 questions reflecting real-world information needs of well engineers. Each question is paired with a manually created, expert-validated "ground-truth" answer.
            \item \textbf{Knowledge Bases:} The artifacts were given access to three distinct, pre-processed knowledge sources from within the organization, vectorized for semantic search:
            \begin{itemize}
                \item \textbf{Learned Lessons:} A repository of learned lessons, best practices, and operational alerts.
                \item \textbf{HSE Alerts:} A collection of ESG alerts and incident reports.
                \item \textbf{Operational Reports:} A database of detailed daily operational reports from drilling rigs.
            \end{itemize}
        \end{itemize}

    \subsection{Evaluation Metrics}

        To provide a quantitative and objective assessment, the following information retrieval metrics, detailed in Section~\ref{sec:precision_recall_f1_review}, were calculated for each response based on the LLM-as-a-Judge's analysis:
        \begin{itemize}
            \item \textbf{Precision:} Measures the accuracy of the information presented in the generated answer. It is the ratio of correct statements (True Positives) to the total number of statements made. 
            \item \textbf{Recall:} Measures the completeness of the answer. It is the ratio of correct statements retrieved to the total number of statements available in the ground truth.
            \item \textbf{F1-Score:} The harmonic mean of Precision and Recall, providing a single, balanced measure of overall performance.
        \end{itemize}

    \subsection{Results}


        To ensure a robust evaluation and account for the inherent non-determinism of language models, each of the 17 questions in the dataset was processed three times for every model and configuration combination. This experimental design resulted in a total of 408 executions (17 questions $\times$ 2 models $\times$ 4 configurations $\times$ 3 runs). Each of the 408 generated answers was then compared to a ground truth answer to calculate performance metrics.

        The results presented in this section are derived from this set of runs. For each of the 136 unique combinations of question, model, and configuration, the best-performing run (out of three) was selected based on the F1-Score. The final metrics reported in Table~\ref{tab:performance_metrics} represent the average of these best-run scores across all 17 questions for each of the eight model-configuration pairs. This approach presents a clear view of the potential of each setup, with the F1-Score serving as the primary metric for performance evaluation.

        \begin{landscape}
            \begin{table}[H]
            \centering
            \caption{Detailed performance metrics by model and agent configuration. The best result for each metric is highlighted in bold and underlined. For the inferior model, the best result is only underlined.}
            \label{tab:performance_metrics}
            % \resizebox{\textwidth}{!}{%
            \begin{tabular}{@{}llcccccccccccc@{}}
                \toprule
                \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Configuration}} & \multicolumn{4}{c}{\textbf{F1-Score}} & \multicolumn{4}{c}{\textbf{Precision}} & \multicolumn{4}{c}{\textbf{Recall}} \\
                \cmidrule(l){3-6} \cmidrule(l){7-10} \cmidrule(l){11-14}
                & & Mean & Std. Dev. & Min & Max & Mean & Std. Dev. & Min & Max & Mean & Std. Dev. & Min & Max \\
                \midrule
                \multirow{4}{*}{GPT-4o} & Linear-Flow (Baseline) & 0.581 & 0.204 & 0.000 & 1.000 & 0.656 & 0.262 & 0.000 & 1.000 & 0.548 & 0.201 & 0.000 & 1.000 \\
                & Linear-Flow w/ Router & \textbf{\underline{0.702}} & 0.202 & 0.333 & 1.000 & \textbf{\underline{0.805}} & 0.185 & 0.400 & 1.000 & \textbf{\underline{0.674}} & 0.242 & 0.286 & 1.000 \\
                & Single-Agent & 0.643 & 0.213 & 0.364 & 1.000 & 0.751 & 0.198 & 0.400 & 1.000 & 0.618 & 0.240 & 0.294 & 1.000 \\
                & Multi-Agent & 0.664 & 0.214 & 0.286 & 1.000 & 0.746 & 0.221 & 0.286 & 1.000 & 0.630 & 0.231 & 0.286 & 1.000 \\
                \midrule
                \multirow{4}{*}{GPT-4o-mini} & Linear-Flow (Baseline) & 0.534 & 0.208 & 0.000 & 0.923 & 0.604 & 0.262 & 0.000 & 1.000 & 0.516 & 0.216 & 0.000 & 0.923 \\
                & Linear-Flow w/ Router & \underline{0.604} & 0.155 & 0.333 & 1.000 & 0.676 & 0.196 & 0.300 & 1.000 & \underline{0.602} & 0.206 & 0.267 & 1.000 \\
                & Single-Agent & 0.576 & 0.184 & 0.308 & 1.000 & 0.719 & 0.214 & 0.286 & 1.000 & 0.544 & 0.227 & 0.231 & 1.000 \\
                & Multi-Agent & 0.596 & 0.182 & 0.348 & 1.000 & \underline{0.687} & 0.198 & 0.400 & 1.000 & 0.578 & 0.201 & 0.235 & 1.000 \\
                \bottomrule
            \end{tabular}%
            % }
            \end{table}
        \end{landscape}


        \begin{figure}[h]
            \centering
            \includegraphics[width=0.7\textwidth]{images_exp2/bar_best_f1_by_model_and_configuration.png}
            \caption{Best F1-Score by model and configuration.}
            \label{fig:best_f1_by_model_and_configuration}
        \end{figure}

        
        The analysis of the results presented in Table~\ref{tab:performance_metrics} and visualized in Figure~\ref{fig:best_f1_by_model_and_configuration} reveals several insights into the performance of the different models and configurations. A primary observation is the consistent performance superiority of the GPT-4o model over its counterpart, GPT-4o-mini, across all tested configurations. The most capable configuration for GPT-4o, \textit{Linear-Flow w/ Router}, achieved a mean F1-Score of 0.702. This represents a significant performance uplift of approximately 16.2\% compared to the best score achieved by GPT-4o-mini (0.604), which was also with the \textit{Linear-Flow w/ Router} configuration. This gap underscores the impact that the model's reasoning and instruction-following capabilities have on the overall performance of the system.

        Furthermore, the results show that more complex configurations brought a notable improvement over the \textit{Linear-Flow (Baseline)} for both models. The \textit{Linear-Flow w/ Router} configuration emerged as the most effective architecture overall. For the superior GPT-4o model, this configuration boosted the mean F1-Score by a relative 20.8\% over the baseline (from 0.581 to 0.702). It also increased the mean precision by 22.7\% (from 0.656 to 0.805), indicating that the router is highly effective at selecting the correct reasoning path or tool, thereby reducing incorrect or irrelevant responses.

        While the \textit{Single-Agent} and \textit{Multi-Agent} configurations also outperformed the baseline, they did not reach the performance level of the router-enhanced linear flow. For GPT-4o, the \textit{Multi-Agent} setup (F1-Score 0.664) slightly outperformed the \textit{Single-Agent} (F1-Score 0.643), but both fell short of the \textit{Linear-Flow w/ Router}. This suggests that for the tasks in this experiment, the added complexity of reflective agent loops or multi-agent collaboration did not yield a proportional benefit over a more direct, intelligent tool-routing approach.

        A crucial aspect of the results is the high standard deviation observed across all configurations, typically around 0.20 for the F1-Score. The wide range between minimum and maximum scores indicates that performance is highly variable and question-dependent. This suggests that even the best-performing systems can fail completely on certain queries.



    \subsection{Discussion} \label{sec:exp2-discussion}
\chapter{Conclusions}

    The results of this study highlight the potential of multi-agent architectures based on LLMs in the O\&G sector, particularly in the domain of well engineering. The ability to process and respond to complex queries paves the way for a significant digital transformation in the area.

    Our comparative analysis of single-agent and multi-agent architectures, using GPT-3.5-turbo and GPT-4, reveals a detailed landscape of trade-offs between performance and economic efficiency. Multi-agent systems demonstrate 28\% greater factuality in question-and-answer (Q\&A) tasks, especially with GPT-4, compared to single-agent systems. However, they incur LLM costs that are, on average, 3.7 times higher due to the complexities of inter-agent communication. In contrast, single-agent systems excel in Text-to-SQL tasks, showing 15\% better performance than multi-agent setups. This cost-benefit dynamic requires careful consideration when implementing RAG in real-world scenarios, where accuracy and financial constraints must be balanced.

    We highlight several challenges encountered during our experiments, including issues with contextualization, the need for more refined information filtering, and the persistence of hallucinations. These challenges underscore the need for continuous research in areas such as domain-specific specialized models, advanced semantic search techniques, and hybrid architectures that combine the strengths of single-agent and multi-agent systems.

    The practical implications of this study extend beyond the O\&G sector. The insights gained here are applicable to any knowledge-intensive domain that deals with large volumes of technical data. By focusing on enhancing retrieval mechanisms, developing domain-specific LLMs, and optimizing interactions between agents and tools, we pave the way for more effective, reliable, and cost-efficient RAG solutions across various sectors.

    The main points of the study are as follows: multi-agent systems offer superior factuality in Q\&A tasks, albeit at a significantly higher cost. Single-agent architectures, on the other hand, excel at Text-to-SQL tasks. Despite the advantages, several challenges persist, including issues with contextualization, filtering, hallucination, and domain-specific vocabulary.

    Future research should focus on developing specialized models, advancing retrieval techniques, and exploring hybrid architectures. The lessons learned from this study have broader implications and can extend to other complex technical domains. By addressing the limitations identified in this study and embracing emerging trends in multi-agent systems and RAG technology, we can unlock their full potential, revolutionizing decision-making, knowledge management, and operational efficiency in complex industries worldwide.



    [CONCLUSIONS FROM EXPERIMENT 2 ARE YET TO BE WRITTEN HERE]

    [INCLUIR: COMENTAR QUE OS PRECOS DE LLM ESTAO CAINDO MTO, MODELOS PEQUENOS COM EXCELENTE DESEMPENHO, O QUE TORNA A ANALISE FINANCEIRA DO 1o CICLO OBSOLETA]
