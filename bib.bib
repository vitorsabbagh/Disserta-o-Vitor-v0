%
% This is file `coppe.bib'.
%
% Bibliographic references for the documentation.
%
% Copyright (C) 2011 CoppeTeX Project and any individual authors listed
% elsewhere in this file.
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License version 3 as
% published by the Free Software Foundation.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
% GNU General Public License version 3 for more details.
%
% You should have received a copy of the GNU General Public License
% version 3 along with this package (see COPYING file).
% If not, see <http://www.gnu.org/licenses/>.
%
% $URL: https://coppetex.svn.sourceforge.net/svnroot/coppetex/trunk/coppe.bib $
% $Id: coppe.bib 118 2008-10-18 14:17:06Z helano $
%
% Author(s): Vicente H. F. Batista
%            George O. Ainsworth Jr.
%

@BOOK{book-example,
   author = "R. Abraham and J. E. Marsden and T. Ratiu",
   title = "Manifolds, Tensor Analysis, and Applications",
   edition = "2",
   address = "New York",
   publisher = "Springer-Verlag",
   year = "1988",
}

@ARTICLE{article-example,
   author = "D. Iesan",
   title = {Existence Theorems in the Theory of Mixtures},
   journal = {Journal of Elasticity},
   volume = {42},
   number = {2},
   pages = {145--163},
   month = feb,
   year = 1996,
}

@TECHREPORT{techreport-exampleIn,
  author = "D. A. Garret",
  title = "The Microscopic Detection of Corrosion in Aluminum Aircraft Structures with Thermal Neutron Beams and Film Imaging Methods",
  type = "In: Report",
  number = "NBSIR 78-1434",
  institution = "National Bureau of Standards",
  address = "Washington, D.C.",
  year = "1977",
}

@TECHREPORT{techreport-example,
   author = "L. Maestrello",
   title = "Two-Point Correlations of Sound Pressure in the Far Field of a Jet: Experiment",
   type = "{NASA}",
   number = "{TM}~{X}-72835",
   year = 1976,
}

@INPROCEEDINGS{inproceedings-example,
   author = "M. E. Gurtin",
   title = "On the nonlinear theory of elasticity",
   booktitle = "Proceedings of the International Symposium on Continuum Mechanics and Partial Differential Equations: Contemporary Developments in Continuum Mechanics and Partial Differential Equations",
   pages = "237-253",
   address = "Rio de Janeiro",
   month = aug,
   year = 1977,
}


@INCOLLECTION{incollection-example,
   author = "S. C. Cowin",
   title = "Adaptive Anisotropy: An Example in Living Bone",
   booktitle = "Non-Classical Continuum Mechanics",
   series = "London Mathematical Society Lecture Note Series",
   volume = "122",
   publisher = "Cambridge University Press",
   pages = "174--186",
   year = 1987,
}

@INBOOK{inbook-example,
   author = "D. K. Edwards",
   title = "Thermal Radiation Measurements",
   address = "New York, USA",
   booktitle = "Measurements in Heat Transfer",
   publisher = "Hemisphere Publishing Corporation",
   editor = "E. R. G. Eckert and R. J. Goldstein",
   edition = 2,
   year = "1976",
   chapter = "10",
}

@MASTERSTHESIS{mastersthesis-example,
   author = "A. Tuntomo",
   title = "Transport Phenomena in a Small Particle with Internal Radiant Absorption",
   school = "University of California at Berkeley",
   type = "{Ph.D.} dissertation",
   address = "Berkeley, California, USA",
   year = 1990,
}

@PHDTHESIS{phdthesis-example,
   author = "Paes~Junior, H. R.",
   title = "Influ{\^e}ncia da Espessura da Camada Intr{\'i}nseca e Energia do Foton na Degrada{\c c}\~ao de C{\'e}lulas Solares de Sil{\'i}cio Amorfo Hidrogenado",
   school = "COPPE/UFRJ",
   address = "Rio de Janeiro, RJ, Brasil",
   type = "Tese de {D.Sc.}",
   year = 1994,
}








@article{Kar2023,
   abstract = {The scope of application of generative artificial intelligence (GAI) in industrial functions is gaining high prominence in academic and industrial discourses. In this article, we explore the usage of GAI and large language models (LLMs) in industrial applications. It promises myriad advantages such as greater engagement, cooperation and accessibility. LLMs like ChatGPT are able to evaluate unstructured queries, assess alternatives and offer actionable advice to users. It is being used to produce fast reports, flexible responses, environment scanning capabilities and insights that can enhance organisation flexibility in making better and quicker decisions, improving customer experiences and thereby augmenting firm profitability. This article offers a comprehensive review of scientific and grey literature in GAI and language models. The synthesis of complementary sources of information brings exciting perspectives in this fast evolving field. We provide directions surrounding future use of GAI as well as research directions for management researchers.},
   author = {Arpan Kumar Kar and P. S. Varsha},
   author2 = {Arpan Kumar Kar and P. S. Varsha and Shivakami Rajan},
   doi = {10.1007/s40171-023-00356-x},
   issn = {09740198},
   issue = {4},
   journal = {Global Journal of Flexible Systems Management},
   keywords = {Artificial intelligence,ChatGPT,Generative artificial intelligence,Industrial innovations,Large language models,Literature review,Technology enabled flexibility},
   month = {12},
   pages = {659-689},
   publisher = {Springer},
   title = {Unravelling the Impact of Generative Artificial Intelligence (GAI) in Industrial Applications: A Review of Scientific and Grey Literature},
   volume = {24},
   year = {2023},
}


@article{Li2023,
   author = {Cheng Li and Jindong Wang and Yixuan Zhang and Kaijie Zhu and Wenxin Hou and Jianxun Lian and Qiang Yang and Xing Xie},
   title = {Large Language Models Understand and Can Be Enhanced by Emotional Stimuli},
   year = {2023},
   doi = {10.48550/arXiv.2307.11760}
}
@misc{xi2023rise,
      title={The Rise and Potential of Large Language Model Based Agents: A Survey}, 
      author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
      year={2023},
      eprint={2309.07864},
      archivePrefix={arXiv},
  doi={10.48550/arXiv.2309.07864},
}

@article{Rane2023,
   abstract = {The advent of generative Artificial Intelligence (AI) models, exemplified by ChatGPT, has marked a transformative epoch for the building and construction industry, aligning seamlessly with the tenets of Industry 4.0, Industry 5.0, and Society 5.0. These expansive language models have evolved into indispensable tools, reshaping communication and decision-making processes within the industry. This exposition explores their contributions, opportunities, and challenges, illuminating their pivotal role in shaping the future of construction methodologies and societal engagements. Generative AI, represented by ChatGPT, has profoundly impacted the construction sector by enriching collaboration and knowledge dissemination. These AI models empower professionals with instant access to extensive information repositories, facilitating well-informed decision-making and nurturing innovation. Within the framework of Industry 4.0, ChatGPT streamlines automation and data-driven decision-making, optimizing operational efficiency and curbing costs. In Industry 5.0, these models enhance human-machine collaboration, emphasizing human-centric approaches, thereby stimulating creativity and problem-solving. Amidst the vast opportunities lie significant challenges. Ethical dilemmas concerning data privacy, bias mitigation, and AI accountability necessitate rigorous scrutiny. Furthermore, ensuring the inclusivity of these technologies in Society 5.0 demands bridging the digital divide and promoting digital literacy. Collaborative efforts among industry stakeholders, policymakers, and AI developers are imperative to unleash the complete potential of generative AI in the building and construction sector. The integration of expansive language models like ChatGPT in the building and construction industry promises a future defined by intelligent, ethical, and inclusive practices. Embracing these technologies responsibly is paramount, ensuring a harmonious coexistence between humans and AI in the evolving landscapes of Industry 4.0, Industry 5.0, and Society 5.0.},
   author = {Nitin Rane},
   doi = {10.2139/ssrn.4603221},
   journal = {SSRN Electronic Journal},
   publisher = {Elsevier BV},
   title = {ChatGPT and Similar Generative Artificial Intelligence (AI) for Building and Construction Industry: Contribution, Opportunities and Challenges of Large Language Models for Industry 4.0, Industry 5.0, and Society 5.0},
   year = {2023}
}

@article{Singh2023,
   author = {Ajay Singh and Tianxia Jia and Varun Nalagatla},
   doi = {10.2118/216267-MS},
   journal = {Day 2 Tue, October 03, 2023},
   month = {10},
   publisher = {SPE},
   title = {Generative AI Enabled Conversational Chatbot for Drilling and Production Analytics},
   url = {https://onepetro.org/SPEADIP/proceedings/23ADIP/2-23ADIP/D021S065R002/534485},
   year = {2023},
   specode = {SPE-216267-MS}
}

@article{Lin2021,
   abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
   author = {Stephanie Lin and Jacob Hilton Openai and Owain Evans},
   title = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
   url = {https://github.com/},
   year = {2021},
}
@article{Hadi2023,
   abstract = {Within the vast expanse of computerized language processing, a revolutionary entity known as Large Language Models (LLMs) has emerged, wielding immense power in its capacity to comprehend intricate linguistic patterns and conjure coherent and contextually fitting responses. Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as powerful tools for a wide range of tasks, including natural language processing (NLP), machine translation, and question-answering. This survey paper provides a comprehensive overview of LLMs, including their history, architecture, training methods, applications, and challenges. The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre-trained transformers (GPT). It then provides an overview of the history of LLMs, their evolution over time, and the different training methods that have been used to train them. The paper then discusses the wide range of applications of LLMs, including medical, education, finance, and engineering. It also discusses how LLMs are shaping the future of AI and how they can be used to solve real-world problems. The paper then discusses the challenges associated with deploying LLMs in real-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements. It also highlights techniques for enhancing the robustness and controllability of LLMs, and addressing bias, fairness, and generation quality issues. Finally, the paper concludes by highlighting the future of LLM research and the challenges that need to be addressed in order to make LLMs more reliable and useful. This survey paper is intended to provide researchers, practitioners, and enthusiasts with a comprehensive understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the field, this survey serves as a valuable resource for further advancements in the development and utilization of LLMs for a wide range of real-world applications. The GitHub repo for this project is available at https://github.com/anas-zafar/LLM-Survey Abstract Within the vast expanse of computerized language processing, a revolutionary entity known as Large Language Models (LLMs) has emerged, wielding immense power in its capacity to comprehend intricate linguistic patterns and conjure coherent and contextually fitting responses. Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as powerful tools for a wide range of tasks, including natural language processing (NLP), machine translation, and question-answering. This survey paper provides a comprehensive overview of LLMs, including their history, architecture, training methods, applications, and challenges. The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre-trained transformers (GPT). It then provides an overview of the history of LLMs, their evolution over time, and the different training methods that have been used to train them. The paper then discusses the wide range of applications of LLMs, including medical, education, finance, and engineering. It also discusses how LLMs are shaping the future of AI and how they can be used to solve real-world problems. The paper then discusses the challenges associated with deploying LLMs in real-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements. It also highlights techniques for enhancing the robustness and controllability of LLMs and addressing bias, fairness, and generation quality issues. Finally, the paper concludes by highlighting the future of LLM research and the challenges that need to be addressed in order to make LLMs more reliable and useful. This survey paper is intended to provide researchers, practitioners, and enthusiasts with a comprehensive understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the field, this survey serves as a valuable resource for further advancements in the development and utilization of LLMs for a wide range of real-world applications. The GitHub repo for this project is available at https://github.com/anas-zafar/LLM-Survey},
   author = {Muhammad Usman Hadi and qasem al tashi and Rizwan Qureshi and Abbas Shah and amgad muneer and Muhammad Irfan and Anas Zafar and Muhammad Bilal Shaikh and Naveed Akhtar and Jia Wu and Seyedali Mirjalili and Qasem Al-Tashi and Amgad Muneer},
   doi = {10.36227/techrxiv.23589741.v1},
   keywords = {AI chatbots 1 A Survey on Large Language Models: Applications,AI-enabled Tools,Bard,Bing,Challenges,ChatGPT,Co-pilots,Conversational AI,GPT,Generative AI,Index Terms Large Language Models,Limitations,Natural language processing,and Practical Usage},
   title = {A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage},
   url = {https://doi.org/10.36227/techrxiv.23589741.v1},
   year = {2023},
}
@article{Antoniak2016,
   author = {M. Antoniak and J. Dalgliesh and M. Verkruyse and J. Lo},
   doi = {10.2118/181015-MS},
   isbn = {9781613994597},
   journal = {Society of Petroleum Engineers - SPE Intelligent Energy International Conference and Exhibition},
   publisher = {Society of Petroleum Engineers},
   title = {Natural language processing techniques on oil and gas drilling data},
   year = {2016},
   specode = {SPE-181015-MS}
}
@article{Arefeen2024,
   author = {Adnan Arefeen and Biplob Debnath and Srimat Chakradhar},
   doi = {10.1016/j.nlp.2024.100065},
   journal = {Natural Language Processing Journal},
   keywords = {Cost-efficient LL,Domain-specific question answering,Large language models,Ms,Reinforcement learning,Text summarization},
   pages = {100065},
   title = {LeanContext: Cost-efficient domain-specific question answering using LLMs},
   volume = {7},
   url = {https://doi.org/10.1016/j.nlp.2024.100065},
   year = {2024},
}
@article{Castineira2018,
   author = {David Castiñeira and Robert Toronyi and Nansen Saleri},
   pages = {23-26},
   title = {Machine Learning and Natural Language Processing for Automated Analysis of Drilling and Completion Data},
   url = {http://onepetro.org/SPESATS/proceedings-pdf/18SATS/All-18SATS/SPE-192280-MS/1246545/spe-192280-ms.pdf/1},
   year = {2018},
   specode = {SPE-192280-MS},
   doi={10.2118/192280-MS}
}
@article{Li2024,
   author = {Junyou Li and Qin Zhang and Yangbin Yu and Qiang Fu and Deheng Ye},
   title = {More Agents Is All You Need},
   doi = {10.48550/arXiv.2402.05120},
   year = {2024},
}
@book{Russell2020,
   abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"–},
   author = {Stuart Russell},
   isbn = {0134610997},
   keywords = {Artificial intelligence},
   pages = {1-1115},
   publisher = {Pearson},
   address = {},
   title = {Artificial intelligence: a modern approach},
   year = {2020},
}
@book{Thomas2004,
   author = {José Eduardo Thomas},
   edition = {2nd},
   publisher = {Editora Interciência},
   title = {Fundamentos de Engenharia de Petróleo},
   address = {Av. Presidente Vargas 435, Rio de Janeiro, RJ - 20.077-900},
   year = {2004},
}
@book{Badiru2016,
   author = {Adedeji B. Badiru and Samuel O. Osisanya},
   doi = {10.1201/b13755},
   isbn = {9781420094268},
   month = {1},
   pages = {1-737},
   publisher = {CRC Press},
   address = {Boca Raton, FL 33487-2742},
   title = {Project Management for the Oil and Gas Industry: A World System Approach},
   year = {2016},
}
@article{Eckroth2023,
   author = {J Eckroth and M Gipson},
   doi = {10.2118/214888-MS},
   pages = {16-18},
   title = {Answering Natural Language Questions with OpenAI's GPT in the Petroleum Industry},
   url = {http://onepetro.org/SPEATCE/proceedings-pdf/23ATCE/3-23ATCE/D031S032R005/3301837/spe-214888-ms.pdf/1},
   year = {2023},
   specode = {SPE-214888-MS}
}

@article{Devlin2018,
   author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova Google and A I Language},
   isbn = {1810.04805v2},
   title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
   url = {https://github.com/tensorflow/tensor2tensor},
   year = {2018},
}

@article{Karpukhin2020,
   abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where em-beddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks. 1},
   author = {Vladimir Karpukhin and Barlas O˘ Guz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-Tau Yih and Facebook Ai},
   title = {Dense Passage Retrieval for Open-Domain Question Answering},
   url = {https://github.com/facebookresearch/DPR.},
   year = {2020},
}

@article{ Büttcher2016, 
author = {Stefan. Büttcher and Charles L. A. 1964- Clarke and Gordon V. Cormack}, isbn = {0262528878}, publisher = {MIT Press}, title = {Information retrieval: implementing and evaluating search engines}, year = {2016}, }

@misc{Lewis2020,
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-Tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   url = {https://github.com/huggingface/transformers/blob/master/},
   year = {2020},
}


@article{Deng2021,   
   author = {Xiang Deng and Ahmed Hassan Awadallah and Christopher Meek and Oleksandr Polozov and Huan Sun and Matthew Richardson},
   pages = {1337-1350},
   title = {Structure-Grounded Pretraining for Text-to-SQL},
   year = {2021},


   url={http://dx.doi.org/10.18653/v1/2021.naacl-main.105},
   DOI={10.18653/v1/2021.naacl-main.105},
   booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
   publisher={Association for Computational Linguistics},
}

@misc{OpenAImodels,
    author = {OpenAI}, 
   title = {Embeddings - OpenAI API},
   year = {2023},
   url = {https://platform.openai.com/docs/guides/embeddings},
}
@misc{OpenAI2024, 
    author = {OpenAI}, 
    journal = {https://platform.openai.com/docs/guides/embeddings (Accessed March 19, 2024)}, 
    month = {3}, 
    title = {Embeddings - OpenAI API}, 
    url = {https://platform.openai.com/docs/guides/embeddings}, 
    year = {2024}, 
}
@article{OpenAI2023,
archivePrefix = {arXiv},
arxivId = {2303.08774},
author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim{\'{o}}n Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, {\L}ukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, {\L}ukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and M{\'{e}}ly, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer{\'{o}}n and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, CJ and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
eprint = {2303.08774},
file = {:C$\backslash$:/Users/vitor/Downloads/2303.08774v6.pdf:pdf},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/LLMs},
pages = {1--100},
title = {{GPT-4 Technical Report}},
url = {http://arxiv.org/abs/2303.08774},
volume = {4},
year = {2023},
doi={10.48550/arXiv.2303.08774}
}
@article{Li2022,
   abstract = {Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.},
   author = {Huayang Li and Yixuan Su and Deng Cai and Yan Wang and Lemao Liu},
   month = {2},
   title = {A Survey on Retrieval-Augmented Text Generation},
   doi = {10.48550/arXiv.2202.01110},
   year = {2022},
}
@article{Liu2023,
   author = {Jiongnan Liu and Jiajie Jin and Zihan Wang and Jiehan Cheng and Zhicheng Dou and Ji-Rong Wen},
   month = {6},
   title = {RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit},
   url = {https://arxiv.org/abs/2306.05212v1},
   year = {2023},
}
@article{Zhao2023,
   abstract = {In this survey, we review methods that retrieve multimodal knowledge to assist and augment generative models. This group of works focuses on retrieving grounding contexts from external sources, including images, codes, tables, graphs, and audio. As multimodal learning and generative AI have become more and more impactful, such retrieval augmentation offers a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. We provide an in-depth review of retrieval-augmented generation in different modalities and discuss potential future directions. As this is an emerging field, we continue to add new papers and methods.},
   author = {Ruochen Zhao and Hailin Chen and Weishi Wang and Fangkai Jiao and Xuan Long Do and Chengwei Qin and Bosheng Ding and Xiaobao Guo and Minzhi Li and Xingxuan Li and Shafiq Joty},
   month = {3},
   title = {Retrieving Multimodal Information for Augmented Generation: A Survey},
   url = {http://arxiv.org/abs/2303.10868},
   year = {2023},
   doi={10.48550/arXiv.2303.10868},
   eprint={2303.10868},
   archivePrefix={arXiv},
}
@article{Deng2023,
   abstract = {We introduce MIND2WEB, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, MIND2WEB provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on MIND2WEB, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world web-sites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.},
   author = {Xiang Deng and Yu Gu and Boyuan Zheng and Shijie Chen and Samuel Stevens and Boshi Wang and Huan Sun and Yu Su},
   title = {MIND2WEB: Towards a Generalist Agent for the Web},
   url = {https://osu-nlp-group.github.io/Mind2Web},
   year = {2023},
}
@misc{Shah2024,
   author = {Bhoomi Shah},
   title = {Large Learning Models: The Rising Demand of Specialized LLM’s},
   url = {https://blogs.infosys.com/emerging-technology-solutions/artificial-intelligence/large-learning-models-the-rising-demand-of-specialized-llms.html},
   year = {2024},
   final_custom_field = { https://blogs.infosys.com/emerging-technology-solutions/artificial-intelligence/large-learning-models-the-rising-demand-of-specialized-llms.html (accessed 10 April 2024)}
}
@misc{Meena2023,
   author = {Shekhar Meena},
   title = {The Future of Large Language Models: Evolution, Specialization, and Market Dynamics},
   url = {https://www.linkedin.com/pulse/future-large-language-models-evolution-specialization-shekhar-meena/},
   year = {2023},
   final_custom_field = { https://www.linkedin.com/pulse/future-large-language-models-evolution-specialization-shekhar-meena/ (accessed 15 April 2024)}
}
@misc{Ghosh2023,
   author = {Bijit Ghosh},
   title = {Emerging Trends in LLM Architecture,},
   url = {https://medium.com/@bijit211987/emerging-trends-in-llm-architecture-a8897d9d987b},
   year = {2023},

   final_custom_field = { https://medium.com/@bijit211987/emerging-trends-in-llm-architecture-a8897d9d987b (accessed 5 March 2024)}
}
@article{Levenshtein1966,
   author = {Vladimir Levenshtein},
   journal = {Cybernetics and Control Theory},
   title = {Binary codes capable of correcting deletions, insertions, and reversals},
   volume = {10},
   year = {1966},
}
@misc{LangchainSelfQuery2023,
   author = {LangChain},
   title = {Self-query Retriever},
   url = {https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/},
   year = {2023},
   final_custom_field = {https://python.langchain.com/docs/ (accessed 15 March 2024)}
}
@misc{LangChain2024,
   author = {LangChain},
   journal = {https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter (Accessed March 19, 2024)},
   title = {Recursively split by character},
   url = {https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter},
   year = {2024},
}
@article{Bilbao2023,
   author = {Darriba Bilbao and Alexander Gelbukh and Alvaro Rodrigo and Minhyeok Lee},
   doi = {10.3390/MATH11102320},
   issn = {2227-7390},
   issue = {10},
   journal = {Mathematics 2023},
   keywords = {ChatGPT,GPT,LLM,creativity,generative pretrained transformers,hallucination,large language model},
   month = {5},
   pages = {2320},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {A Mathematical Investigation of Hallucination and Creativity in GPT Models},
   volume = {11},
   url = {https://www.mdpi.com/2227-7390/11/10/2320/htm https://www.mdpi.com/2227-7390/11/10/2320},
   year = {2023},
}
@article{Sun2023,
   author = {Weiwei Sun and Lingyong Yan and Xinyu Ma and Shuaiqiang Wang and Pengjie Ren and Zhumin Chen and Dawei Yin and Zhaochun Ren},
   title = {Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents},
   year = {2023},
      eprint={2304.09542},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2304.09542}, 
   doi={10.48550/arXiv.2304.09542}
}
@article{Carraro2024,
   author = {Diego Carraro},
   keywords = {CCS Concepts: • Information systems → Information retrieval diversity,Language models,Recommender systems Additional Key Words and Phrases: Recommender Systems, Large Language Models, Diversity, Re-ranking},
   title = {Enhancing Recommendation Diversity by Re-ranking with Large Language Models},
   year = {2024},
   doi = {10.48550/arXiv.2401.11506},
   final_custom_field = {(preprint; last revised 17 June 2024)}

}
@misc{Qdrant2024,
   author = {Qdrant},
   title = {Qdrant Documentation},
   url = {https://qdrant.tech/documentation},
   year = {2024},
}
@article{Wu2023,
   author = {Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Awadallah and Ryen W White and Doug Burger and Chi Wang},
   title = {AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
   year = {2023},
doi={10.48550/arXiv.2308.08155},
      eprint={2308.08155},
      archivePrefix={arXiv},
}
@misc{Pal2024,
   author = {Soumen Pal and Manojit Bhattacharya and Sang Soo Lee and Chiranjib Chakraborty},
   doi = {10.1007/s10439-023-03306-x},
   issn = {15739686},
   issue = {3},
   journal = {Annals of Biomedical Engineering},
   keywords = {Biomedical engineering,ChatBot,ChatGPT,Large language model},
   month = {3},
   pages = {451-454},
   pmid = {37428337},
   publisher = {Springer},
   title = {A Domain-Specific Next-Generation Large Language Model (LLM) or ChatGPT is Required for Biomedical Engineering and Research},
   volume = {52},
   year = {2024},
}
@misc{Hatzius2023,
   author = {Jan Hatzius and Joseph Briggs and Devesh Kodnani and Giovanni Pierdomenico},
   title = {The Potentially Large Effects of Artiﬁcial Intelligence on
Economic Growth},
   year = {2023},
   journal = {Goldman Sachs - Global Economics Analyst},
}
@article{Dellacqua2023,
   author = {Fabrizio Dellacqua and Acqua Saran and Rajendran Edward Mcfowland and Lisa Krayer and Ethan Mollick and François Candelon and Hila Lifshitz-Assaf and Karim R Lakhani and Katherine C Kellogg},
   
   title = {Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality},
journal={Harvard Business School: Technology and Operations Management Unit Working Paper Series},
   url = {https://ssrn.com/abstract=4573321},
   year = {2023},
doi={10.2139/ssrn.4573321}
}
@book{Liddy2001,
    author = {Elizabeth Liddy},
    publisher = {Encyclopedia of Library and Information Science},
    journal = {Encyclopedia of Library and Information Science},
    title = {{Natural Language Processing}},
    url = {https://surface.syr.edu/istpub},
    address = {},
    year = {2001}
}
@article{Gudala2021,
author = {Gudala, Manojkumar and Naiya, Tarun Kumar and Govindarajan, Suresh Kumar},
doi = {10.2118/203824-PA},
issn = {1086055X},
journal = {SPE Journal},
month = {apr},

volume = {26},
number = {2},
pages = {1050-1071},

publisher = {Society of Petroleum Engineers (SPE)},
title = {{Remediation of heavy oil transportation problems via pipelines using biodegradable additives: An experimental and artificial intelligence approach}},
year = {2021},
specode = {SPE-203824-PA}
}
@inproceedings{Phan2022,
abstract = {Selection of a safe mud weight is crucial in drilling operations to reduce costly wellbore-instability problems. Advanced physics models and their analytical solutions for mud-weight-window computation are available but still demanding in terms of central-processing-unit (CPU) time. This paper presents an artificial-intelligence (AI) solution for predicting time-dependent safe mud-weight windows and very refined polar charts in real time. The AI agents are trained and tested on data generated from a time-dependent coupled analytical solution (poroelastic) because numerical solutions are prohibitively slow. Different AI techniques, including linear regression, decision tree, random forest, extra trees, adaptive neuro fuzzy inference system (ANFIS), and neural networks are evaluated to select the most suitable one. The results show that neural networks have the best performances and are capable of predicting time-dependent mud-weight windows and polar charts as accurately as the analytical solution, with 1/1,000 of the computer time needed, making them very applicable to real-time drilling operations. The trained neural networks achieve a mean squared error (MSE) of 0.0352 and a coefficient of determination (R2) of 0.9984 for collapse mud weights, and an MSE of 0.0072 and an R2 of 0.9998 for fracturing mud weights on test data sets. The neural networks are statistically guaranteed to predict mud weights that are within 5{\%} and 10{\%} of the analytical solutions with probability up to 0.986 and 0.997, respectively, for collapse mud weights, and up to 0.9992 and 0.9998, respectively, for fracturing mud weights. Their time performances are significantly faster and less demanding in computing capacity than the analytical solution, consistently showing three-orders-of-magnitude speedups in computational speed tests. The AI solution is integrated into a deployed wellbore-stability analyzer, which is used to demonstrate the AI's performances and advantages through three case studies.},
author = {Phan, Dung T. and Liu, Chao and AlTammar, Murtadha J. and Han, Yanhui and Abousleiman, Younane N.},
booktitle = {SPE Journal},
doi = {10.2118/206748-PA},
file = {:C$\backslash$:/Users/vitor/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Phan et al. - 2022 - Application of Artificial Intelligence To Predict Time-Dependent Mud-Weight Windows in Real Time.pdf:pdf},
issn = {1086055X},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/SPE},
month = {feb},
number = {1},
pages = {39--59},
publisher = {Society of Petroleum Engineers (SPE)},
title = {{Application of Artificial Intelligence To Predict Time-Dependent Mud-Weight Windows in Real Time}},
volume = {27},
year = {2022},
specode = {SPE-206748-PA}
}
@article{He2022,
author = {He, Jincong and Tang, Meng and Hu, Chaoshun and Tanaka, Shusei and Wang, Kainan and Wen, Xian Huan and Nasir, Yusuf},
doi = {10.2118/203951-PA},
file = {:C$\backslash$:/Users/vitor/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2022 - Deep Reinforcement Learning for Generalizable Field Development Optimization.pdf:pdf},
issn = {1086055X},
journal = {SPE Journal},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/SPE},
month = {feb},
number = {1},
pages = {226--245},
publisher = {Society of Petroleum Engineers (SPE)},
title = {{Deep Reinforcement Learning for Generalizable Field Development Optimization}},
volume = {27},
year = {2022},
specode = {SPE-203951-PA}
}
@techreport{Qu2023,
author = {Qu, Hai and Chen, Xiangjun and Hong, Jun and Xu, Yang and Li, Chengying and Li, Zhelun and Liu, Ying},
booktitle = {SPE Journal},
file = {:C$\backslash$:/Users/vitor/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qu et al. - 2023 - Experimental and 3D Numerical Investigation on Proppant Distribution in a Perforation Cluster Involving the Artificia.pdf:pdf},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/SPE},
pages = {1650},
title = {{Experimental and 3D Numerical Investigation on Proppant Distribution in a Perforation Cluster Involving the Artificial Neural Network Prediction}},
url = {http://onepetro.org/SJ/article-pdf/28/04/1650/3186783/spe-214316-pa.pdf/1},
year = {2023},
specode = {SPE-214316-PA}
}
@techreport{Zhang2023,
author = {Zhang, Daowei and Li, Heng},
booktitle = {SPE Journal},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/SPE},
title = {{Efficient Surrogate Modeling Based on Improved Vision Transformer Neural Network for History Matching}},
url = {http://onepetro.org/SJ/article-pdf/28/06/3046/3332534/spe-215856-pa.pdf},
volume = {3046},
year = {2023},
specode = {SPE-215856-PA}
}
@techreport{Cedeno2023,
author = {Cede{\~{n}}o, M A and Enriquez-Fernandez, A and Moncayo-Riascos, I and Cort{\'{e}}s, F B and Franco, C A},
booktitle = {SPE Journal},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/SPE},
pages = {1470},
title = {{Artificial Intelligence Applied to Nanotechnology in the Oil and Gas Industry: Study of Asphaltene Adsorption Using Nanoparticles}},
url = {http://onepetro.org/SJ/article-pdf/28/03/1470/3120733/spe-212847-pa.pdf/1},
year = {2023},
specode = {SPE-212847-PA}
}
@techreport{OLD_Gohari2024_OLD,
author = {Gohari, Mohammad and {Emami Niri}, Mohammad and Sadeghnejad, Saeid and Ghiasi-Freez, Javad},
booktitle = {January 2024 SPE Journal},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/SPE},
title = {{Synthetic Graphic Well Log Generation Using an Enhanced Deep Learning Workflow: Imbalanced Multiclass Data, Sample Size, and Scalability Challenges}},
url = {http://onepetro.org/SJ/article-pdf/29/01/1/3358626/spe-217466-pa.pdf/1},
volume = {1},
year = {2024},
doi={10.2118/217466-PA},
specode = {SPE-217466-PA}
}
@article{Gohari2024,
   author = {Mohammad Saleh Jamshidi Gohari and Mohammad Emami Niri and Saeid Sadeghnejad and Javad Ghiasi-Freez},
   doi = {10.2118/217466-PA},
   issn = {1086055X},
   issue = {1},
   journal = {SPE Journal},
   pages = {1-20},
   title = {Synthetic Graphic Well Log Generation Using an Enhanced Deep Learning Workflow: Imbalanced Multiclass Data, Sample Size, and Scalability Challenges},
   volume = {29},
   url = {http://onepetro.org/SJ/article-pdf/29/01/1/3358626/spe-217466-pa.pdf/1},
   year = {2024},
specode = {SPE-217466-PA}
}

@article{Bravo2014,
author = {Bravo, C{\'{e}}sar and Saputelli, Luigi and Rivas, Francklin and P{\'{e}}rez, Anna Gabriela and Nikolaou, Michael and Zangl, Georg and {De Guzm{\'{a}}n}, Neil and Mohaghegh, Shahab and Nunez, Gustavo},
journal = {SPE Journal},
doi = {10.2118/150314-pa},
issn = {1086055X},
keywords = {C{\'{e}} sar Bravo, Halliburton,Francklin Rivas and Anna Gabriela P{\'{e}} rez, Universidad de Los Andes,Georg Zangl, Fractured Reservoir Dynamics,Luigi Saputelli, Frontender Corporation,Michael Nikolaou, University of Houston,Neil de Guzm{\'{a}} n, Intelligent Agent Corporation,Shahab Mohaghegh, West Virginia University,and Gustavo Nunez, Schlumberger},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/SPE},
number = {4},
pages = {547-563},
title = {State of the art of artificial intelligence and predictive analytics in the E{\&}P industry: A technology survey},
url = {http://onepetro.org/SJ/article-pdf/19/04/547/2099035/spe-150314-pa.pdf/1},
volume = {19},
year = {2014},
specode = {SPE-150314-PA}
}
@inproceedings{Mosser2024,
author = {Mosser, L. and Aursand, P. and Brakstad, K. S. and Lehre, C. and Myhre-Bakkevig, J.},
booktitle = {Day 1 Wed, April 17, 2024},
doi = {10.2118/218439-MS},
mendeley-groups = {Disserta{\c{c}}{\~{a}}o/LLMs},
month = {apr},
publisher = {SPE},
title = {{Exploration Robot Chat: Uncovering Decades of Exploration Knowledge and Data with Conversational Large Language Models}},
url = {https://onepetro.org/SPEBERG/proceedings/24BERG/1-24BERG/D011S002R006/544177},
year = {2024},
specode = {SPE-218439-MS}
}
@article{Rahmani2021,
   abstract = {Recent advances in sensor networks and the Internet of Things (IoT) technologies have led to the gathering of an enormous scale of data. The exploration of such huge quantities of data needs more efficient methods with high analysis accuracy. Artificial Intelligence (AI) techniques such as machine learning and evolutionary algorithms able to provide more precise, faster, and scalable outcomes in big data analytics. Despite this interest, as far as we are aware there is not any complete survey of various artificial intelligence techniques for big data analytics. The present survey aims to study the research done on big data analytics using artificial intelligence techniques. The authors select related research papers using the Systematic Literature Review (SLR) method. Four groups are considered to investigate these mechanisms which are machine learning, knowledge-based and reasoning methods, decision-making algorithms, and search methods and optimization theory. A number of articles are investigated within each category. Furthermore, this survey denotes the strengths and weaknesses of the selected AI-driven big data analytics techniques and discusses the related parameters, comparing them in terms of scalability, efficiency, precision, and privacy. Furthermore, a number of important areas are provided to enhance the big data analytics mechanisms in the future.},
   author = {Amir Masoud Rahmani and Elham Azhir and Saqib Ali and Mokhtar Mohammadi and Omed Hassan Ahmed and Marwan Yassin Ghafour and Sarkar Hasan Ahmed and Mehdi Hosseinzadeh},
   doi = {10.7717/peerj-cs.488},
   issn = {23765992},
   journal = {PeerJ Computer Science},
   keywords = {Artificial intelligence,Big data,Machine learning,Methods,Systematic literature review},
   month = {4},
   pages = {1-28},
   publisher = {PeerJ Inc.},
   title = {Artificial intelligence approaches and mechanisms for big data analytics: a systematic study},
   volume = {7},
   year = {2021},
}
@misc{Iske2005,
   abstract = {Purpose - In this paper the aim is to describe the role that question-driven knowledge exchange systems can play in the transfer of knowledge between people and to describe the conditions to be fulfilled for successful implementation. Design/methodology/approach - The conclusions in this paper are based on interpretation of results of case studies. These are combined with literature research. Findings - The major conclusion of the work is that question and answer (Q&A) systems are more promising than traditional Yellow Pages systems. However, some challenges remain the same, especially those related to motivating people to ask (the right) questions. Research limitations/implications - The authors believe that further study would be helpful to better understand the causal relationships between the success of a Q&A-driven knowledge system and the context where they are applied. More case studies and a fundamental study of the types of knowledge and organizations that could benefit from this approach would help people to make better decisions when considering the implementation of a Q&A system. Practical implications - The aim of this work is to help people make better decisions when they consider the implementation of a system that connects people with a knowledge question to people with the relevant knowledge. It helps them to understand whether such a system can add value at all and, if so, how to increase the probability of success. Originality/value - As far as is known, there has not been a study so far, explicitly focusing on this type of system and the comparison of the application of Q&A systems to "traditional" Yellow Pages. The application of scenario-thinking to this field is also new. © Emerald Group Publishing Limited.},
   author = {Paul Iske and Willem Boersma},
   doi = {10.1108/13673270510583018},
   issn = {13673270},
   issue = {1},
   journal = {Journal of Knowledge Management},
   keywords = {Culture (sociology),Knowledge management,Return on investment},
   pages = {126-145},
   title = {Connected brains. Question and answer systems for knowledge sharing: Concepts, implementation and return on investment},
   volume = {9},
   year = {2005},
}
@inproceedings{Treude2011,
author = {Treude, Christoph and Barzilay, Ohad and Storey, Margaret-Anne},
title = {How do programmers ask and answer questions on the web? (NIER track)},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985907},
doi = {10.1145/1985793.1985907},
abstract = {Question and Answer (Q&A) websites, such as Stack Overflow, use social media to facilitate knowledge exchange between programmers and fill archives with millions of entries that contribute to the body of knowledge in software development. Understanding the role of Q&A websites in the documentation landscape will enable us to make recommendations on how individuals and companies can leverage this knowledge effectively. In this paper, we analyze data from Stack Overflow to categorize the kinds of questions that are asked, and to explore which questions are answered well and which ones remain unanswered. Our preliminary findings indicate that Q&A websites are particularly effective at code reviews and conceptual questions. We pose research questions and suggest future work to explore the motivations of programmers that contribute to Q&A websites, and to understand the implications of turning Q&A exchanges into technical mini-blogs through the editing of questions and answers.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {804–807},
numpages = {4},
keywords = {q&a, questions, social media, stack overflow},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@article{Shinn2023,
   author = {Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
   month = {3},
   title = {Reflexion: Language Agents with Verbal Reinforcement Learning},
   url = {http://arxiv.org/abs/2303.11366},
   year = {2023},
      eprint={2303.11366},
      archivePrefix={arXiv},

}
@article{Qin2022,
   author = {Bowen Qin and Binyuan Hui and Lihan Wang and Min Yang and Jinyang Li and Binhua Li and Ruiying Geng and Rongyu Cao and Jian Sun and Luo Si and Fei Huang and Yongbin Li},
   month = {8},
   title = {A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions},
   url = {http://arxiv.org/abs/2208.13629},
   year = {2022},
  doi={10.48550/arXiv.2208.13629},
  eprint={2208.13629},
  archivePrefix={arXiv}
}
@misc{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Google Brain and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   title = {Attention Is All You Need},
  eprint={1706.03762},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/1706.03762}, 
  year={2017},
  doi={10.48550/arXiv.1706.03762}
}
@misc{Petrobras2024,
   author = {Petrobras},
   title = {Petrobras 2023 Results - Unaudited Condensed Consolidated Interim Financial Statements},
   url = {https://www.investidorpetrobras.com.br/en/results-and-announcements/results-center/},
   year = {2024},
}
@inproceedings{Khan2024,
   author = {Abdul Muqtadir Khan},
   doi = {10.2523/IPTC-24228-MS},
   isbn = {9781959025184},
   journal = {International Petroleum Technology Conference, IPTC 2024},
   publisher = {International Petroleum Technology Conference (IPTC)},
   title = {Digital Integration Scope in Fracturing: Leveraging Domain Knowledge for Intelligent Advisors-Part I},
   year = {2024},
}
@inproceedings{Tharayil2024,
   author = {Sarafudheen M. Tharayil and Badr Aldhalaan and Basim Dossary},
   doi = {10.2118/219324-MS},
   isbn = {9781959025405},
   booktitle = {Society of Petroleum Engineers - GOTECH Conference 2024},
   keywords = {ERP,Fine Tuning,Generative AI,LLM,Machine Learning,SAP},
   publisher = {Society of Petroleum Engineers},
   title = {A Language Model for Natural Language Interaction with Transactional Screens in the Oil and Gas Industry},
   year = {2024},
   specode = {SPE-219324-MS},
}







@article{Ashok_Urlana_2024,
 annote = {TL;DR: The objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context and examine 68 industry papers to address these questions and derive meaningful conclusions. },
 author = {Ashok Urlana and Charaka Vinayak Kumar and Ajeet Kumar Singh and Bala Mallikarjunarao Garlapati and Srinivasa Rao Chalamala and Rahul Mishra},
 doi = {10.48550/arxiv.2402.14558},
 journal = {arXiv.org},
 publication_type = {article},
 title = {LLMs with Industrial Lens: Deciphering the Challenges and Prospects - A Survey},
 volume = {abs/2402.14558},
 year = {2024}
}

@inproceedings{E_Ferrigno_2024,
 annote = {TL;DR: This paper introduces an AI model leveraging Large Language Models (LLM) to classify log records and WITSML data in real-time, enhancing decision-making efficiency by 50 times and reducing search time from hours to minutes in well construction control rooms. },
 author = {E. Ferrigno and E. Davidsson},
 doi = {10.2118/220798-ms},
 publication_type = {inproceedings},
 title = {Revolutionizing Drilling Operations: Next-gen Llm-AI for Real-time Support in Well Construction Control Rooms},
 year = {2024}
}

@article{Ganesh_Shankar_Gowekar_2024,
 annote = {TL;DR: The oil and gas industry is transforming with AI and ML, leveraging data analysis for actionable insights, optimizing operations, and improving safety, efficiency, and decision-making through real-time predictive analytics and data-driven strategies. },
 author = {Ganesh Shankar Gowekar},
 doi = {10.30574/wjarr.2024.23.3.2722},
 journal = {World Journal Of Advanced Research and Reviews},
 number = {3},
 pages = {1234-1238},
 publication_type = {article},
 title = {How oil and gas industry are transforming with AI and ML},
 volume = {23},
 year = {2024}
}

@misc{Gaurav_Singh_2024,
 annote = {TL;DR: LLM-Assisted Inference enhances decision-making in optimization by illuminating key decision variables and articulating contextual trade-offs in complex multi-objective optimization solutions. },
 author = {Gaurav Singh and Kavitesh Kumar Bali},
 doi = {10.48550/arxiv.2405.07212},
 publication_type = {misc},
 title = {Enhancing Decision-Making in Optimization through LLM-Assisted
  Inference: A Neural Networks Perspective},
 url = {https://arxiv.org/pdf/2405.07212},
 year = {2024}
}

@inproceedings{Huan_Wang_2023,
 annote = {TL;DR: A new solution: LLMs empowered by domain-specific knowledge base (LLM-DSKB) is explored, which can provide more accurate, specific, and industrially relevant results compared to traditional LLMs, and significantly enhancing the efficiency, effectiveness, and quality of IEOM. },
 author = {Huan Wang and Yan-Fu Li},
 doi = {10.1109/srse59585.2023.10336112},
 pages = {474-479},
 publication_type = {inproceedings},
 title = {Large Language Model Empowered by Domain-Specific Knowledge Base for Industrial Equipment Operation and Maintenance},
 year = {2023}
}

@article{Marcelo_dos_Santos_PÃ³voas_2025,
 annote = {TL;DR: This study examines the integration of machine learning and artificial intelligence in the oil and gas industry, highlighting trends and patterns in forecasting and optimization, and demonstrating the potential for enhanced efficiency, reduced costs, and increased safety through AI and ML applications. },
 author = {Marcelo dos Santos PÃ³voas and J. Moreira and GÃ­lson Brito Alves Lima and Severino VirgÃ­nio Martins Neto},
 doi = {10.34140/bjbv6n4-066},
 journal = {Brazilian Journal of Business},
 number = {4},
 pages = {e76387-e76387},
 publication_type = {article},
 title = {Applications of machine learning and artificial intelligence in the oil and gas industry: a study of keywords and research results},
 volume = {6},
 year = {2025}
}

@article{Michael_Yi_2024,
 annote = {TL;DR: Large language models can significantly improve well construction planning and real-time operation by providing quick and accurate information retrieval. },
 author = {Michael Yi and Kamil Ceglinski and Pradeepkumar Ashok and Michael Behounek and Spencer White and Trey Peroyea and Taylor Thetford},
 doi = {10.2118/217700-ms},
 publication_type = {article},
 title = {Applications of Large Language Models in Well Construction Planning and Real-Time Operation},
 year = {2024}
}

@article{Myriam_Amour_2024,
 annote = {TL;DR: Researchers propose a novel framework using Text-to-SQL with Large Language Models to empower drilling engineers with efficient data access, transforming complex data operations into seamless processes requiring no special expertise, enhancing data analysis and decision-making in the oil and gas industry. },
 author = {Myriam Amour and Benedictus Kent Rachmat and Agustin Soriano Rementeria and Valerian Guillot and Ekaterina Millan},
 doi = {10.2118/221862-ms},
 publication_type = {article},
 title = {Empowering Drilling and Optimization with Generative AI},
 year = {2024}
}

@inproceedings{Syatria_Kumala_Putra_2024,
 annote = {TL;DR: This technical paper integrates machine learning, AI, and data governance to optimize well design and operations in the oil and gas industry, enhancing decision-making, efficiency, and safety through the Company's DreamWell tool and standardization practices. },
 author = {Syatria Kumala Putra and Mohammad Faisal Umar and Ridwan Sangaji and Andi Nugroho and Ichsan Farandi Dananjaya and Ardilla Sufri and Rafli Nur Prima},
 doi = {10.2118/219613-ms},
 publication_type = {inproceedings},
 title = {Optimizing Well Design and Operation with Technology: The Role of Machine Learning, Artificial Intelligence, Data Governance and Standardization},
 year = {2024}
}


@misc{li2025generationjudgmentopportunitieschallenges,
      title={From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge}, 
      author={Dawei Li and Bohan Jiang and Liangjie Huang and Alimohammad Beigi and Chengshuai Zhao and Zhen Tan and Amrita Bhattacharjee and Yuxuan Jiang and Canyu Chen and Tianhao Wu and Kai Shu and Lu Cheng and Huan Liu},
      year={2025},
      eprint={2411.16594},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.16594}, 
}

@misc{sollenberger2024llm4vvexploringllmasajudgevalidation,
      title={LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites}, 
      author={Zachariah Sollenberger and Jay Patel and Christian Munley and Aaron Jarmusch and Sunita Chandrasekaran},
      year={2024},
      eprint={2408.11729},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.11729}, 
}

@misc{shi2025optimizationbasedpromptinjectionattack,
      title={Optimization-based Prompt Injection Attack to LLM-as-a-Judge}, 
      author={Jiawen Shi and Zenghui Yuan and Yinuo Liu and Yue Huang and Pan Zhou and Lichao Sun and Neil Zhenqiang Gong},
      year={2025},
      eprint={2403.17710},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.17710}, 
}

@misc{jeong2025agentasjudgefactualsummarizationlong,
      title={Agent-as-Judge for Factual Summarization of Long Narratives}, 
      author={Yeonseok Jeong and Minsoo Kim and Seung-won Hwang and Byung-Hak Kim},
      year={2025},
      eprint={2501.09993},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.09993}
}

@article{Gu2025,
   abstract = {Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of "LLM-as-a-Judge," where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.},
   author = {Jiawei Gu and Xuhui Jiang and Zhichao Shi and Hexiang Tan and Xuehao Zhai and Chengjin Xu and Wei Li and Yinghan Shen and Shengjie Ma and Honghao Liu and Saizhuo Wang and Kun Zhang and Yuanzhuo Wang and Wen Gao and Lionel Ni and Jian Guo},
   month = {3},
   title = {A Survey on LLM-as-a-Judge},
   url = {http://arxiv.org/abs/2411.15594},
   year = {2025}
}
@article{Zheng2023,
   abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
   author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
   month = {12},
   title = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
   url = {http://arxiv.org/abs/2306.05685},
   year = {2023}
}

@misc{li2024llmsasjudgescomprehensivesurveyllmbased,
      title={LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods}, 
      author={Haitao Li and Qian Dong and Junjie Chen and Huixue Su and Yujia Zhou and Qingyao Ai and Ziyi Ye and Yiqun Liu},
      year={2024},
      eprint={2412.05579},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.05579}, 
}

@article{Kim2024,
   abstract = {Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at https://kaistai.github.io/prometheus/.},
   author = {Seungone Kim and Jamin Shin and Yejin Cho and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo},
   month = {3},
   title = {Prometheus: Inducing Fine-grained Evaluation Capability in Language Models},
   url = {http://arxiv.org/abs/2310.08491},
   year = {2024}
}
@article{hevner2007three,
  title={A three cycle view of design science research},
  author={Hevner, Alan R},
  journal={Scandinavian journal of information systems},
  volume={19},
  number={2},
  pages={4},
  year={2007}
}

@book{Oswald2023,
   author = {Maria Luiza Magalhães Bastos Oswald and Adriana Hoffmann and Dagmar de Mello e Silva and Dilton Ribeiro Couto Junior and Helenice Mirabelli Cassino Ferreira},
   title={Metodologias De Pesquisa Online: Investigando Em Rede Com o Outro},
   edition = {1},
   address = {Rio de Janeiro},
   publisher = {Ayvu Editora},
   year = {2023}
}


@misc{lin2024parrotefficientservingllmbased,
      title={Parrot: Efficient Serving of LLM-based Applications with Semantic Variable}, 
      author={Chaofan Lin and Zhenhua Han and Chengruidong Zhang and Yuqing Yang and Fan Yang and Chen Chen and Lili Qiu},
      year={2024},
      eprint={2405.19888},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.19888}, 
}

@misc{tzachristas2024creatingllmbasedaiagenthighlevel,
      title={Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs}, 
      author={Ioannis Tzachristas},
      year={2024},
      eprint={2412.13233},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2412.13233}, 
}
@misc{mailach2025themesbuildingllmbasedapplications,
      title={Themes of Building LLM-based Applications for Production: A Practitioner's View}, 
      author={Alina Mailach and Sebastian Simon and Johannes Dorn and Norbert Siegmund},
      year={2025},
      eprint={2411.08574},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2411.08574}, 
}

@article{HUANG2024120923,
      title = {Strengthening LLM ecosystem security: Preventing mobile malware from manipulating LLM-based applications},
      journal = {Information Sciences},
      volume = {681},
      pages = {120923},
      year = {2024},
      issn = {0020-0255},
      doi = {https://doi.org/10.1016/j.ins.2024.120923},
      url = {https://www.sciencedirect.com/science/article/pii/S0020025524008375},
      author = {Lu Huang and Jingfeng Xue and Yong Wang and Junbao Chen and Tianwei Lei},
      keywords = {LLM security, LLM ecosystem, Android malware detection, Contrastive learning, Model aging},
}
@misc{ge2023,
      title={LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem}, 
      author={Yingqiang Ge and Yujie Ren and Wenyue Hua and Shuyuan Xu and Juntao Tan and Yongfeng Zhang},
      year={2023},
      eprint={2312.03815},
      archivePrefix={arXiv},
      primaryClass={cs.OS},
      url={https://arxiv.org/abs/2312.03815}, 
}

@misc{GrandViewResearch2025,
  author = {GrandViewResearch},
  title = {Large Language Models Market Size, Share and Trends Analysis Report},
  year = {2025},
  month = {6},
  url = {https://www.grandviewresearch.com/industry-analysis/large-language-model-llm-market-report},
  note = {Accessed: 2025-06-30}
}

@article{Kaddour2023,
  author       = {Jean Kaddour and Joshua Harris and Maximilian Mozes and Herbie Bradley and Roberta Raileanu and Robert McHardy},
  title        = {A Comprehensive Overview of Large Language Models},
  journal      = {arXiv preprint arXiv:2307.06435},
  year         = {2023},
  url          = {https://arxiv.org/abs/2307.06435}
}

@misc{V7Labs2025,
  author = {V7 Labs},
  title = {11 Best Applications of Large Language Models (LLMs) [2025]},
  year = {2025},
  month = {5},
  url = {https://www.v7labs.com/blog/best-llm-applications},
  note = {Accessed: 2025-06-30}
}

@misc{KeywordsAI2025,
  author = {Keywords AI},
  title = {My top 10 LLM research papers in 2024},
  year = {2025},
  month = {1},
  url = {https://www.keywordsai.co/blog/top-10-llm-research-papers},
  note = {Accessed: 2025-06-30}
}