
% \chapter{Revisão de Literatura} 

%     \section{IA na indústria de petroleo}
    
        % O uso de IA na indústria de Exploração e Produção (E\&P) de petróleo tem sido extenso. Nas últimas décadas, a maioria das aplicações de IA na indústria envolve mineração de dados e redes neurais \cite{Bravo2014}. Um exemplo é o trabalho de \cite{Gudala2021} sobre a otimização das propriedades de fluxo de óleo pesado, utilizando redes neurais para otimizar parâmetros que influenciam o fluxo.
        
        % Uma contribuição significativa vem da integração do conhecimento do domínio com estruturas digitais para aprimorar a tomada de decisões em tratamentos de fraturamento. Essa abordagem, demonstrada por \cite{Khan2024}, utiliza aprendizado de máquina para melhorar a eficiência operacional e reduzir custos.
        
        % Outro desenvolvimento foi um fluxo de trabalho de aprendizado profundo proposto por \cite{Gohari2024}, com a geração de logs gráficos sintéticos de poços através da aplicação de aprendizado por transferência. Esses desenvolvimentos ilustram o potencial da IA em melhorar processos e a precisão e eficiência da análise de dados \cite{Rahmani2021}.
        
        % Processamento de Linguagem Natural (PLN) situa-se na interseção da ciência da computação e linguística, representando um domínio dentro da inteligência artificial que visa permitir que computadores compreendam e processem a linguagem humana de maneira significativa e eficaz \cite{Liddy2001}. Este campo integra uma variedade de técnicas computacionais para analisar e representar texto em vários níveis de detalhe linguístico, buscando emular a compreensão da linguagem humana. Como uma área ativa de pesquisa, tradicionalmente o PLN emprega múltiplas camadas de análise linguística, cada uma contribuindo de forma única para a interpretação e geração de linguagem, encontrando aplicações práticas em diversos setores \cite{Liddy2001}.
        
        % Na indústria de O\&G, a gestão de dados não estruturados, como textos, imagens e documentos, é crucial, com o Processamento de Linguagem Natural (PLN) e o Aprendizado de Máquina desempenhando papéis chave. Pesquisas de \cite{Antoniak2016} e \cite{Castineira2018} exploraram o uso de PLN para analisar riscos e relatórios de perfuração.
    
\chapter{Revisão de Literatura} 

    \section{IA na indústria de petroleo}

        The use of AI in the Exploration and Production (E\&P) industry has been extensive. In the last decades the majority of AI applications in the industry involve data mining and neural networks \cite{Bravo2014}. One example is the work by \cite{Gudala2021} on the optimization of heavy oil flow properties, through the use of a neural networks to optimize flow-influencing parameters.
        Another development was a deep learning workflow proposed by \cite{Gohari2024}, with the generation of synthetic graphic well logs through the application of transfer learning. These developments illustrate the potential of AI in improving processes and the accuracy and efficiency of data analysis \cite{Rahmani2021}.
    
        Natural Language Processing (NLP) stands at the intersection of computer science and linguistics, representing a domain within artificial intelligence aimed at enabling computers to understand and process human language in a way that is both meaningful and effective \cite{Liddy2001}. This field integrates a diverse range of computational techniques to analyze and represent text at various levels of linguistic detail, striving to emulate human-like language understanding. As an active area of research, traditionally NLP  employs multiple layers of language analysis, each contributing uniquely to the interpretation and generation of language, which finds practical applications across various sectors \cite{Liddy2001}.      
        In the O\&G industry, the management of unstructured data such as texts, images, and documents is crucial, with Natural Language Processing (NLP) and Machine Learning playing key roles.
        Research by \cite{Antoniak2016} and \cite{Castineira2018} has explored the use of NLP to analyze risks and drilling reports.           
        
        % \section{Modelos de Linguagem de Grande Escala (LLMs).}  
        
        %     Os LLMs são modelos avançados baseados em redes neurais projetados para entender e gerar textos semelhantes aos humanos. Eles utilizam a arquitetura Transformer, apresentada no artigo seminal "Attention is All You Need" por \cite{Vaswani2017}. Esta arquitetura depende de mecanismos de auto-atenção, permitindo que o modelo avalie efetivamente a importância de diferentes palavras em uma sentença.
            
        %     O surgimento dos LLMs tornou possível compreender e produzir informações textuais. Espera-se que esses sistemas revolucionem várias indústrias ao apoiar processos complexos de tomada de decisão. Os modelos GPT \cite{OpenAI2023}, em particular, aproveitam seu vasto conjunto de dados de treinamento para fornecer respostas semelhantes às humanas \cite{Mosser2024}, o que pode ser altamente benéfico em contextos que exigem compreensão e geração de linguagem natural.
            
        %     Como destacado por \cite{Singh2023}, a integração de soluções baseadas em LLMs, como chatbots conversacionais, oferece uma abordagem para otimizar operações em vários segmentos de negócios, incluindo perfuração, completação e produção. \cite{Singh2023} usa modelos LLMs para extrair, analisar e interpretar conjuntos de dados, permitindo a geração de insights e recomendações.
            
        %     Apesar de seu impacto generalizado, os modelos de linguagem não estão isentos de limitações. Em muitas aplicações específicas da indústria, as informações críticas necessárias são frequentemente proprietárias, não compartilhadas com terceiros e, portanto, ausentes dos dados de treinamento desses LLMs \cite{Mosser2024}. Essa lacuna significa que os modelos GPT podem não ter acesso às informações mais atualizadas ou sensíveis necessárias para certas tarefas. Além disso, devido à sua natureza probabilística, os LLMs podem experimentar alucinações, produzindo respostas confiantes, mas incorretas ou sem sentido, com base na entrada do usuário \cite{OpenAI2023}.
    
    \section{Large Language Models}         

        Large Language Models (LLMs) are advanced neural network-based models designed to understand and generate human-like text. They leverage the Transformer architecture introduced in the seminal paper "Attention is All You Need" by \cite{Vaswani2017}. This architecture relies on self-attention mechanisms, allowing the model to weigh the importance of different words in a sentence effectively. 

        The emergence of LLMs has made it possible to comprehend and produce textual information. These systems are expected to revolutionize various industries by supporting complex decision-making processes. GPT models \cite{OpenAI2023}, in particular, leverages its vast training data to provide human-like responses \cite{Mosser2024}, which can be highly beneficial in contexts requiring natural language understanding and generation. 
        
        As highlighted by \cite{Singh2023}, the integration of LLM-based solutions, such as conversational chatbots, offers an approach to optimizing operations across various business segments, including drilling, completion, and production. \cite{Singh2023} uses LLMs models to extract, analyze, and interpret datasets, enabling generation of insights and recommendations. 

        Despite its widespread impact, language models are not without its limitations. In many industry-specific applications, the critical information required is often proprietary, not shared with third parties, and thus absent from the training data of these LLMs \cite{Mosser2024}. This gap means that GPT models might not have access to the most up-to-date or sensitive information needed for certain tasks. Moreover, due to their probabilistic nature, LLMs can experience hallucinations, producing confident yet incorrect or nonsensical responses based on user input \cite{OpenAI2023}. 

    % \section{Tarefas de Pergunta e Resposta (Q\&A).}
    
    %     A tarefa de Pergunta e Resposta (Q\&A) representa um método para facilitar a transferência de conhecimento entre indivíduos dentro das organizações \cite{Iske2005}. Conceitualmente, os sistemas Q\&A são projetados para conectar indivíduos que possuem conhecimento específico com aqueles que buscam esse conhecimento por meio de um formato estruturado de pergunta e resposta.
    %     O papel do Q\&A no cenário da documentação, exemplificado por plataformas como o Stack Overflow, destaca sua importância em disciplinas técnicas \cite{Treude2011}. Esse entendimento pode orientar as organizações a tomarem decisões mais informadas sobre a implementação de tais sistemas para aprimorar a transferência de conhecimento e o aprendizado organizacional \cite{Iske2005}.

    \section{Q\&A tasks}     

        Question and Answer (Q\&A) represent a method for facilitating knowledge transfer between individuals within organizations \citep{Iske2005}. Conceptually, Q\&A systems are designed to connect individuals who possess specific knowledge with those seeking that knowledge through a structured question-and-answer format. 
        The role of Q\&A in the documentation landscape, as exemplified by platforms like Stack Overflow, highlights their significance in technical disciplines \citep{Treude2011}. This understanding can guide organizations in making more informed decisions about implementing such systems to enhance knowledge transfer and organizational learning \citep{Iske2005}.


    % \section{Tarefas Text-to-SQL}
    
    %     As tarefas de Text-to-SQL no contexto da inteligência artificial envolvem a tradução automática de perguntas ou comandos em linguagem natural para consultas SQL (Structured Query Language) estruturadas \cite{Qin2022}. Esta é uma área importante no processamento de linguagem natural (NLP), permitindo que os usuários interajam com bancos de dados usando linguagem comum, em vez de precisar saber como escrever consultas SQL complexas.
        
    %     A chegada de modelos de linguagem avançados como GPT-3 e GPT-4 \cite{OpenAI2023} marcou um salto significativo nas aplicações de Text-to-SQL \cite{Singh2023}, demonstrando capacidades notáveis no tratamento dessas tarefas. Isso pode ser atribuído ao seu extenso treinamento em conjuntos de dados diversificados \cite{Deng2021}, que incluem não apenas grandes volumes de texto, mas também dados estruturados como tabelas e código, permitindo ao modelo entender as relações intrincadas entre linguagem e estruturas de dados. O estudo de \cite{Deng2023} introduz um framework de pré-treinamento para tradução de texto para SQL, enfatizando o alinhamento entre texto e tabelas nas tarefas de Text-to-SQL.
    
    \section{Text-to-SQL tasks} 

        Text-to-SQL tasks in the context of artificial intelligence involve the automatic translation of natural language questions or commands into structured SQL (Structured Query Language) queries \citep{Qin2022}. This is an important area in natural language processing (NLP), allowing users to interact with databases using plain language rather than needing to know how to write complex SQL queries.         
        
        The arrival of advanced language models like GPT-3 and GPT-4 \citep{OpenAImodels} has marked a significant leap in Text-to-SQL applications \citep{Singh2023}, demonstrating remarkable capabilities in handling these tasks. This can be attributed to their extensive training on diverse datasets \citep{Deng2021}, which include not only large amounts of text but also structured data like tables and code, enabling the model to understand the intricate relationships between language and data structures. The study by \citep{Deng2023} introduces a pre-training framework for text to SQL translation, emphasizing the alignment between text and tables in Text-to-SQL tasks.
    
    % \section{Multi-Agent Setup} 

    %     Conforme demonstrado por \cite{xi2023rise}, a busca pela Inteligência Artificial Geral (AGI) tem se beneficiado significativamente do desenvolvimento de agentes baseados em LLM, capazes de percepção, tomada de decisão e ação em diversos cenários.        
    %     Seu estudo delineia uma estrutura fundamental para tais agentes, composta por componentes de cérebro, percepção e ação, que podem ser personalizados para várias aplicações, incluindo cenários de agente único, sistemas multi-agentes e colaboração humano-agente. 
    %     A pesquisa abrangente destaca o papel crucial dos LLMs no avanço em direção à AGI, sugerindo um horizonte promissor para a eficiência operacional e os processos de tomada de decisão em configurações organizacionais complexas \cite{xi2023rise}.
    %     \cite{Li2024} demonstrou que, através de um método de amostragem e votação, o desempenho dos LLMs escala com o número de agentes instanciados.
    %     Outro framework de código aberto é o AutoGen \cite{Wu2023}, que permite a criação de aplicações multi-agentes LLM, possibilitando a personalização em vários modos. Ele apoia diversas aplicações em campos como matemática, programação e pesquisa operacional, demonstrando sua eficácia por meio de estudos empíricos \cite{Wu2023}.


    \section{LLM-based applications}

        \subsection{Retrieval-Augmented Generation (RAG)} 

            Retrieval-Augmented Generation (RAG) technique combines LLMs with information retrieval to generate accurate and up-to-date responses, as introduced by \citet{Lewis2020}. It employs a search in a database to find relevant information, overcoming the inherent limitations of LLMs that rely solely on the prior knowledge embedded in the language model during the training phase. 
            % Thus, RAG endows LLMs with the ability to stay updated and context-aware, enhancing the quality and accuracy of the responses provided \citep{Lewis2020}. 
            With the ongoing evolution of information retrieval, which has moved from term-based methods to more semantic approaches leveraging deep learning and large datasets to tackle more complex challenges.
            
            % In exploring advancements in language models, it is important to recognize the role of RAG in enhancing the precision and applicability of these models in various NLP tasks. 
            As elucidated by \citet{Lewis2020}, RAG unites the strengths of pre-trained parametric and non-parametric memory, using a dense vector index and a semantic retriever. 
            % Their findings highlight the superiority of RAG in generating more specific, diverse, and factual language, marking a significant step in the evolution of NLP technologies. 
            As demonstrated by \citet{Li2022} in their analysis, RAG is surpassing traditional generative models in terms of performance across a variety of tasks. The study provides a detailed survey on this topic, emphasizing the fundamental concepts and its applicability in specific contexts.

            New tools have been developed to facilitate the implementation of RAG solutions. \citet{Liu2023} present a toolkit that integrates augmented retrieval techniques into LLMs, including modules for query rewriting, document retrieval, passage extraction, response generation, and fact-checking, enabling the creation of more factual and specific responses. The recent study by \citet{Zhao2023} extends this horizon by examining the incorporation of multimodal knowledge into generative models, exploring the integration of diverse external sources such as images, code, tables, graphs, and audio, to enhance the grounding context and improve usability. It also explores potential future trajectories in this emerging field, marking a relevant contribution to the evolving narrative of RAG and its applications.

        \subsection{Intelligent Agents} 

            According to \citet{Russell2020}, an agent is something that performs actions. When it comes to computerized agents (in our case, AI-based), these agents are expected to do more: operate autonomously, perceive the environment, persist over time, adapt to changes, create, and strive to achieve goals.
            The agent program implements the agent function.
            There is a variety of basic agent program designs that vary in efficiency, compactness, and flexibility. The appropriate design of the agent program depends on the nature of the environment. In this work, a goal-based agent design was implemented, which acts to achieve defined goals \citep{Russell2020}.
            Other possible types include simple reflex agents, which directly respond to perceptions, while model-based reflex agents maintain an internal state to track aspects of the world that are not evident in the current perception. Finally, there are utility-based agents, which try to maximize their expected "happiness" \citep{Russell2020}.
            
        \subsection{Multi-Agent Setup} 
        
            As demonstrated by \citet{xi2023rise}, the pursuit of Artificial General Intelligence (AGI) has significantly benefited from the development of LLM-based agents, capable of sensing, decision-making, and acting across diverse scenarios.        
            His study outline a foundational framework for such agents, consisting of brain, perception, and action components, which can be customized for various applications including single-agent scenarios, multi-agent systems, and human-agent collaboration . 
            The comprehensive survey underscores the crucial role of LLMs in moving towards AGI, suggesting a promising horizon for operational efficiency and decision-making processes in complex organizational settings \citep{xi2023rise}.

            \citet{Li2024} demonstrated that, through a sampling and voting method, the performance of LLMs scales with the number of instantiated agents.
            Another open-source framework is AutoGen \citep{Wu2023}, that enables the creation of LLM multi-agent applications, allowing for customization across various modes including. It supports diverse applications in fields such as mathematics, coding, and operations research, demonstrating its effectiveness through empirical studies \citep{Wu2023}.

        \subsection{LLM-as-judge}
            
            % LLM-as-Judge is defined as the use of LLMs to evaluate and assess textual information, providing a structured framework for judgment tasks.
            % A comprehensive taxonomy explores this paradigm from three dimensions: what to judge, how to judge, and where to judge, offering a systematic approach to understanding its applications and challenges (\cite{li2025generationjudgmentopportunitieschallenges}).

            % % Applications and Methodology

            % LLMs-as-Judges are applied in various fields, including software development, where they validate and verify testsuites, enhancing the quality of compiler implementations (\cite{sollenberger2024llm4vvexploringllmasajudgevalidation}).
            % The methodology involves constructing evaluation systems with LLMs, focusing on their functionality, application domains, and evaluation methods (\cite{li2025generationjudgmentopportunitieschallenges}).

            % % Challenges and Limitations

            % Despite their potential, LLM-as-Judge systems face challenges such as vulnerability to prompt injection attacks, which can manipulate model evaluations. The JudgeDeceiver method exemplifies how optimization-based attacks can alter decision outcomes, highlighting the need for robust security measures(\cite{shi2025optimizationbasedpromptinjectionattack}).
            % Limitations also include issues with factual consistency in summarization tasks, where LLMs may struggle with understanding complex narratives and character relationships(\cite{jeong2025agentasjudgefactualsummarizationlong}).
            % While LLM-as-Judge offers promising advancements in evaluation tasks, it is crucial to address its limitations and vulnerabilities. The ongoing research aims to enhance the robustness and reliability of these systems, ensuring they can effectively complement or replace human judgment in various applications.

            The LLM-as-Judge paradigm represents a significant shift in the evaluation of NLP systems in general, utilizing a language model as a scalable proxy for human evaluators (\cite{li2024llmsasjudgescomprehensivesurveyllmbased}). This approach was developed to overcome the semantic shallowness of traditional metrics like BLEU or ROUGE and the logistical challenges of extensive human annotation (\cite{Zheng2023}). By providing a "judge" LLM with a clear rubric and context, it can perform nuanced assessments of qualities like coherence, relevance, and factual accuracy (\cite{li2024llmsasjudgescomprehensivesurveyllmbased}). This method has proven effective for complex, open-ended tasks where simple string matching is insufficient, with models like GPT-4 demonstrating over 80\% agreement with human preferences in benchmarking studies (\cite{Zheng2023}).

            For evaluating Retrieval-Augmented Generation (RAG) systems, the LLM-as-Judge framework can be adapted to produce structured, quantitative assessments. In this application, the judge LLM is tasked with comparing the RAG-generated answer against a ground-truth dataset. By leveraging a meticulously crafted prompt that defines the classification criteria, the judge can systematically categorize each output into classes such as True Positive (TP) (factually consistent with the ground truth), False Positive (FP) (introduces unsupported information), True Negative (TN) (a correct refusal to answer), or False Negative (FN) (missing relevant information). This structured approach moves beyond subjective scoring towards a more objective, task-specific evaluation, similar to methodologies that use specialized judge models trained on fine-grained feedback for enhanced reliability (\cite{Kim2024}).

            The primary advantage of this methodology is its ability to translate qualitative, AI-driven judgments directly into a confusion matrix. By aggregating the classifications across an entire evaluation dataset, it becomes possible to calculate standard, interpretable metrics such as precision (Equation~\ref{eq:precision}), recall (Equation~\ref{eq:recall}), and F1-score (Equation~\ref{eq:f1-score}). This process establishes a robust and replicable pipeline for benchmarking the factual accuracy of a RAG system at scale. While it is important to acknowledge the potential for inherent biases in LLM judges (\cite{Gu2025}), studies show high correlation with human-expert evaluations (\cite{li2024llmsasjudgescomprehensivesurveyllmbased}), making it a powerful tool for iterative development and system comparison.

            \begin{equation}
                \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                \label{eq:precision}
            \end{equation}

            \begin{equation}
                \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                \label{eq:recall}
            \end{equation}

            \begin{equation}
                \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                \label{eq:f1-score}
            \end{equation}
